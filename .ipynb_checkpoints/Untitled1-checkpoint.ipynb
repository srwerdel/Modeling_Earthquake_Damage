{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    '''Get the datasets from the Driven Data website by specific URLs and return a dataframe.'''\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    text = html.text.splitlines()\n",
    "    \n",
    "    # split each line into the columns, seperated by commas\n",
    "    split_text = []\n",
    "    for i in text:\n",
    "        split_text.append(i.split(','))\n",
    "     \n",
    "    # get column names from first line\n",
    "    col_names = split_text[0]\n",
    "    \n",
    "    value_dict = dict()\n",
    "    # for each line add number of line as key and data as values to value_dictionary\n",
    "    for i in range(1,len(split_text)):\n",
    "        value_dict[i] = split_text[i]\n",
    "    \n",
    "    # create dataframe using column names and dictionary of data\n",
    "    df = pd.DataFrame.from_dict(value_dict, orient='index', columns = col_names)\n",
    "    \n",
    "    # convert dtype into int or category\n",
    "    pattern = re.compile(r'_id$|count_|age|_percentage$|has_|_grade$')\n",
    "    for i in df.columns:\n",
    "        if re.search(pattern, i):\n",
    "            df[i] = df[i].astype(int)\n",
    "        else:\n",
    "            df[i] = df[i].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of train values\n",
    "X_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_values.csv\")\n",
    "y_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_labels.csv\")\n",
    "X_OHE = pd.get_dummies(X_df,drop_first=True)\n",
    "X_OHE = X_OHE.drop(labels=['building_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_OHE)\n",
    "\n",
    "# Scale the train set\n",
    "X = scaler.transform(X_OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260601, 60)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y_df['damage_grade'])\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_labels = keras.utils.to_categorical(y-1, num_classes=3,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X, y_labels, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback code for monitoring learning rate\n",
    "class lr_mon(keras.callbacks.Callback):\n",
    "     def on_epoch_end(self, epoch, logs=None):\n",
    "        print(' ### Learnig rate at the end of epoch {} is {} \\n'.format(\n",
    "            epoch, K.eval(self.model.optimizer.lr) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "W0809 16:41:00.784026 4548113856 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0809 16:41:00.952054 4548113856 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 182420 samples, validate on 78181 samples\n",
      "Epoch 1/200\n",
      "182420/182420 [==============================] - 5s 30us/step - loss: 0.7756 - acc: 0.6155 - val_loss: 0.7874 - val_acc: 0.6026\n",
      " ### Learnig rate at the end of epoch 0 is 1.0 \n",
      "\n",
      "Epoch 2/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.7310 - acc: 0.6474 - val_loss: 0.7268 - val_acc: 0.6523\n",
      " ### Learnig rate at the end of epoch 1 is 1.0 \n",
      "\n",
      "Epoch 3/200\n",
      "182420/182420 [==============================] - 5s 25us/step - loss: 0.7118 - acc: 0.6618 - val_loss: 0.7355 - val_acc: 0.6397\n",
      " ### Learnig rate at the end of epoch 2 is 1.0 \n",
      "\n",
      "Epoch 4/200\n",
      "182420/182420 [==============================] - 5s 25us/step - loss: 0.7027 - acc: 0.6678 - val_loss: 0.7121 - val_acc: 0.6641\n",
      " ### Learnig rate at the end of epoch 3 is 1.0 \n",
      "\n",
      "Epoch 5/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.6954 - acc: 0.6722 - val_loss: 0.7213 - val_acc: 0.6596\n",
      " ### Learnig rate at the end of epoch 4 is 1.0 \n",
      "\n",
      "Epoch 6/200\n",
      "182420/182420 [==============================] - 5s 26us/step - loss: 0.6897 - acc: 0.6751 - val_loss: 0.7232 - val_acc: 0.6617\n",
      " ### Learnig rate at the end of epoch 5 is 1.0 \n",
      "\n",
      "Epoch 7/200\n",
      "182420/182420 [==============================] - 5s 25us/step - loss: 0.6843 - acc: 0.6794 - val_loss: 0.7065 - val_acc: 0.6652\n",
      " ### Learnig rate at the end of epoch 6 is 1.0 \n",
      "\n",
      "Epoch 8/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.6800 - acc: 0.6820 - val_loss: 0.7128 - val_acc: 0.6638\n",
      " ### Learnig rate at the end of epoch 7 is 1.0 \n",
      "\n",
      "Epoch 9/200\n",
      "182420/182420 [==============================] - 5s 26us/step - loss: 0.6754 - acc: 0.6840 - val_loss: 0.7065 - val_acc: 0.6656\n",
      " ### Learnig rate at the end of epoch 8 is 1.0 \n",
      "\n",
      "Epoch 10/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.6709 - acc: 0.6852 - val_loss: 0.7052 - val_acc: 0.6691\n",
      " ### Learnig rate at the end of epoch 9 is 1.0 \n",
      "\n",
      "Epoch 11/200\n",
      "182420/182420 [==============================] - 3s 19us/step - loss: 0.6670 - acc: 0.6889 - val_loss: 0.7011 - val_acc: 0.6726\n",
      " ### Learnig rate at the end of epoch 10 is 1.0 \n",
      "\n",
      "Epoch 12/200\n",
      "182420/182420 [==============================] - 3s 17us/step - loss: 0.6628 - acc: 0.6907 - val_loss: 0.7021 - val_acc: 0.6716\n",
      " ### Learnig rate at the end of epoch 11 is 1.0 \n",
      "\n",
      "Epoch 13/200\n",
      "182420/182420 [==============================] - 2s 14us/step - loss: 0.6591 - acc: 0.6941 - val_loss: 0.7205 - val_acc: 0.6670\n",
      " ### Learnig rate at the end of epoch 12 is 1.0 \n",
      "\n",
      "Epoch 14/200\n",
      "182420/182420 [==============================] - 3s 16us/step - loss: 0.6552 - acc: 0.6948 - val_loss: 0.6989 - val_acc: 0.6768\n",
      " ### Learnig rate at the end of epoch 13 is 1.0 \n",
      "\n",
      "Epoch 15/200\n",
      "182420/182420 [==============================] - 3s 17us/step - loss: 0.6517 - acc: 0.6974 - val_loss: 0.7076 - val_acc: 0.6731\n",
      " ### Learnig rate at the end of epoch 14 is 1.0 \n",
      "\n",
      "Epoch 16/200\n",
      "182420/182420 [==============================] - 3s 16us/step - loss: 0.6474 - acc: 0.6995 - val_loss: 0.6983 - val_acc: 0.6768\n",
      " ### Learnig rate at the end of epoch 15 is 1.0 \n",
      "\n",
      "Epoch 17/200\n",
      "182420/182420 [==============================] - 3s 16us/step - loss: 0.6441 - acc: 0.7014 - val_loss: 0.7074 - val_acc: 0.6735\n",
      " ### Learnig rate at the end of epoch 16 is 1.0 \n",
      "\n",
      "Epoch 18/200\n",
      "182420/182420 [==============================] - 3s 15us/step - loss: 0.6405 - acc: 0.7028 - val_loss: 0.7141 - val_acc: 0.6709\n",
      " ### Learnig rate at the end of epoch 17 is 1.0 \n",
      "\n",
      "Epoch 19/200\n",
      "182420/182420 [==============================] - 3s 16us/step - loss: 0.6361 - acc: 0.7056 - val_loss: 0.7156 - val_acc: 0.6773\n",
      " ### Learnig rate at the end of epoch 18 is 1.0 \n",
      "\n",
      "Epoch 20/200\n",
      "182420/182420 [==============================] - 3s 15us/step - loss: 0.6330 - acc: 0.7065 - val_loss: 0.7152 - val_acc: 0.6739\n",
      " ### Learnig rate at the end of epoch 19 is 1.0 \n",
      "\n",
      "Epoch 21/200\n",
      "182420/182420 [==============================] - 3s 14us/step - loss: 0.6297 - acc: 0.7095 - val_loss: 0.7170 - val_acc: 0.6735\n",
      " ### Learnig rate at the end of epoch 20 is 1.0 \n",
      "\n",
      "Epoch 22/200\n",
      "182420/182420 [==============================] - 3s 15us/step - loss: 0.6262 - acc: 0.7107 - val_loss: 0.7071 - val_acc: 0.6752\n",
      " ### Learnig rate at the end of epoch 21 is 1.0 \n",
      "\n",
      "Epoch 23/200\n",
      "182420/182420 [==============================] - 3s 14us/step - loss: 0.6228 - acc: 0.7130 - val_loss: 0.7150 - val_acc: 0.6772\n",
      " ### Learnig rate at the end of epoch 22 is 1.0 \n",
      "\n",
      "Epoch 24/200\n",
      "182420/182420 [==============================] - 3s 15us/step - loss: 0.6197 - acc: 0.7140 - val_loss: 0.7159 - val_acc: 0.6757\n",
      " ### Learnig rate at the end of epoch 23 is 1.0 \n",
      "\n",
      "Epoch 25/200\n",
      "182420/182420 [==============================] - 3s 14us/step - loss: 0.6156 - acc: 0.7158 - val_loss: 0.7285 - val_acc: 0.6702\n",
      " ### Learnig rate at the end of epoch 24 is 1.0 \n",
      "\n",
      "Epoch 26/200\n",
      "182420/182420 [==============================] - 3s 15us/step - loss: 0.6118 - acc: 0.7179 - val_loss: 0.7203 - val_acc: 0.6770\n",
      " ### Learnig rate at the end of epoch 25 is 1.0 \n",
      "\n",
      "Epoch 27/200\n",
      "182420/182420 [==============================] - 3s 14us/step - loss: 0.5771 - acc: 0.7362 - val_loss: 0.7141 - val_acc: 0.6849\n",
      " ### Learnig rate at the end of epoch 26 is 0.10000000149011612 \n",
      "\n",
      "Epoch 28/200\n",
      "182420/182420 [==============================] - 3s 18us/step - loss: 0.5652 - acc: 0.7419 - val_loss: 0.7195 - val_acc: 0.6830\n",
      " ### Learnig rate at the end of epoch 27 is 0.10000000149011612 \n",
      "\n",
      "Epoch 29/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5603 - acc: 0.7445 - val_loss: 0.7230 - val_acc: 0.6833\n",
      " ### Learnig rate at the end of epoch 28 is 0.10000000149011612 \n",
      "\n",
      "Epoch 30/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5566 - acc: 0.7472 - val_loss: 0.7261 - val_acc: 0.6835\n",
      " ### Learnig rate at the end of epoch 29 is 0.10000000149011612 \n",
      "\n",
      "Epoch 31/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5536 - acc: 0.7487 - val_loss: 0.7339 - val_acc: 0.6827\n",
      " ### Learnig rate at the end of epoch 30 is 0.10000000149011612 \n",
      "\n",
      "Epoch 32/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5508 - acc: 0.7505 - val_loss: 0.7350 - val_acc: 0.6829\n",
      " ### Learnig rate at the end of epoch 31 is 0.10000000149011612 \n",
      "\n",
      "Epoch 33/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5484 - acc: 0.7513 - val_loss: 0.7383 - val_acc: 0.6829\n",
      " ### Learnig rate at the end of epoch 32 is 0.10000000149011612 \n",
      "\n",
      "Epoch 34/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5460 - acc: 0.7528 - val_loss: 0.7422 - val_acc: 0.6823\n",
      " ### Learnig rate at the end of epoch 33 is 0.10000000149011612 \n",
      "\n",
      "Epoch 35/200\n",
      "182420/182420 [==============================] - 5s 28us/step - loss: 0.5438 - acc: 0.7540 - val_loss: 0.7464 - val_acc: 0.6820\n",
      " ### Learnig rate at the end of epoch 34 is 0.10000000149011612 \n",
      "\n",
      "Epoch 36/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5417 - acc: 0.7546 - val_loss: 0.7470 - val_acc: 0.6814\n",
      " ### Learnig rate at the end of epoch 35 is 0.10000000149011612 \n",
      "\n",
      "Epoch 37/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5338 - acc: 0.7594 - val_loss: 0.7498 - val_acc: 0.6826\n",
      " ### Learnig rate at the end of epoch 36 is 0.009999999776482582 \n",
      "\n",
      "Epoch 38/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5329 - acc: 0.7597 - val_loss: 0.7516 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 37 is 0.009999999776482582 \n",
      "\n",
      "Epoch 39/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5325 - acc: 0.7600 - val_loss: 0.7516 - val_acc: 0.6828\n",
      " ### Learnig rate at the end of epoch 38 is 0.009999999776482582 \n",
      "\n",
      "Epoch 40/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5321 - acc: 0.7599 - val_loss: 0.7527 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 39 is 0.009999999776482582 \n",
      "\n",
      "Epoch 41/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5318 - acc: 0.7600 - val_loss: 0.7537 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 40 is 0.009999999776482582 \n",
      "\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5315 - acc: 0.7603 - val_loss: 0.7538 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 41 is 0.009999999776482582 \n",
      "\n",
      "Epoch 43/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5312 - acc: 0.7607 - val_loss: 0.7544 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 42 is 0.009999999776482582 \n",
      "\n",
      "Epoch 44/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5309 - acc: 0.7607 - val_loss: 0.7550 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 43 is 0.009999999776482582 \n",
      "\n",
      "Epoch 45/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5306 - acc: 0.7610 - val_loss: 0.7553 - val_acc: 0.6822\n",
      " ### Learnig rate at the end of epoch 44 is 0.009999999776482582 \n",
      "\n",
      "Epoch 46/200\n",
      "182420/182420 [==============================] - 4s 20us/step - loss: 0.5304 - acc: 0.7608 - val_loss: 0.7555 - val_acc: 0.6823\n",
      " ### Learnig rate at the end of epoch 45 is 0.009999999776482582 \n",
      "\n",
      "Epoch 47/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5293 - acc: 0.7615 - val_loss: 0.7557 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 46 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 48/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5293 - acc: 0.7616 - val_loss: 0.7558 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 47 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 49/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5292 - acc: 0.7615 - val_loss: 0.7560 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 48 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 50/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5292 - acc: 0.7616 - val_loss: 0.7561 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 49 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 51/200\n",
      "182420/182420 [==============================] - 10s 52us/step - loss: 0.5292 - acc: 0.7616 - val_loss: 0.7562 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 50 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 52/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5292 - acc: 0.7616 - val_loss: 0.7563 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 51 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 53/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5291 - acc: 0.7617 - val_loss: 0.7564 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 52 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 54/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5291 - acc: 0.7616 - val_loss: 0.7564 - val_acc: 0.6825\n",
      " ### Learnig rate at the end of epoch 53 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 55/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5291 - acc: 0.7617 - val_loss: 0.7565 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 54 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 56/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5290 - acc: 0.7615 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 55 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 57/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 56 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 58/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5289 - acc: 0.7618 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 57 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 59/200\n",
      "182420/182420 [==============================] - 4s 21us/step - loss: 0.5289 - acc: 0.7618 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 58 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 60/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 59 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 61/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 60 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 62/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 61 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 63/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 62 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 64/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 63 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 65/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 64 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 66/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 65 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 67/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 66 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 68/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 67 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 69/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 68 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 70/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 69 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 71/200\n",
      "182420/182420 [==============================] - 4s 22us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 70 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 72/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 71 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 73/200\n",
      "182420/182420 [==============================] - 4s 23us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 72 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 74/200\n",
      "182420/182420 [==============================] - 5s 25us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 73 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 75/200\n",
      "182420/182420 [==============================] - 4s 24us/step - loss: 0.5289 - acc: 0.7617 - val_loss: 0.7566 - val_acc: 0.6824\n",
      " ### Learnig rate at the end of epoch 74 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 76/200\n",
      "155776/182420 [========================>.....] - ETA: 0s - loss: 0.5294 - acc: 0.7617"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-63fed6d59832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlr_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation=LeakyReLU(alpha=0.03), input_dim=60))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.0001,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "RMS = keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adadelta, metrics=['accuracy'])\n",
    "\n",
    "lr_call = lr_mon()\n",
    "lr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=128,validation_data=(X_test,y_test), callbacks=[lr_call,lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of train values\n",
    "X_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_values.csv\")\n",
    "y_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_labels.csv\")\n",
    "X_df = pd.get_dummies(X_df,drop_first=True)\n",
    "X_df = X_df.drop(labels=['building_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/test_values.csv\")\n",
    "X_test_df = pd.get_dummies(X_test_df,drop_first=True)\n",
    "X_test = X_test_df.drop(labels=['building_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_df)\n",
    "\n",
    "# Scale the train set\n",
    "X = scaler.transform(X_df)\n",
    "\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y_train = np.array(y_df['damage_grade'])\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_labels = keras.utils.to_categorical(y_train-1, num_classes=3,dtype=int)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X, y_labels, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/200\n",
      "208480/208480 [==============================] - 11s 51us/step - loss: 0.7710 - acc: 0.6184 - f1_m: 0.6011 - val_loss: 0.7552 - val_acc: 0.6388 - val_f1_m: 0.6312\n",
      " ### Learnig rate at the end of epoch 0 is 1.0 \n",
      "\n",
      "Epoch 2/200\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.7250 - acc: 0.6534 - f1_m: 0.6464 - val_loss: 0.7214 - val_acc: 0.6602 - val_f1_m: 0.6554\n",
      " ### Learnig rate at the end of epoch 1 is 1.0 \n",
      "\n",
      "Epoch 3/200\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.7094 - acc: 0.6644 - f1_m: 0.6593 - val_loss: 0.7147 - val_acc: 0.6648 - val_f1_m: 0.6587\n",
      " ### Learnig rate at the end of epoch 2 is 1.0 \n",
      "\n",
      "Epoch 4/200\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.7004 - acc: 0.6700 - f1_m: 0.6653 - val_loss: 0.7110 - val_acc: 0.6685 - val_f1_m: 0.6643\n",
      " ### Learnig rate at the end of epoch 3 is 1.0 \n",
      "\n",
      "Epoch 5/200\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6937 - acc: 0.6735 - f1_m: 0.6693 - val_loss: 0.7040 - val_acc: 0.6685 - val_f1_m: 0.6633\n",
      " ### Learnig rate at the end of epoch 4 is 1.0 \n",
      "\n",
      "Epoch 6/200\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.6877 - acc: 0.6769 - f1_m: 0.6727 - val_loss: 0.7000 - val_acc: 0.6702 - val_f1_m: 0.6653\n",
      " ### Learnig rate at the end of epoch 5 is 1.0 \n",
      "\n",
      "Epoch 7/200\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6824 - acc: 0.6807 - f1_m: 0.6769 - val_loss: 0.7020 - val_acc: 0.6688 - val_f1_m: 0.6643\n",
      " ### Learnig rate at the end of epoch 6 is 1.0 \n",
      "\n",
      "Epoch 8/200\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6772 - acc: 0.6832 - f1_m: 0.6795 - val_loss: 0.6977 - val_acc: 0.6721 - val_f1_m: 0.6678\n",
      " ### Learnig rate at the end of epoch 7 is 1.0 \n",
      "\n",
      "Epoch 9/200\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6732 - acc: 0.6852 - f1_m: 0.6815 - val_loss: 0.7010 - val_acc: 0.6727 - val_f1_m: 0.6679\n",
      " ### Learnig rate at the end of epoch 8 is 1.0 \n",
      "\n",
      "Epoch 10/200\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6690 - acc: 0.6873 - f1_m: 0.6831 - val_loss: 0.6958 - val_acc: 0.6752 - val_f1_m: 0.6697\n",
      " ### Learnig rate at the end of epoch 9 is 1.0 \n",
      "\n",
      "Epoch 11/200\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.6647 - acc: 0.6890 - f1_m: 0.6852 - val_loss: 0.6943 - val_acc: 0.6764 - val_f1_m: 0.6723\n",
      " ### Learnig rate at the end of epoch 10 is 1.0 \n",
      "\n",
      "Epoch 12/200\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6605 - acc: 0.6924 - f1_m: 0.6890 - val_loss: 0.6963 - val_acc: 0.6754 - val_f1_m: 0.6695\n",
      " ### Learnig rate at the end of epoch 11 is 1.0 \n",
      "\n",
      "Epoch 13/200\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6565 - acc: 0.6949 - f1_m: 0.6910 - val_loss: 0.6924 - val_acc: 0.6783 - val_f1_m: 0.6741\n",
      " ### Learnig rate at the end of epoch 12 is 1.0 \n",
      "\n",
      "Epoch 14/200\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6522 - acc: 0.6977 - f1_m: 0.6941 - val_loss: 0.7050 - val_acc: 0.6756 - val_f1_m: 0.6735\n",
      " ### Learnig rate at the end of epoch 13 is 1.0 \n",
      "\n",
      "Epoch 15/200\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6489 - acc: 0.6991 - f1_m: 0.6957 - val_loss: 0.6885 - val_acc: 0.6840 - val_f1_m: 0.6800\n",
      " ### Learnig rate at the end of epoch 14 is 1.0 \n",
      "\n",
      "Epoch 16/200\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6444 - acc: 0.7015 - f1_m: 0.6986 - val_loss: 0.6908 - val_acc: 0.6806 - val_f1_m: 0.6763\n",
      " ### Learnig rate at the end of epoch 15 is 1.0 \n",
      "\n",
      "Epoch 17/200\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6414 - acc: 0.7028 - f1_m: 0.6997 - val_loss: 0.6927 - val_acc: 0.6813 - val_f1_m: 0.6770\n",
      " ### Learnig rate at the end of epoch 16 is 1.0 \n",
      "\n",
      "Epoch 18/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6383 - acc: 0.7039 - f1_m: 0.7010 - val_loss: 0.6945 - val_acc: 0.6782 - val_f1_m: 0.6744\n",
      " ### Learnig rate at the end of epoch 17 is 1.0 \n",
      "\n",
      "Epoch 19/200\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6342 - acc: 0.7072 - f1_m: 0.7041 - val_loss: 0.6970 - val_acc: 0.6834 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 18 is 1.0 \n",
      "\n",
      "Epoch 20/200\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6313 - acc: 0.7085 - f1_m: 0.7056 - val_loss: 0.6895 - val_acc: 0.6836 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 19 is 1.0 \n",
      "\n",
      "Epoch 21/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6280 - acc: 0.7109 - f1_m: 0.7081 - val_loss: 0.6896 - val_acc: 0.6844 - val_f1_m: 0.6778\n",
      " ### Learnig rate at the end of epoch 20 is 1.0 \n",
      "\n",
      "Epoch 22/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6246 - acc: 0.7124 - f1_m: 0.7097 - val_loss: 0.7020 - val_acc: 0.6805 - val_f1_m: 0.6780\n",
      " ### Learnig rate at the end of epoch 21 is 1.0 \n",
      "\n",
      "Epoch 23/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6212 - acc: 0.7138 - f1_m: 0.7111 - val_loss: 0.6975 - val_acc: 0.6842 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 22 is 1.0 \n",
      "\n",
      "Epoch 24/200\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6189 - acc: 0.7147 - f1_m: 0.7118 - val_loss: 0.6993 - val_acc: 0.6809 - val_f1_m: 0.6772\n",
      " ### Learnig rate at the end of epoch 23 is 1.0 \n",
      "\n",
      "Epoch 25/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6158 - acc: 0.7169 - f1_m: 0.7143 - val_loss: 0.7055 - val_acc: 0.6860 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 24 is 1.0 \n",
      "\n",
      "Epoch 26/200\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6131 - acc: 0.7182 - f1_m: 0.7156 - val_loss: 0.7033 - val_acc: 0.6832 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 25 is 1.0 \n",
      "\n",
      "Epoch 27/200\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6098 - acc: 0.7200 - f1_m: 0.7172 - val_loss: 0.7053 - val_acc: 0.6813 - val_f1_m: 0.6786\n",
      " ### Learnig rate at the end of epoch 26 is 1.0 \n",
      "\n",
      "Epoch 28/200\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6073 - acc: 0.7213 - f1_m: 0.7189 - val_loss: 0.7056 - val_acc: 0.6801 - val_f1_m: 0.6767\n",
      " ### Learnig rate at the end of epoch 27 is 1.0 \n",
      "\n",
      "Epoch 29/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6049 - acc: 0.7229 - f1_m: 0.7204 - val_loss: 0.7035 - val_acc: 0.6828 - val_f1_m: 0.6795\n",
      " ### Learnig rate at the end of epoch 28 is 1.0 \n",
      "\n",
      "Epoch 30/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6016 - acc: 0.7239 - f1_m: 0.7216 - val_loss: 0.7168 - val_acc: 0.6792 - val_f1_m: 0.6761\n",
      " ### Learnig rate at the end of epoch 29 is 1.0 \n",
      "\n",
      "Epoch 31/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5994 - acc: 0.7253 - f1_m: 0.7227 - val_loss: 0.7123 - val_acc: 0.6821 - val_f1_m: 0.6796\n",
      " ### Learnig rate at the end of epoch 30 is 1.0 \n",
      "\n",
      "Epoch 32/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5972 - acc: 0.7261 - f1_m: 0.7240 - val_loss: 0.7111 - val_acc: 0.6822 - val_f1_m: 0.6789\n",
      " ### Learnig rate at the end of epoch 31 is 1.0 \n",
      "\n",
      "Epoch 33/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5936 - acc: 0.7290 - f1_m: 0.7269 - val_loss: 0.7129 - val_acc: 0.6825 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 32 is 1.0 \n",
      "\n",
      "Epoch 34/200\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5917 - acc: 0.7299 - f1_m: 0.7275 - val_loss: 0.7305 - val_acc: 0.6843 - val_f1_m: 0.6819\n",
      " ### Learnig rate at the end of epoch 33 is 1.0 \n",
      "\n",
      "Epoch 35/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5892 - acc: 0.7320 - f1_m: 0.7299 - val_loss: 0.7146 - val_acc: 0.6826 - val_f1_m: 0.6791\n",
      " ### Learnig rate at the end of epoch 34 is 1.0 \n",
      "\n",
      "Epoch 36/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5511 - acc: 0.7507 - f1_m: 0.7489 - val_loss: 0.7200 - val_acc: 0.6880 - val_f1_m: 0.6852\n",
      " ### Learnig rate at the end of epoch 35 is 0.10000000149011612 \n",
      "\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5400 - acc: 0.7561 - f1_m: 0.7545 - val_loss: 0.7280 - val_acc: 0.6871 - val_f1_m: 0.6851\n",
      " ### Learnig rate at the end of epoch 36 is 0.10000000149011612 \n",
      "\n",
      "Epoch 38/200\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5354 - acc: 0.7590 - f1_m: 0.7575 - val_loss: 0.7291 - val_acc: 0.6870 - val_f1_m: 0.6850\n",
      " ### Learnig rate at the end of epoch 37 is 0.10000000149011612 \n",
      "\n",
      "Epoch 39/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5321 - acc: 0.7604 - f1_m: 0.7588 - val_loss: 0.7375 - val_acc: 0.6875 - val_f1_m: 0.6854\n",
      " ### Learnig rate at the end of epoch 38 is 0.10000000149011612 \n",
      "\n",
      "Epoch 40/200\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.5294 - acc: 0.7618 - f1_m: 0.7602 - val_loss: 0.7362 - val_acc: 0.6855 - val_f1_m: 0.6836\n",
      " ### Learnig rate at the end of epoch 39 is 0.10000000149011612 \n",
      "\n",
      "Epoch 41/200\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.5271 - acc: 0.7630 - f1_m: 0.7615 - val_loss: 0.7430 - val_acc: 0.6854 - val_f1_m: 0.6833\n",
      " ### Learnig rate at the end of epoch 40 is 0.10000000149011612 \n",
      "\n",
      "Epoch 42/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5252 - acc: 0.7640 - f1_m: 0.7625 - val_loss: 0.7460 - val_acc: 0.6860 - val_f1_m: 0.6839\n",
      " ### Learnig rate at the end of epoch 41 is 0.10000000149011612 \n",
      "\n",
      "Epoch 43/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5233 - acc: 0.7650 - f1_m: 0.7637 - val_loss: 0.7481 - val_acc: 0.6855 - val_f1_m: 0.6837\n",
      " ### Learnig rate at the end of epoch 42 is 0.10000000149011612 \n",
      "\n",
      "Epoch 44/200\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.5214 - acc: 0.7658 - f1_m: 0.7645 - val_loss: 0.7508 - val_acc: 0.6859 - val_f1_m: 0.6837\n",
      " ### Learnig rate at the end of epoch 43 is 0.10000000149011612 \n",
      "\n",
      "Epoch 45/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5198 - acc: 0.7674 - f1_m: 0.7658 - val_loss: 0.7550 - val_acc: 0.6857 - val_f1_m: 0.6832\n",
      " ### Learnig rate at the end of epoch 44 is 0.10000000149011612 \n",
      "\n",
      "Epoch 46/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5182 - acc: 0.7679 - f1_m: 0.7666 - val_loss: 0.7598 - val_acc: 0.6840 - val_f1_m: 0.6818\n",
      " ### Learnig rate at the end of epoch 45 is 0.10000000149011612 \n",
      "\n",
      "Epoch 47/200\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.5108 - acc: 0.7720 - f1_m: 0.7705 - val_loss: 0.7591 - val_acc: 0.6854 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 46 is 0.009999999776482582 \n",
      "\n",
      "Epoch 48/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5099 - acc: 0.7726 - f1_m: 0.7710 - val_loss: 0.7603 - val_acc: 0.6856 - val_f1_m: 0.6833\n",
      " ### Learnig rate at the end of epoch 47 is 0.009999999776482582 \n",
      "\n",
      "Epoch 49/200\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5095 - acc: 0.7725 - f1_m: 0.7712 - val_loss: 0.7600 - val_acc: 0.6855 - val_f1_m: 0.6832\n",
      " ### Learnig rate at the end of epoch 48 is 0.009999999776482582 \n",
      "\n",
      "Epoch 50/200\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5092 - acc: 0.7728 - f1_m: 0.7715 - val_loss: 0.7608 - val_acc: 0.6856 - val_f1_m: 0.6832\n",
      " ### Learnig rate at the end of epoch 49 is 0.009999999776482582 \n",
      "\n",
      "Epoch 51/200\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.5090 - acc: 0.7729 - f1_m: 0.7715 - val_loss: 0.7614 - val_acc: 0.6853 - val_f1_m: 0.6831\n",
      " ### Learnig rate at the end of epoch 50 is 0.009999999776482582 \n",
      "\n",
      "Epoch 52/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5088 - acc: 0.7731 - f1_m: 0.7717 - val_loss: 0.7612 - val_acc: 0.6852 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 51 is 0.009999999776482582 \n",
      "\n",
      "Epoch 53/200\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5085 - acc: 0.7731 - f1_m: 0.7718 - val_loss: 0.7617 - val_acc: 0.6854 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 52 is 0.009999999776482582 \n",
      "\n",
      "Epoch 54/200\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5083 - acc: 0.7732 - f1_m: 0.7718 - val_loss: 0.7623 - val_acc: 0.6848 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 53 is 0.009999999776482582 \n",
      "\n",
      "Epoch 55/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5081 - acc: 0.7734 - f1_m: 0.7721 - val_loss: 0.7627 - val_acc: 0.6851 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 54 is 0.009999999776482582 \n",
      "\n",
      "Epoch 56/200\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5079 - acc: 0.7736 - f1_m: 0.7723 - val_loss: 0.7633 - val_acc: 0.6849 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 55 is 0.009999999776482582 \n",
      "\n",
      "Epoch 57/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5069 - acc: 0.7740 - f1_m: 0.7726 - val_loss: 0.7634 - val_acc: 0.6851 - val_f1_m: 0.6831\n",
      " ### Learnig rate at the end of epoch 56 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 58/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5069 - acc: 0.7740 - f1_m: 0.7727 - val_loss: 0.7635 - val_acc: 0.6852 - val_f1_m: 0.6831\n",
      " ### Learnig rate at the end of epoch 57 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 59/200\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5069 - acc: 0.7740 - f1_m: 0.7727 - val_loss: 0.7636 - val_acc: 0.6853 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 58 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 60/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5068 - acc: 0.7740 - f1_m: 0.7726 - val_loss: 0.7637 - val_acc: 0.6852 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 59 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 61/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5068 - acc: 0.7741 - f1_m: 0.7726 - val_loss: 0.7638 - val_acc: 0.6852 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 60 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 62/200\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5068 - acc: 0.7740 - f1_m: 0.7727 - val_loss: 0.7639 - val_acc: 0.6851 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 61 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 63/200\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5068 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7639 - val_acc: 0.6850 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 62 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 64/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5067 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7640 - val_acc: 0.6852 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 63 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 65/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5067 - acc: 0.7740 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 64 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 66/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5067 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 65 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 67/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 66 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 68/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 67 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 69/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 68 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 70/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 69 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 70 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 72/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 71 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 73/200\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6851 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 72 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 74/200\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 73 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 75/200\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 74 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 76/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7642 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 75 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 77/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 76 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 78/200\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 77 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 79/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 78 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 80/200\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 79 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 81/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 80 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 82/200\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 81 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 83/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 82 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 84/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 83 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 85/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 84 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 86/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 85 is 9.99999883788405e-06 \n",
      "\n",
      "Epoch 87/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 86 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 88/200\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 87 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 89/200\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 88 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 90/200\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 89 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 91/200\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 90 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 92/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7727 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 91 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 93/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 92 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 94/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 93 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 95/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 94 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 96/200\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 95 is 9.99999883788405e-07 \n",
      "\n",
      "Epoch 97/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 96 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 98/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 97 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 99/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 98 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 100/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 99 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 101/200\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 100 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 102/200\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 101 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 103/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 102 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 104/200\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 103 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.5066 - acc: 0.7741 - f1_m: 0.7728 - val_loss: 0.7641 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 104 is 9.999998695775503e-08 \n",
      "\n",
      "Epoch 106/200\n",
      " 38016/208480 [====>.........................] - ETA: 4s - loss: 0.5054 - acc: 0.7740 - f1_m: 0.7727"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-322-48a120c9810f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlr_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation=LeakyReLU(alpha=0.03), input_dim=60))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "#model.add(Dense(units=128, activation=LeakyReLU(alpha=0.03)))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.00005,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "RMS = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
    "adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adadelta, metrics=['accuracy',f1_m])\n",
    "\n",
    "lr_call = lr_mon()\n",
    "lr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=128,validation_data=(X_val,y_val),callbacks=[lr_call,lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.69820116e-05, 1.04940787e-01, 8.95012259e-01],\n",
       "       [2.02587359e-02, 7.49166012e-01, 2.30575338e-01],\n",
       "       [1.35279144e-04, 8.88132274e-01, 1.11732475e-01],\n",
       "       ...,\n",
       "       [1.78457230e-01, 7.27377951e-01, 9.41648036e-02],\n",
       "       [4.26077656e-03, 9.90517557e-01, 5.22168679e-03],\n",
       "       [1.13328978e-01, 8.74200165e-01, 1.24708423e-02]], dtype=float32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prediction = np.argmax(y_pred,1)+1\n",
    "y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.63046328e-04, 1.42713726e-01, 8.57023239e-01],\n",
       "       [2.92673856e-02, 8.00030053e-01, 1.70702532e-01],\n",
       "       [7.44587034e-02, 6.06389940e-01, 3.19151431e-01],\n",
       "       ...,\n",
       "       [7.36160800e-02, 6.55939639e-01, 2.70444304e-01],\n",
       "       [1.19322315e-02, 8.16920757e-01, 1.71146959e-01],\n",
       "       [5.39853685e-02, 8.83169115e-01, 6.28455803e-02]], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = model.predict(X_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.argmax(y_test,1)+1\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7867799419809366"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_predict == y_prediction).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of Buildings of Predicted Damage Grade')"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEICAYAAACXo2mmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHg1JREFUeJzt3XuYXWV99vHvbcIhQCDBjCkkgaBEMdDKIUIs1lKoIWA12CpCK0QEYl9Q8X3rAX2tQQ6KrZUCVSpCgOAhjaASvcAYEUSsERJEOUkZEEhCIIGciIga/PWP5zfJyrhnsidZO5vJ3J/r2tes/azTsw5732s9a609igjMzMzq8JJ2V8DMzLYdDhUzM6uNQ8XMzGrjUDEzs9o4VMzMrDYOFTMzq41DpR+SdLWk89s0b0m6StJKSXe0YPofk3RFdo+VFJIG9zDsOZK+nN17SVoraVDdddpckg6X9FDW67itPO+N1p2kmyRN3QrzXb9NrH6S3iXp9nbXozcOlRpIelTSMkk7V8pOk3RrG6vVKq8H3giMjohDu/fMnf6F/CJdK+kRSf+n2YlHxKci4rS+VioiHo+IXSLihb6O20LnAv+R9fpW95653/wm19NTebCwSysqEhHHRMQ1mxou6/TXraiDpCMk/aGybyyWNFvSa1sxv61N0h6SviTpicq+f7Wk/dpdt63JoVKfQcBZ7a5EX23Gkf3ewKMR8etehvlJfpHuAvwd8C+SDtrsSvZfewP3bWKYN+d6OhiYAHy8+wB5dritfFafyOUdCkwEfgn8SNJR7a3WlpH0UuC/gZ2Av6As38HADykHYY3GaXgG3t9tKzvqi8G/Ah+UNKx7j0bNOJJulXRadr9L0o8lXSRpVR7h/HmWL8qzoO5NFyMkzZP0rKQfStq7Mu39st8KSQ9KOr7S72pJl0m6UdKvgb9qUN89Jc3J8TslnZ7lpwJXAK/LI7FPbmqlRMTPgAeAV+c0jpC0uNv81h8d99Z8ImmfXNZnJc0DRvS0jnP9npfr9VlJ35NUHf5kSY9JekbSP3erw6GSFkhak2cQn+tp+SSdnutoRa6zPbP8YeDlwLdzXe2wifW0BLgJOKBS/wsk/Rh4Dni5pN0kXSlpqaQlks7vOiiQNEjSZyU9LekR4E3d6rl+f6vU+4FcN/dLOljStcBelTp/OIedKOm/c9/8uaQjmtkmm1jeiIjFEfEJyj71mco0L879fo2khZL+otLvHElfl/TlnOc9kl4p6aP5OVkkaVJl+FMqy/mIpPd0Wy8fzvX5hErrQkjaN/vtkOv08dwP/lPSkB4W6f8Ca4CTIuLhXL5VEXFVRFya0+vaR0+V9Djwgyz/uqQnJa2WdJuk/Sv1e2nuV2tUmptf0a3+PX7W2yYi/NrCF/Ao8NfAN4Dzs+w04NbsHgsEMLgyzq3Aadn9LmAdcArljOd84HHg88AOwCTgWWCXHP7qfP+G7H8xcHv22xlYlNMaDBwEPA2Mr4y7GjicclCxY4PluQ34ArAjcCCwHDiyUtfbe1kXG/UHXgusAl6Z748AFjdaf9l9DvDlRusN+AnwuVzmN+Q66GnYW4GHgVcCQ/L9hdlvPLCW0pS3PfBZ4PeVOvyE8uUAsAswsYdlPTLX7cFZp0uB2xotV2/7TXaPoZzVnFep/+PA/rkdtwO+CXwxt/HLgDuA9+Tw/0g56h8D7A7c0mB9dO1vbweW5LYRsC+wd6M6A6OAZ4BjKfvLG/N9x6a2SYPl/aNtX1mPfwB2zvfvBF6ay/1PwJPkfkrZP54Hjs7+M4FfAf8/19HpwK8q034T5YtYwF9SAvrg7Dc5p70/5Qzjy7nO9s3+FwFzcn0OBb4NfLqHZZsPnLOJ74mxOf2ZuQ2HZPm7c/o7AP8O3F0ZZxYwO4c/ILdbU5/1tn0ftnPm28qLDaFyAOULu4O+h8pDlX5/msOPrJQ9AxyY3VcDsyr9dgFeoHyhvAP4Ubf6fRGYXhl3Zi/LMianNbRS9mng6kpdNxUq6yhB8mwux6WAsv8RbEaoUI6g15FfPNn/q42Grazfj1eGPQP4bnZ/Avhapd9OwO8qdbgN+CQwYhPb/UrgX7pth98DY7svVy/7zdpcV49RgnxIpf7nVoYdCfy2q3+WnQjckt0/AP6x0m9Sg/XRtb/NBc7qbV+uvP8IcG23YeYCUze1TRpM+4+2fZbvl3Ud1cN4K4HXVPaPeZV+b851OCjfD81pDethWt/qWnZgBpWQoIRr5F8BvwZeUen/OiqB1W26nd3W/1vY8Bn4Xrd99OW97BPDcpjdKAeYvwf2q/T/FBtCpdfPertebv6qUUTcC3wHOHszRn+q0v2bnF73supF3EWV+a4FVgB7UtrxD8umilWSVgH/APxJo3Eb2BNYERHPVsoeoxyxNmt+RAyLiKE53/0pH4YtsSewMja+lvPYJsZ5stL9HBvW355svP6eo4R2l1MpZzi/lHSnpL/ppU7r65Db4Rn6tq6Oy3W1d0ScERG/qfSrbqe9KUfiSyvb9YuUM5Y/WiZ6XzdjKGdxzdgbeHu3/en1wB5s3jZpZBTli3QVgKQPZpPV6pzfbmzcrNb9c/F0bLhBo2v97ZLTOkbS/GweWkU54+qaVvd1Vu3uoBxsLKws93ezvJFnKOsEgIiYExHDKM1i23cbdv18stnyQkkPS1pDCXWyjh2UA6qetmszn/WtzqFSv+mUU/DqF0vXh26nStmWbvgxXR0qdwztDjxB2QF/mF9UXa9dIqJ6B1b0Mt0ngN0lDa2U7UU57e6zDMbrKUeUUNbF+vWQ1wR6+qBWLQWGq3KHXdZrcywFRlfqMITS3NJV54ci4kTKF/ZngOu6zbfLE5QPdtd0ds7pbNa6aqC6nRZRzlRGVLbrrhHR1f6+lMo+Qe/rZhHd2uZ7mGfXsNd22592jogLqW+bvBW4KyJ+nddPPgwcDwzPL+bVlDOHPsnrWNdTmjdH5rRurExro/2Ajdff05SA2r+y3LtFucmgkZuB49TcDRXVdfz3wBRKS8dulLMZso7LKWeCPW3XZj7rW51DpWYR0Qn8F/D+StlyyhfNO/PI5N30/KFu1rGSXi9pe+A8ytnBIsqZ0islnSRpu3y9VtKrm6z/IspdLJ+WtKOkP6McuW/Wswcqd8W8lQ13Qf0PsKOkN0najnK3U68XsbNejwELgE9K2l7S69kQVH11HfBmlZshtqc0qaz/0pL0TkkdEfEH8uiZ0ubf3deAUyQdmF9gnwJ+GhGPbma9ehQRS4HvAf8maVdJL5H0Ckl/mYPMBt4vabSk4fR+tnwF5aaSQ1Tsqw03ejxFucGgy5cp6+ro3Hd3VLnZYvSWbJOc7yhJ0ylNxR/LXkMpX6TLgcGSPgHs2sw0G9iesm8tB9ZJOobSLNhlNmX7vVrSTsA/d/XIbf8l4CJJL8s6j5J0dA/z+hwwHLg2t4vywOzATdRxKOVg4RnKwdb6M/o8+/oGcI6knSSNpzQ7dtmiz3qrOFRa41zKRbSq04EPUXae/Slf3Fviq5SzohXAIZSLm2Sz1STgBMqR9JOUo+1NfnFXnEg5YnqCcnF4ekR8vw/jd90dtpZy59dy4H1Zv9WU6xtXUIL218DinibUzd8Dh1GWeTrlgmefRcR9WZ9ZlKPVtcAyyocbygXc+7L+FwMndGuW6prO9ylfRNfndF5BWe+tcjLli/J+ynWG69jQ5PIlyrWOnwN3Ub6MGoqIrwMXUPahZynXGXbP3p8GPp7NKR/Mg4wplC/95ZSj4w+x4bujr9tkz1yva4E7KdcPj4iI72X/uZRmpv+hNPU8T+/NtT3Kz8L7KeGxMus6p9L/JuASyk0NnZSL7bBhP/hIV3k2TX0feFUP83qacov088DtlPV6NyU0ejtzmElZziWU7Tq/W//3UprynqRcD72q2/Jt6We9dl0XT80GrGw+XAWMi4hftbs+1h55hH8vsENErGt3fforn6nYgCTpzdmksDOlzf0eNlwktQFC0ltVnkcZTjnK/7YDZcs4VGygmkJpMngCGEdp4vJp+8DzHkrT58OUW+nbepF7W9DS5i+Vp8uvoDy/EZSHfB6kXMgeSzkyPD4iVkoSpf36WMrtn++KiLtyOlPZ8PMV50f+hpGkQyjtjEMod3Wc5S8GM7P2afWZysWUB872A15DuWh7NnBzRIyj3IbXdZfKMZQjxnHANOAyAEm7Uy4AHgYcCkzPU1VymNMr401u8fKYmVkvWnamImk3yt0PL6+ePUh6kHK3x1JJe1CeOn+VpC9m99eqw3W9IuI9Wf5FytPBt1KeJt4vy0+sDteTESNGxNixY2tcUjOzbdvChQufjohmniejlb+SuQ/lFsSrJL0GWEj5Fd+Rec89lFvgRmb3KDa+dXBxlvVWvrhBea/Gjh3LggUL+rwwZmYDlaSmfymhlc1fgyk/tHdZRBxEeR5howey8gym5ddAJE1T+dXZBcuXL2/17MzMBqxWhspiyo/H/TTfX0cJmaey2Yv8uyz7L2HjnyMYnWW9lY9uUP5HIuLyiJgQERM6Opo6gzMzs83QslCJiCeBRZK6nkA9ivLE6Bw2/NTAVOCG7J4DnJw/bzARWJ3NZHOBSZKG5wX6ScDc7LdG5X89iPK0cde0zMysDVr9n8feB3wlf1/pEcrv/r8EmK3yD58eo/xwHJRbgo+l/CzCczksEbFC0nmUn3SA8nPgK7L7DDbcUnxTvszMrE0G3M+0TJgwIXyh3syseZIWRsSEZob1E/VmZlYbh4qZmdXGoWJmZrVxqJiZWW1affeXWVs8fu6ftrsKA8Jen7in3VWwFxmfqZiZWW0cKmZmVhuHipmZ1cahYmZmtXGomJlZbRwqZmZWG4eKmZnVxqFiZma1caiYmVltHCpmZlYbh4qZmdXGoWJmZrVxqJiZWW0cKmZmVhuHipmZ1cahYmZmtXGomJlZbRwqZmZWG4eKmZnVxqFiZma1caiYmVltHCpmZlabloaKpEcl3SPpbkkLsmx3SfMkPZR/h2e5JF0iqVPSLyQdXJnO1Bz+IUlTK+WH5PQ7c1y1cnnMzKx3W+NM5a8i4sCImJDvzwZujohxwM35HuAYYFy+pgGXQQkhYDpwGHAoML0riHKY0yvjTW794piZWU/a0fw1Bbgmu68BjquUz4xiPjBM0h7A0cC8iFgRESuBecDk7LdrRMyPiABmVqZlZmZt0OpQCeB7khZKmpZlIyNiaXY/CYzM7lHAosq4i7Ost/LFDcrNzKxNBrd4+q+PiCWSXgbMk/TLas+ICEnR4jqQgTYNYK+99mr17MzMBqyWnqlExJL8uwz4JuWayFPZdEX+XZaDLwHGVEYfnWW9lY9uUN6oHpdHxISImNDR0bGli2VmZj1oWahI2lnS0K5uYBJwLzAH6LqDaypwQ3bPAU7Ou8AmAquzmWwuMEnS8LxAPwmYm/3WSJqYd32dXJmWmZm1QSubv0YC38y7fAcDX42I70q6E5gt6VTgMeD4HP5G4FigE3gOOAUgIlZIOg+4M4c7NyJWZPcZwNXAEOCmfJmZWZu0LFQi4hHgNQ3KnwGOalAewJk9TGsGMKNB+QLggC2urJmZ1cJP1JuZWW0cKmZmVhuHipmZ1cahYmZmtXGomJlZbRwqZmZWG4eKmZnVxqFiZma1caiYmVltHCpmZlYbh4qZmdXGoWJmZrVxqJiZWW0cKmZmVhuHipmZ1cahYmZmtXGomJlZbRwqZmZWG4eKmZnVxqFiZma1caiYmVltHCpmZlYbh4qZmdXGoWJmZrVxqJiZWW0cKmZmVhuHipmZ1abloSJpkKSfSfpOvt9H0k8ldUr6L0nbZ/kO+b4z+4+tTOOjWf6gpKMr5ZOzrFPS2a1eFjMz693WOFM5C3ig8v4zwEURsS+wEjg1y08FVmb5RTkcksYDJwD7A5OBL2RQDQI+DxwDjAdOzGHNzKxNWhoqkkYDbwKuyPcCjgSuy0GuAY7L7in5nux/VA4/BZgVEb+NiF8BncCh+eqMiEci4nfArBzWzMzapNVnKv8OfBj4Q75/KbAqItbl+8XAqOweBSwCyP6rc/j15d3G6anczMzapGWhIulvgGURsbBV8+hDXaZJWiBpwfLly9tdHTOzbVYrz1QOB94i6VFK09SRwMXAMEmDc5jRwJLsXgKMAcj+uwHPVMu7jdNT+R+JiMsjYkJETOjo6NjyJTMzs4ZaFioR8dGIGB0RYykX2n8QEf8A3AK8LQebCtyQ3XPyPdn/BxERWX5C3h22DzAOuAO4ExiXd5Ntn/OY06rlMTOzTRu86UFq9xFglqTzgZ8BV2b5lcC1kjqBFZSQICLukzQbuB9YB5wZES8ASHovMBcYBMyIiPu26pKYmdlGtkqoRMStwK3Z/Qjlzq3uwzwPvL2H8S8ALmhQfiNwY41VNTOzLeAn6s3MrDYOFTMzq41DxczMauNQMTOz2jhUzMysNg4VMzOrjUPFzMxq41AxM7PaOFTMzKw2DhUzM6uNQ8XMzGrjUDEzs9o4VMzMrDYOFTMzq41DxczMauNQMTOz2jhUzMysNg4VMzOrjUPFzMxq01SoSLq5mTIzMxvYBvfWU9KOwE7ACEnDAWWvXYFRLa6bmZn1M72GCvAe4APAnsBCNoTKGuA/WlgvMzPrh3oNlYi4GLhY0vsi4tKtVCczM+unNnWmAkBEXCrpz4Gx1XEiYmaL6mVmZv1QU6Ei6VrgFcDdwAtZHIBDxczM1msqVIAJwPiIiFZWxszM+rdmn1O5F/iTVlbEzMz6v2bPVEYA90u6A/htV2FEvKUltTIzs36p2VA5p68TzmdcbgN2yPlcFxHTJe0DzAJeSrlN+aSI+J2kHSjXaA4BngHeERGP5rQ+CpxKuZ7z/oiYm+WTgYuBQcAVEXFhX+tpZmb1afburx9uxrR/CxwZEWslbQfcLukm4P8BF0XELEn/SQmLy/LvyojYV9IJwGeAd0gaD5wA7E95Xub7kl6Z8/g88EZgMXCnpDkRcf9m1NXMzGrQ7M+0PCtpTb6el/SCpDW9jRPF2ny7Xb4COBK4LsuvAY7L7in5nux/lCRl+ayI+G1E/AroBA7NV2dEPBIRv6Oc/UxpZnnMzKw1mgqViBgaEbtGxK7AEODvgC9sajxJgyTdDSwD5gEPA6siYl0OspgNP/cyCliU81sHrKY0ka0v7zZOT+WN6jFN0gJJC5YvX97EEpuZ2ebo868U5xnIt4Cjmxj2hYg4EBhNObPYr+9V3HIRcXlETIiICR0dHe2ogpnZgNDsw49/W3n7EspzK883O5OIWCXpFuB1wDBJg/NsZDSwJAdbAowBFksaDOxGuWDfVd6lOk5P5WZm1gbNnqm8ufI6GniWTVy/kNQhaVh2D6FcUH8AuAV4Ww42Fbghu+fke7L/D/JhyznACZJ2yDvHxgF3AHcC4yTtI2l7ysX8OU0uj5mZtUCzd3+dshnT3gO4RtIgSnjNjojvSLofmCXpfOBnwJU5/JXAtZI6gRWUkCAi7pM0G7gfWAecGREvAEh6LzCXckvxjIi4bzPqaWZmNWm2+Ws0cClweBb9CDgrIhb3NE5E/AI4qEH5I5TrK93Lnwfe3sO0LgAuaFB+I3BjE4tgZmZbQbPNX1dRmpb2zNe3s8zMzGy9ZkOlIyKuioh1+boa8G1UZma2kWZD5RlJ78znTgZJeiflziwzM7P1mg2VdwPHA08CSyl3Z72rRXUyM7N+qtkflDwXmBoRKwEk7Q58lhI2ZmZmQPNnKn/WFSgAEbGCBnd2mZnZwNZsqLxE0vCuN3mm0uxZjpmZDRDNBsO/AT+R9PV8/3YaPDdiZmYDW7NP1M+UtIDys/UAf+v/W2JmZt013YSVIeIgMTOzHvX5p+/NzMx64lAxM7PaOFTMzKw2DhUzM6uNQ8XMzGrjUDEzs9o4VMzMrDb+qRUze9E5/NLDNz2QbZEfv+/HLZmuz1TMzKw2DhUzM6uNQ8XMzGrjUDEzs9o4VMzMrDYOFTMzq41DxczMauNQMTOz2jhUzMysNi0LFUljJN0i6X5J90k6K8t3lzRP0kP5d3iWS9Ilkjol/ULSwZVpTc3hH5I0tVJ+iKR7cpxLJKlVy2NmZpvWyjOVdcA/RcR4YCJwpqTxwNnAzRExDrg53wMcA4zL1zTgMighBEwHDgMOBaZ3BVEOc3plvMktXB4zM9uEloVKRCyNiLuy+1ngAWAUMAW4Jge7Bjguu6cAM6OYDwyTtAdwNDAvIlZExEpgHjA5++0aEfMjIoCZlWmZmVkbbJVrKpLGAgcBPwVGRsTS7PUkMDK7RwGLKqMtzrLeyhc3KG80/2mSFkhasHz58i1aFjMz61nLQ0XSLsD1wAciYk21X55hRKvrEBGXR8SEiJjQ0dHR6tmZmQ1YLQ0VSdtRAuUrEfGNLH4qm67Iv8uyfAkwpjL66CzrrXx0g3IzM2uTVt79JeBK4IGI+Fyl1xyg6w6uqcANlfKT8y6wicDqbCabC0ySNDwv0E8C5ma/NZIm5rxOrkzLzMzaoJX/pOtw4CTgHkl3Z9nHgAuB2ZJOBR4Djs9+NwLHAp3Ac8ApABGxQtJ5wJ053LkRsSK7zwCuBoYAN+XLzMzapGWhEhG3Az09N3JUg+EDOLOHac0AZjQoXwAcsAXVNDOzGvmJejMzq41DxczMauNQMTOz2jhUzMysNg4VMzOrjUPFzMxq41AxM7PaOFTMzKw2DhUzM6uNQ8XMzGrjUDEzs9o4VMzMrDYOFTMzq41DxczMauNQMTOz2jhUzMysNg4VMzOrjUPFzMxq41AxM7PaOFTMzKw2DhUzM6uNQ8XMzGrjUDEzs9o4VMzMrDYOFTMzq41DxczMauNQMTOz2rQsVCTNkLRM0r2Vst0lzZP0UP4dnuWSdImkTkm/kHRwZZypOfxDkqZWyg+RdE+Oc4kktWpZzMysOa08U7kamNyt7Gzg5ogYB9yc7wGOAcblaxpwGZQQAqYDhwGHAtO7giiHOb0yXvd5mZnZVtayUImI24AV3YqnANdk9zXAcZXymVHMB4ZJ2gM4GpgXESsiYiUwD5ic/XaNiPkREcDMyrTMzKxNtvY1lZERsTS7nwRGZvcoYFFluMVZ1lv54gblDUmaJmmBpAXLly/fsiUwM7Mete1CfZ5hxFaa1+URMSEiJnR0dGyNWZqZDUhbO1SeyqYr8u+yLF8CjKkMNzrLeisf3aDczMzaaGuHyhyg6w6uqcANlfKT8y6wicDqbCabC0ySNDwv0E8C5ma/NZIm5l1fJ1emZWZmbTK4VROW9DXgCGCEpMWUu7guBGZLOhV4DDg+B78ROBboBJ4DTgGIiBWSzgPuzOHOjYiui/9nUO4wGwLclC8zM2ujloVKRJzYQ6+jGgwbwJk9TGcGMKNB+QLggC2po5mZ1ctP1JuZWW0cKmZmVhuHipmZ1cahYmZmtXGomJlZbRwqZmZWG4eKmZnVxqFiZma1caiYmVltWvZE/bbgkA/NbHcVtnkL//XkdlfBzGrkMxUzM6uNQ8XMzGrjUDEzs9o4VMzMrDYOFTMzq41DxczMauNQMTOz2jhUzMysNg4VMzOrjUPFzMxq41AxM7PaOFTMzKw2DhUzM6uNQ8XMzGrjUDEzs9o4VMzMrDYOFTMzq02/DxVJkyU9KKlT0tntro+Z2UDWr0NF0iDg88AxwHjgREnj21srM7OBq1+HCnAo0BkRj0TE74BZwJQ218nMbMBSRLS7DptN0tuAyRFxWr4/CTgsIt7bbbhpwLR8+yrgwa1a0a1nBPB0uythm83br3/blrff3hHR0cyAg1tdkxeDiLgcuLzd9Wg1SQsiYkK762Gbx9uvf/P2K/p789cSYEzl/egsMzOzNujvoXInME7SPpK2B04A5rS5TmZmA1a/bv6KiHWS3gvMBQYBMyLivjZXq522+Sa+bZy3X//m7Uc/v1BvZmYvLv29+cvMzF5EHCpmZlYbh8o2QNIMScsk3dvuuljfSRoj6RZJ90u6T9JZ7a6TNUfSjpLukPTz3HafbHed2s3XVLYBkt4ArAVmRsQB7a6P9Y2kPYA9IuIuSUOBhcBxEXF/m6tmmyBJwM4RsVbSdsDtwFkRMb/NVWsbn6lsAyLiNmBFu+thmycilkbEXdn9LPAAMKq9tbJmRLE2326XrwF9pO5QMXsRkTQWOAj4aXtrYs2SNEjS3cAyYF5EDOht51Axe5GQtAtwPfCBiFjT7vpYcyLihYg4kPKLHodKGtBN0A4VsxeBbI+/HvhKRHyj3fWxvouIVcAtwOR216WdHCpmbZYXe68EHoiIz7W7PtY8SR2ShmX3EOCNwC/bW6v2cqhsAyR9DfgJ8CpJiyWd2u46WZ8cDpwEHCnp7nwd2+5KWVP2AG6R9AvKbxHOi4jvtLlObeVbis3MrDY+UzEzs9o4VMzMrDYOFTMzq41DxczMauNQMTOz2jhUzMysNg4VMzOrzf8CwbFYmsHTbuMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_predict)\n",
    "plt.title('Number of Buildings of Predicted Damage Grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# create the submission format with building_id and predicted damage_grade\n",
    "submission_df = X_test_df[['building_id']]\n",
    "submission_df['damage_grade'] = y_predict\n",
    "submission_df.to_csv('submission_standardize_RMS.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
