{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import re\n",
    "import pandas as pd\n",
    "from data_collection import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of train values and labels\n",
    "Train_values_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_values.csv\")\n",
    "train_labels_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802906</td>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>12198</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28830</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2812</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>o</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94947</td>\n",
       "      <td>21</td>\n",
       "      <td>363</td>\n",
       "      <td>8973</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>590882</td>\n",
       "      <td>22</td>\n",
       "      <td>418</td>\n",
       "      <td>10694</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201944</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>1488</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "1       802906               6             487           12198   \n",
       "2        28830               8             900            2812   \n",
       "3        94947              21             363            8973   \n",
       "4       590882              22             418           10694   \n",
       "5       201944              11             131            1488   \n",
       "\n",
       "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "1                    2   30                6                  5   \n",
       "2                    2   10                8                  7   \n",
       "3                    2   10                5                  5   \n",
       "4                    2   10                6                  5   \n",
       "5                    3   30                8                  9   \n",
       "\n",
       "  land_surface_condition foundation_type  ... has_secondary_use_agriculture  \\\n",
       "1                      t               r  ...                             0   \n",
       "2                      o               r  ...                             0   \n",
       "3                      t               r  ...                             0   \n",
       "4                      t               r  ...                             0   \n",
       "5                      t               r  ...                             0   \n",
       "\n",
       "  has_secondary_use_hotel has_secondary_use_rental  \\\n",
       "1                       0                        0   \n",
       "2                       0                        0   \n",
       "3                       0                        0   \n",
       "4                       0                        0   \n",
       "5                       0                        0   \n",
       "\n",
       "  has_secondary_use_institution has_secondary_use_school  \\\n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "5                             0                        0   \n",
       "\n",
       "   has_secondary_use_industry  has_secondary_use_health_post  \\\n",
       "1                           0                              0   \n",
       "2                           0                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "5                           0                              0   \n",
       "\n",
       "   has_secondary_use_gov_office  has_secondary_use_use_police  \\\n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "5                             0                             0   \n",
       "\n",
       "   has_secondary_use_other  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "5                        0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_values_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09640791861888481"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_labels_df['damage_grade'] ==1)/len(train_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>damage_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28830</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94947</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>590882</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201944</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>333020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>728451</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>475515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>441126</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>989500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    building_id  damage_grade\n",
       "1        802906             3\n",
       "2         28830             2\n",
       "3         94947             3\n",
       "4        590882             2\n",
       "5        201944             3\n",
       "6        333020             2\n",
       "7        728451             3\n",
       "8        475515             1\n",
       "9        441126             2\n",
       "10       989500             1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train labels'+'\\n')\n",
    "train_labels_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of test values\n",
    "Test_values_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/test_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_agriculture</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300051</td>\n",
       "      <td>17</td>\n",
       "      <td>596</td>\n",
       "      <td>11307</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99355</td>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>11987</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>890251</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>10044</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>745817</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>421793</td>\n",
       "      <td>17</td>\n",
       "      <td>289</td>\n",
       "      <td>7970</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "1       300051              17             596           11307   \n",
       "2        99355               6             141           11987   \n",
       "3       890251              22              19           10044   \n",
       "4       745817              26              39             633   \n",
       "5       421793              17             289            7970   \n",
       "\n",
       "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "1                    3   20                7                  6   \n",
       "2                    2   25               13                  5   \n",
       "3                    2    5                4                  5   \n",
       "4                    1    0               19                  3   \n",
       "5                    3   15                8                  7   \n",
       "\n",
       "  land_surface_condition foundation_type  ... has_secondary_use_agriculture  \\\n",
       "1                      t               r  ...                             0   \n",
       "2                      t               r  ...                             1   \n",
       "3                      t               r  ...                             0   \n",
       "4                      t               r  ...                             0   \n",
       "5                      t               r  ...                             0   \n",
       "\n",
       "  has_secondary_use_hotel has_secondary_use_rental  \\\n",
       "1                       0                        0   \n",
       "2                       0                        0   \n",
       "3                       0                        0   \n",
       "4                       0                        1   \n",
       "5                       0                        0   \n",
       "\n",
       "  has_secondary_use_institution has_secondary_use_school  \\\n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "5                             0                        0   \n",
       "\n",
       "   has_secondary_use_industry  has_secondary_use_health_post  \\\n",
       "1                           0                              0   \n",
       "2                           0                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "5                           0                              0   \n",
       "\n",
       "   has_secondary_use_gov_office  has_secondary_use_use_police  \\\n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "5                             0                             0   \n",
       "\n",
       "   has_secondary_use_other  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "5                        0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_values_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "building_id                                  int64\n",
       "geo_level_1_id                               int64\n",
       "geo_level_2_id                               int64\n",
       "geo_level_3_id                               int64\n",
       "count_floors_pre_eq                          int64\n",
       "age                                          int64\n",
       "area_percentage                              int64\n",
       "height_percentage                            int64\n",
       "land_surface_condition                    category\n",
       "foundation_type                           category\n",
       "roof_type                                 category\n",
       "ground_floor_type                         category\n",
       "other_floor_type                          category\n",
       "position                                  category\n",
       "plan_configuration                        category\n",
       "has_superstructure_adobe_mud                 int64\n",
       "has_superstructure_mud_mortar_stone          int64\n",
       "has_superstructure_stone_flag                int64\n",
       "has_superstructure_cement_mortar_stone       int64\n",
       "has_superstructure_mud_mortar_brick          int64\n",
       "has_superstructure_cement_mortar_brick       int64\n",
       "has_superstructure_timber                    int64\n",
       "has_superstructure_bamboo                    int64\n",
       "has_superstructure_rc_non_engineered         int64\n",
       "has_superstructure_rc_engineered             int64\n",
       "has_superstructure_other                     int64\n",
       "legal_ownership_status                    category\n",
       "count_families                               int64\n",
       "has_secondary_use                            int64\n",
       "has_secondary_use_agriculture                int64\n",
       "has_secondary_use_hotel                      int64\n",
       "has_secondary_use_rental                     int64\n",
       "has_secondary_use_institution                int64\n",
       "has_secondary_use_school                     int64\n",
       "has_secondary_use_industry                   int64\n",
       "has_secondary_use_health_post                int64\n",
       "has_secondary_use_gov_office                 int64\n",
       "has_secondary_use_use_police                 int64\n",
       "has_secondary_use_other                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_values_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "      <td>260601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.900353</td>\n",
       "      <td>701.074685</td>\n",
       "      <td>6257.876148</td>\n",
       "      <td>2.129723</td>\n",
       "      <td>26.535029</td>\n",
       "      <td>8.018051</td>\n",
       "      <td>5.434365</td>\n",
       "      <td>0.088645</td>\n",
       "      <td>0.761935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.033617</td>\n",
       "      <td>412.710734</td>\n",
       "      <td>3646.369645</td>\n",
       "      <td>0.727665</td>\n",
       "      <td>73.565937</td>\n",
       "      <td>4.392231</td>\n",
       "      <td>1.918418</td>\n",
       "      <td>0.284231</td>\n",
       "      <td>0.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>3073.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>6270.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>9412.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>1427.000000</td>\n",
       "      <td>12567.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>995.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       geo_level_1_id  geo_level_2_id  geo_level_3_id  count_floors_pre_eq  \\\n",
       "count   260601.000000   260601.000000   260601.000000        260601.000000   \n",
       "mean        13.900353      701.074685     6257.876148             2.129723   \n",
       "std          8.033617      412.710734     3646.369645             0.727665   \n",
       "min          0.000000        0.000000        0.000000             1.000000   \n",
       "25%          7.000000      350.000000     3073.000000             2.000000   \n",
       "50%         12.000000      702.000000     6270.000000             2.000000   \n",
       "75%         21.000000     1050.000000     9412.000000             2.000000   \n",
       "max         30.000000     1427.000000    12567.000000             9.000000   \n",
       "\n",
       "                 age  area_percentage  height_percentage  \\\n",
       "count  260601.000000    260601.000000      260601.000000   \n",
       "mean       26.535029         8.018051           5.434365   \n",
       "std        73.565937         4.392231           1.918418   \n",
       "min         0.000000         1.000000           2.000000   \n",
       "25%        10.000000         5.000000           4.000000   \n",
       "50%        15.000000         7.000000           5.000000   \n",
       "75%        30.000000         9.000000           6.000000   \n",
       "max       995.000000       100.000000          32.000000   \n",
       "\n",
       "       has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  \n",
       "count                 260601.000000                        260601.000000  \n",
       "mean                       0.088645                             0.761935  \n",
       "std                        0.284231                             0.425900  \n",
       "min                        0.000000                             0.000000  \n",
       "25%                        0.000000                             1.000000  \n",
       "50%                        0.000000                             1.000000  \n",
       "75%                        0.000000                             1.000000  \n",
       "max                        1.000000                             1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_values_df.describe().iloc[:,1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dummies for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_dummies = pd.get_dummies(Train_values_df,drop_first=True)\n",
    "Test_dummies = pd.get_dummies(Test_values_df,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>...</th>\n",
       "      <th>plan_configuration_f</th>\n",
       "      <th>plan_configuration_m</th>\n",
       "      <th>plan_configuration_n</th>\n",
       "      <th>plan_configuration_o</th>\n",
       "      <th>plan_configuration_q</th>\n",
       "      <th>plan_configuration_s</th>\n",
       "      <th>plan_configuration_u</th>\n",
       "      <th>legal_ownership_status_r</th>\n",
       "      <th>legal_ownership_status_v</th>\n",
       "      <th>legal_ownership_status_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802906</td>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>12198</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28830</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2812</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94947</td>\n",
       "      <td>21</td>\n",
       "      <td>363</td>\n",
       "      <td>8973</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>590882</td>\n",
       "      <td>22</td>\n",
       "      <td>418</td>\n",
       "      <td>10694</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201944</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>1488</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "1       802906               6             487           12198   \n",
       "2        28830               8             900            2812   \n",
       "3        94947              21             363            8973   \n",
       "4       590882              22             418           10694   \n",
       "5       201944              11             131            1488   \n",
       "\n",
       "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "1                    2   30                6                  5   \n",
       "2                    2   10                8                  7   \n",
       "3                    2   10                5                  5   \n",
       "4                    2   10                6                  5   \n",
       "5                    3   30                8                  9   \n",
       "\n",
       "   has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  ...  \\\n",
       "1                             1                                    1  ...   \n",
       "2                             0                                    1  ...   \n",
       "3                             0                                    1  ...   \n",
       "4                             0                                    1  ...   \n",
       "5                             1                                    0  ...   \n",
       "\n",
       "   plan_configuration_f  plan_configuration_m  plan_configuration_n  \\\n",
       "1                     0                     0                     0   \n",
       "2                     0                     0                     0   \n",
       "3                     0                     0                     0   \n",
       "4                     0                     0                     0   \n",
       "5                     0                     0                     0   \n",
       "\n",
       "   plan_configuration_o  plan_configuration_q  plan_configuration_s  \\\n",
       "1                     0                     0                     0   \n",
       "2                     0                     0                     0   \n",
       "3                     0                     0                     0   \n",
       "4                     0                     0                     0   \n",
       "5                     0                     0                     0   \n",
       "\n",
       "   plan_configuration_u  legal_ownership_status_r  legal_ownership_status_v  \\\n",
       "1                     0                         0                         1   \n",
       "2                     0                         0                         1   \n",
       "3                     0                         0                         1   \n",
       "4                     0                         0                         1   \n",
       "5                     0                         0                         1   \n",
       "\n",
       "   legal_ownership_status_w  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "5                         0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>...</th>\n",
       "      <th>plan_configuration_f</th>\n",
       "      <th>plan_configuration_m</th>\n",
       "      <th>plan_configuration_n</th>\n",
       "      <th>plan_configuration_o</th>\n",
       "      <th>plan_configuration_q</th>\n",
       "      <th>plan_configuration_s</th>\n",
       "      <th>plan_configuration_u</th>\n",
       "      <th>legal_ownership_status_r</th>\n",
       "      <th>legal_ownership_status_v</th>\n",
       "      <th>legal_ownership_status_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300051</td>\n",
       "      <td>17</td>\n",
       "      <td>596</td>\n",
       "      <td>11307</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99355</td>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>11987</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>890251</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>10044</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>745817</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>421793</td>\n",
       "      <td>17</td>\n",
       "      <td>289</td>\n",
       "      <td>7970</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "1       300051              17             596           11307   \n",
       "2        99355               6             141           11987   \n",
       "3       890251              22              19           10044   \n",
       "4       745817              26              39             633   \n",
       "5       421793              17             289            7970   \n",
       "\n",
       "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "1                    3   20                7                  6   \n",
       "2                    2   25               13                  5   \n",
       "3                    2    5                4                  5   \n",
       "4                    1    0               19                  3   \n",
       "5                    3   15                8                  7   \n",
       "\n",
       "   has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  ...  \\\n",
       "1                             0                                    1  ...   \n",
       "2                             0                                    1  ...   \n",
       "3                             0                                    1  ...   \n",
       "4                             0                                    0  ...   \n",
       "5                             0                                    1  ...   \n",
       "\n",
       "   plan_configuration_f  plan_configuration_m  plan_configuration_n  \\\n",
       "1                     0                     0                     0   \n",
       "2                     0                     0                     0   \n",
       "3                     0                     0                     0   \n",
       "4                     0                     0                     0   \n",
       "5                     0                     0                     0   \n",
       "\n",
       "   plan_configuration_o  plan_configuration_q  plan_configuration_s  \\\n",
       "1                     0                     0                     0   \n",
       "2                     0                     0                     0   \n",
       "3                     0                     0                     0   \n",
       "4                     0                     0                     0   \n",
       "5                     0                     0                     0   \n",
       "\n",
       "   plan_configuration_u  legal_ownership_status_r  legal_ownership_status_v  \\\n",
       "1                     0                         0                         1   \n",
       "2                     0                         0                         1   \n",
       "3                     0                         0                         1   \n",
       "4                     0                         0                         1   \n",
       "5                     0                         0                         1   \n",
       "\n",
       "   legal_ownership_status_w  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "5                         0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id',\n",
       "       'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
       "       'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
       "       'has_superstructure_stone_flag',\n",
       "       'has_superstructure_cement_mortar_stone',\n",
       "       'has_superstructure_mud_mortar_brick',\n",
       "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
       "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
       "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
       "       'count_families', 'has_secondary_use', 'has_secondary_use_agriculture',\n",
       "       'has_secondary_use_hotel', 'has_secondary_use_rental',\n",
       "       'has_secondary_use_institution', 'has_secondary_use_school',\n",
       "       'has_secondary_use_industry', 'has_secondary_use_health_post',\n",
       "       'has_secondary_use_gov_office', 'has_secondary_use_use_police',\n",
       "       'has_secondary_use_other', 'land_surface_condition_o',\n",
       "       'land_surface_condition_t', 'foundation_type_i', 'foundation_type_r',\n",
       "       'foundation_type_u', 'foundation_type_w', 'roof_type_q', 'roof_type_x',\n",
       "       'ground_floor_type_m', 'ground_floor_type_v', 'ground_floor_type_x',\n",
       "       'ground_floor_type_z', 'other_floor_type_q', 'other_floor_type_s',\n",
       "       'other_floor_type_x', 'position_o', 'position_s', 'position_t',\n",
       "       'plan_configuration_c', 'plan_configuration_d', 'plan_configuration_f',\n",
       "       'plan_configuration_m', 'plan_configuration_n', 'plan_configuration_o',\n",
       "       'plan_configuration_q', 'plan_configuration_s', 'plan_configuration_u',\n",
       "       'legal_ownership_status_r', 'legal_ownership_status_v',\n",
       "       'legal_ownership_status_w'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foundation_type_i</th>\n",
       "      <th>foundation_type_r</th>\n",
       "      <th>foundation_type_u</th>\n",
       "      <th>foundation_type_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    foundation_type_i  foundation_type_r  foundation_type_u  foundation_type_w\n",
       "1                 0.0                1.0                0.0                0.0\n",
       "2                 0.0                1.0                0.0                0.0\n",
       "3                 0.0                1.0                0.0                0.0\n",
       "4                 0.0                1.0                0.0                0.0\n",
       "5                 0.0                1.0                0.0                0.0\n",
       "6                 0.0                1.0                0.0                0.0\n",
       "7                 0.0                1.0                0.0                0.0\n",
       "8                 0.0                0.0                0.0                1.0\n",
       "9                 0.0                1.0                0.0                0.0\n",
       "10                1.0                0.0                0.0                0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_dummies.iloc[:,32:36].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[-0.20569948 -2.30085854 -0.24059767  4.02961487]\n",
      "[-0.20569948  0.43462037 -0.24059767 -0.24816267]\n",
      "[ 4.86146098 -2.30085854 -0.24059767 -0.24816267]\n"
     ]
    }
   ],
   "source": [
    "X_foundation_std = []\n",
    "for i in range(10):\n",
    "    print(X[i][32:36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding for Train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels_df['damage_grade'])\n",
    "y_labels = keras.utils.to_categorical(train_labels-1, num_classes=3,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260601, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_dummies = Train_dummies.drop(labels=['building_id'],axis=1).astype(float)\n",
    "Test_dummies = Test_dummies.drop(labels=['building_id'],axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_df = Train_dummies\n",
    "y = train_labels\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_df,y)\n",
    "#print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_df.columns)\n",
    "ft = feat_importances\n",
    "num = ft.where(ft>0.005).count()\n",
    "\n",
    "ft.nlargest(num).plot(kind='barh')\n",
    "ft_importance_cols = ft.nlargest(num).index\n",
    "plt.title('Feature Importance')\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_importance = Train_dummies[ft_importance_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_importance = Test_dummies[ft_importance_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler \n",
    "#scaler = StandardScaler().fit(Train_dummies)\n",
    "scaler = StandardScaler().fit(Train_importance)\n",
    "# Scale the train set\n",
    "#X = scaler.transform(Train_dummies)\n",
    "X = scaler.transform(Train_importance)\n",
    "# Scale the test set\n",
    "#X_test = scaler.transform(Test_dummies)\n",
    "X_test = scaler.transform(Test_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((260601, 27), (86868, 27))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_foundation_std = []\n",
    "for i in range(len(X)):\n",
    "    X_foundation_std.append(X[i][32:36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>plan_configuration_f</th>\n",
       "      <th>plan_configuration_m</th>\n",
       "      <th>plan_configuration_n</th>\n",
       "      <th>plan_configuration_o</th>\n",
       "      <th>plan_configuration_q</th>\n",
       "      <th>plan_configuration_s</th>\n",
       "      <th>plan_configuration_u</th>\n",
       "      <th>legal_ownership_status_r</th>\n",
       "      <th>legal_ownership_status_v</th>\n",
       "      <th>legal_ownership_status_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>12198.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2812.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>8973.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>10694.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   geo_level_1_id  geo_level_2_id  geo_level_3_id  count_floors_pre_eq   age  \\\n",
       "1             6.0           487.0         12198.0                  2.0  30.0   \n",
       "2             8.0           900.0          2812.0                  2.0  10.0   \n",
       "3            21.0           363.0          8973.0                  2.0  10.0   \n",
       "4            22.0           418.0         10694.0                  2.0  10.0   \n",
       "5            11.0           131.0          1488.0                  3.0  30.0   \n",
       "\n",
       "   area_percentage  height_percentage  has_superstructure_adobe_mud  \\\n",
       "1              6.0                5.0                           1.0   \n",
       "2              8.0                7.0                           0.0   \n",
       "3              5.0                5.0                           0.0   \n",
       "4              6.0                5.0                           0.0   \n",
       "5              8.0                9.0                           1.0   \n",
       "\n",
       "   has_superstructure_mud_mortar_stone  has_superstructure_stone_flag  ...  \\\n",
       "1                                  1.0                            0.0  ...   \n",
       "2                                  1.0                            0.0  ...   \n",
       "3                                  1.0                            0.0  ...   \n",
       "4                                  1.0                            0.0  ...   \n",
       "5                                  0.0                            0.0  ...   \n",
       "\n",
       "   plan_configuration_f  plan_configuration_m  plan_configuration_n  \\\n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "5                   0.0                   0.0                   0.0   \n",
       "\n",
       "   plan_configuration_o  plan_configuration_q  plan_configuration_s  \\\n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "5                   0.0                   0.0                   0.0   \n",
       "\n",
       "   plan_configuration_u  legal_ownership_status_r  legal_ownership_status_v  \\\n",
       "1                   0.0                       0.0                       1.0   \n",
       "2                   0.0                       0.0                       1.0   \n",
       "3                   0.0                       0.0                       1.0   \n",
       "4                   0.0                       0.0                       1.0   \n",
       "5                   0.0                       0.0                       1.0   \n",
       "\n",
       "   legal_ownership_status_w  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "5                       0.0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foundation_type_i</th>\n",
       "      <th>foundation_type_r</th>\n",
       "      <th>foundation_type_u</th>\n",
       "      <th>foundation_type_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.606010e+05</td>\n",
       "      <td>2.606010e+05</td>\n",
       "      <td>2.606010e+05</td>\n",
       "      <td>2.606010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.453108e-19</td>\n",
       "      <td>-2.595680e-17</td>\n",
       "      <td>-2.475711e-17</td>\n",
       "      <td>1.145698e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000002e+00</td>\n",
       "      <td>1.000002e+00</td>\n",
       "      <td>1.000002e+00</td>\n",
       "      <td>1.000002e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.056995e-01</td>\n",
       "      <td>-2.300859e+00</td>\n",
       "      <td>-2.405977e-01</td>\n",
       "      <td>-2.481627e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.056995e-01</td>\n",
       "      <td>4.346204e-01</td>\n",
       "      <td>-2.405977e-01</td>\n",
       "      <td>-2.481627e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.056995e-01</td>\n",
       "      <td>4.346204e-01</td>\n",
       "      <td>-2.405977e-01</td>\n",
       "      <td>-2.481627e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-2.056995e-01</td>\n",
       "      <td>4.346204e-01</td>\n",
       "      <td>-2.405977e-01</td>\n",
       "      <td>-2.481627e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.861461e+00</td>\n",
       "      <td>4.346204e-01</td>\n",
       "      <td>4.156316e+00</td>\n",
       "      <td>4.029615e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       foundation_type_i  foundation_type_r  foundation_type_u  \\\n",
       "count       2.606010e+05       2.606010e+05       2.606010e+05   \n",
       "mean        5.453108e-19      -2.595680e-17      -2.475711e-17   \n",
       "std         1.000002e+00       1.000002e+00       1.000002e+00   \n",
       "min        -2.056995e-01      -2.300859e+00      -2.405977e-01   \n",
       "25%        -2.056995e-01       4.346204e-01      -2.405977e-01   \n",
       "50%        -2.056995e-01       4.346204e-01      -2.405977e-01   \n",
       "75%        -2.056995e-01       4.346204e-01      -2.405977e-01   \n",
       "max         4.861461e+00       4.346204e-01       4.156316e+00   \n",
       "\n",
       "       foundation_type_w  \n",
       "count       2.606010e+05  \n",
       "mean        1.145698e-16  \n",
       "std         1.000002e+00  \n",
       "min        -2.481627e-01  \n",
       "25%        -2.481627e-01  \n",
       "50%        -2.481627e-01  \n",
       "75%        -2.481627e-01  \n",
       "max         4.029615e+00  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names=['foundation_type_i', 'foundation_type_r',\n",
    "       'foundation_type_u', 'foundation_type_w']\n",
    "X_foundation_df = pd.DataFrame(X_foundation_std,columns=col_names)\n",
    "X_foundation_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Validate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X, y_labels, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208480, 27), (52121, 27))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208480, 3), (52121, 3))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import *\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback code for monitoring learning rate\n",
    "class lr_mon(keras.callbacks.Callback):\n",
    "     def on_epoch_end(self, epoch, logs=None):\n",
    "        print(' ### Learnig rate at the end of epoch {} is {} \\n'.format(\n",
    "            epoch, K.eval(self.model.optimizer.lr) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/90\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.7836 - acc: 0.6045 - f1_m: 0.5866 - val_loss: 0.7579 - val_acc: 0.6296 - val_f1_m: 0.6215\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.7377 - acc: 0.6415 - f1_m: 0.6345 - val_loss: 0.7277 - val_acc: 0.6532 - val_f1_m: 0.6455\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.7215 - acc: 0.6536 - f1_m: 0.6478 - val_loss: 0.7190 - val_acc: 0.6610 - val_f1_m: 0.6562\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.7146 - acc: 0.6595 - f1_m: 0.6546 - val_loss: 0.7200 - val_acc: 0.6553 - val_f1_m: 0.6504\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.7082 - acc: 0.6644 - f1_m: 0.6598 - val_loss: 0.7142 - val_acc: 0.6651 - val_f1_m: 0.6610\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.7027 - acc: 0.6680 - f1_m: 0.6641 - val_loss: 0.7042 - val_acc: 0.6707 - val_f1_m: 0.6651\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/90\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6983 - acc: 0.6699 - f1_m: 0.6661 - val_loss: 0.7166 - val_acc: 0.6538 - val_f1_m: 0.6512\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.6936 - acc: 0.6735 - f1_m: 0.6702 - val_loss: 0.7036 - val_acc: 0.6658 - val_f1_m: 0.6623\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/90\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.6897 - acc: 0.6763 - f1_m: 0.6728 - val_loss: 0.6983 - val_acc: 0.6726 - val_f1_m: 0.6681\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.6854 - acc: 0.6781 - f1_m: 0.6747 - val_loss: 0.6930 - val_acc: 0.6791 - val_f1_m: 0.6756\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.6809 - acc: 0.6809 - f1_m: 0.6775 - val_loss: 0.6904 - val_acc: 0.6761 - val_f1_m: 0.6728\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/90\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.6785 - acc: 0.6822 - f1_m: 0.6788 - val_loss: 0.6900 - val_acc: 0.6744 - val_f1_m: 0.6716\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.6746 - acc: 0.6849 - f1_m: 0.6816 - val_loss: 0.6888 - val_acc: 0.6782 - val_f1_m: 0.6753\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.6721 - acc: 0.6867 - f1_m: 0.6834 - val_loss: 0.6844 - val_acc: 0.6824 - val_f1_m: 0.6795\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.6688 - acc: 0.6880 - f1_m: 0.6848 - val_loss: 0.6865 - val_acc: 0.6815 - val_f1_m: 0.6795\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/90\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6674 - acc: 0.6884 - f1_m: 0.6850 - val_loss: 0.6797 - val_acc: 0.6840 - val_f1_m: 0.6810\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/90\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6641 - acc: 0.6902 - f1_m: 0.6870 - val_loss: 0.6755 - val_acc: 0.6879 - val_f1_m: 0.6851\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/90\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6610 - acc: 0.6932 - f1_m: 0.6898 - val_loss: 0.6845 - val_acc: 0.6829 - val_f1_m: 0.6804\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.6580 - acc: 0.6944 - f1_m: 0.6914 - val_loss: 0.6781 - val_acc: 0.6867 - val_f1_m: 0.6838\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.6568 - acc: 0.6943 - f1_m: 0.6912 - val_loss: 0.6752 - val_acc: 0.6880 - val_f1_m: 0.6851\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.6542 - acc: 0.6963 - f1_m: 0.6935 - val_loss: 0.6796 - val_acc: 0.6851 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.6512 - acc: 0.6983 - f1_m: 0.6954 - val_loss: 0.6794 - val_acc: 0.6870 - val_f1_m: 0.6840\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.6479 - acc: 0.6998 - f1_m: 0.6974 - val_loss: 0.6724 - val_acc: 0.6902 - val_f1_m: 0.6872\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.6462 - acc: 0.7004 - f1_m: 0.6976 - val_loss: 0.6723 - val_acc: 0.6908 - val_f1_m: 0.6866\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/90\n",
      "208480/208480 [==============================] - 4s 18us/step - loss: 0.6436 - acc: 0.7020 - f1_m: 0.6991 - val_loss: 0.6806 - val_acc: 0.6825 - val_f1_m: 0.6809\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.6407 - acc: 0.7034 - f1_m: 0.7005 - val_loss: 0.6693 - val_acc: 0.6920 - val_f1_m: 0.6886\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6389 - acc: 0.7041 - f1_m: 0.7014 - val_loss: 0.6767 - val_acc: 0.6904 - val_f1_m: 0.6882\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6368 - acc: 0.7054 - f1_m: 0.7026 - val_loss: 0.6725 - val_acc: 0.6937 - val_f1_m: 0.6905\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6341 - acc: 0.7062 - f1_m: 0.7039 - val_loss: 0.6749 - val_acc: 0.6898 - val_f1_m: 0.6856\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6313 - acc: 0.7084 - f1_m: 0.7055 - val_loss: 0.6764 - val_acc: 0.6889 - val_f1_m: 0.6865\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6288 - acc: 0.7082 - f1_m: 0.7056 - val_loss: 0.6709 - val_acc: 0.6933 - val_f1_m: 0.6898\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.6255 - acc: 0.7110 - f1_m: 0.7086 - val_loss: 0.6722 - val_acc: 0.6909 - val_f1_m: 0.6870\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.6236 - acc: 0.7118 - f1_m: 0.7094 - val_loss: 0.6682 - val_acc: 0.6961 - val_f1_m: 0.6932\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6213 - acc: 0.7120 - f1_m: 0.7096 - val_loss: 0.6772 - val_acc: 0.6878 - val_f1_m: 0.6845\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6187 - acc: 0.7139 - f1_m: 0.7112 - val_loss: 0.6803 - val_acc: 0.6862 - val_f1_m: 0.6832\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6165 - acc: 0.7155 - f1_m: 0.7129 - val_loss: 0.6748 - val_acc: 0.6911 - val_f1_m: 0.6879\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6138 - acc: 0.7167 - f1_m: 0.7139 - val_loss: 0.6717 - val_acc: 0.6928 - val_f1_m: 0.6908\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6113 - acc: 0.7184 - f1_m: 0.7159 - val_loss: 0.6777 - val_acc: 0.6912 - val_f1_m: 0.6884\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6094 - acc: 0.7192 - f1_m: 0.7167 - val_loss: 0.6724 - val_acc: 0.6945 - val_f1_m: 0.6917\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6065 - acc: 0.7197 - f1_m: 0.7169 - val_loss: 0.6825 - val_acc: 0.6917 - val_f1_m: 0.6894\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.6030 - acc: 0.7207 - f1_m: 0.7188 - val_loss: 0.6765 - val_acc: 0.6921 - val_f1_m: 0.6892\n",
      " ### Learnig rate at the end of epoch 40 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 42/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.6016 - acc: 0.7221 - f1_m: 0.7197 - val_loss: 0.6800 - val_acc: 0.6903 - val_f1_m: 0.6876\n",
      " ### Learnig rate at the end of epoch 41 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 43/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.5990 - acc: 0.7235 - f1_m: 0.7211 - val_loss: 0.6812 - val_acc: 0.6941 - val_f1_m: 0.6905\n",
      " ### Learnig rate at the end of epoch 42 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 44/90\n",
      "208480/208480 [==============================] - 3s 17us/step - loss: 0.5694 - acc: 0.7382 - f1_m: 0.7364 - val_loss: 0.6761 - val_acc: 0.6986 - val_f1_m: 0.6961\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/90\n",
      "208480/208480 [==============================] - 4s 17us/step - loss: 0.5625 - acc: 0.7419 - f1_m: 0.7400 - val_loss: 0.6776 - val_acc: 0.6989 - val_f1_m: 0.6969\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/90\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5596 - acc: 0.7432 - f1_m: 0.7415 - val_loss: 0.6812 - val_acc: 0.6992 - val_f1_m: 0.6970\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5575 - acc: 0.7443 - f1_m: 0.7425 - val_loss: 0.6807 - val_acc: 0.6992 - val_f1_m: 0.6973\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5556 - acc: 0.7450 - f1_m: 0.7431 - val_loss: 0.6829 - val_acc: 0.6993 - val_f1_m: 0.6967\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5541 - acc: 0.7459 - f1_m: 0.7442 - val_loss: 0.6851 - val_acc: 0.6995 - val_f1_m: 0.6975\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5525 - acc: 0.7467 - f1_m: 0.7448 - val_loss: 0.6871 - val_acc: 0.7003 - val_f1_m: 0.6982\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 51/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5512 - acc: 0.7472 - f1_m: 0.7454 - val_loss: 0.6886 - val_acc: 0.6996 - val_f1_m: 0.6975\n",
      " ### Learnig rate at the end of epoch 50 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 52/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5498 - acc: 0.7478 - f1_m: 0.7463 - val_loss: 0.6879 - val_acc: 0.6988 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 51 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 53/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5485 - acc: 0.7486 - f1_m: 0.7471 - val_loss: 0.6903 - val_acc: 0.6998 - val_f1_m: 0.6973\n",
      " ### Learnig rate at the end of epoch 52 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 54/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5474 - acc: 0.7491 - f1_m: 0.7474 - val_loss: 0.6909 - val_acc: 0.6988 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 53 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 55/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5461 - acc: 0.7498 - f1_m: 0.7481 - val_loss: 0.6958 - val_acc: 0.6989 - val_f1_m: 0.6971\n",
      " ### Learnig rate at the end of epoch 54 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 56/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5450 - acc: 0.7510 - f1_m: 0.7493 - val_loss: 0.6949 - val_acc: 0.7004 - val_f1_m: 0.6982\n",
      " ### Learnig rate at the end of epoch 55 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 57/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5439 - acc: 0.7511 - f1_m: 0.7495 - val_loss: 0.6923 - val_acc: 0.6979 - val_f1_m: 0.6951\n",
      " ### Learnig rate at the end of epoch 56 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 58/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5427 - acc: 0.7515 - f1_m: 0.7496 - val_loss: 0.6961 - val_acc: 0.6998 - val_f1_m: 0.6973\n",
      " ### Learnig rate at the end of epoch 57 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 59/90\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5417 - acc: 0.7523 - f1_m: 0.7506 - val_loss: 0.6980 - val_acc: 0.6972 - val_f1_m: 0.6947\n",
      " ### Learnig rate at the end of epoch 58 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 60/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5405 - acc: 0.7527 - f1_m: 0.7512 - val_loss: 0.7014 - val_acc: 0.6988 - val_f1_m: 0.6970\n",
      " ### Learnig rate at the end of epoch 59 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 61/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5396 - acc: 0.7536 - f1_m: 0.7518 - val_loss: 0.7008 - val_acc: 0.6995 - val_f1_m: 0.6973\n",
      " ### Learnig rate at the end of epoch 60 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 62/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5385 - acc: 0.7534 - f1_m: 0.7516 - val_loss: 0.7013 - val_acc: 0.6990 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 61 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 63/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5375 - acc: 0.7546 - f1_m: 0.7527 - val_loss: 0.7036 - val_acc: 0.6986 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 62 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 64/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5365 - acc: 0.7556 - f1_m: 0.7539 - val_loss: 0.7056 - val_acc: 0.6973 - val_f1_m: 0.6949\n",
      " ### Learnig rate at the end of epoch 63 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 65/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5355 - acc: 0.7553 - f1_m: 0.7539 - val_loss: 0.7054 - val_acc: 0.6968 - val_f1_m: 0.6944\n",
      " ### Learnig rate at the end of epoch 64 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 66/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5344 - acc: 0.7559 - f1_m: 0.7544 - val_loss: 0.7119 - val_acc: 0.6973 - val_f1_m: 0.6952\n",
      " ### Learnig rate at the end of epoch 65 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 67/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5287 - acc: 0.7589 - f1_m: 0.7572 - val_loss: 0.7082 - val_acc: 0.6991 - val_f1_m: 0.6967\n",
      " ### Learnig rate at the end of epoch 66 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 68/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5279 - acc: 0.7594 - f1_m: 0.7579 - val_loss: 0.7086 - val_acc: 0.6989 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 67 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 69/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5276 - acc: 0.7597 - f1_m: 0.7580 - val_loss: 0.7097 - val_acc: 0.6988 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 68 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 70/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5274 - acc: 0.7597 - f1_m: 0.7581 - val_loss: 0.7093 - val_acc: 0.6990 - val_f1_m: 0.6967\n",
      " ### Learnig rate at the end of epoch 69 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 71/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5272 - acc: 0.7598 - f1_m: 0.7582 - val_loss: 0.7101 - val_acc: 0.6988 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 70 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 72/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5271 - acc: 0.7599 - f1_m: 0.7582 - val_loss: 0.7098 - val_acc: 0.6991 - val_f1_m: 0.6964\n",
      " ### Learnig rate at the end of epoch 71 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 73/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5269 - acc: 0.7602 - f1_m: 0.7585 - val_loss: 0.7108 - val_acc: 0.6985 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 72 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 74/90\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5268 - acc: 0.7602 - f1_m: 0.7585 - val_loss: 0.7112 - val_acc: 0.6990 - val_f1_m: 0.6964\n",
      " ### Learnig rate at the end of epoch 73 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 75/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5266 - acc: 0.7602 - f1_m: 0.7585 - val_loss: 0.7110 - val_acc: 0.6985 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 74 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 76/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5265 - acc: 0.7604 - f1_m: 0.7587 - val_loss: 0.7110 - val_acc: 0.6988 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 75 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 77/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5257 - acc: 0.7609 - f1_m: 0.7592 - val_loss: 0.7113 - val_acc: 0.6987 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 76 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 78/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5256 - acc: 0.7609 - f1_m: 0.7592 - val_loss: 0.7114 - val_acc: 0.6986 - val_f1_m: 0.6961\n",
      " ### Learnig rate at the end of epoch 77 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 79/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5256 - acc: 0.7608 - f1_m: 0.7592 - val_loss: 0.7115 - val_acc: 0.6986 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 78 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 80/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5256 - acc: 0.7608 - f1_m: 0.7592 - val_loss: 0.7116 - val_acc: 0.6986 - val_f1_m: 0.6962\n",
      " ### Learnig rate at the end of epoch 79 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 81/90\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.5256 - acc: 0.7609 - f1_m: 0.7592 - val_loss: 0.7116 - val_acc: 0.6987 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 80 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 82/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5256 - acc: 0.7609 - f1_m: 0.7592 - val_loss: 0.7116 - val_acc: 0.6987 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 81 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 83/90\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.5255 - acc: 0.7609 - f1_m: 0.7592 - val_loss: 0.7116 - val_acc: 0.6987 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 82 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 84/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5255 - acc: 0.7609 - f1_m: 0.7592 - val_loss: 0.7116 - val_acc: 0.6986 - val_f1_m: 0.6964\n",
      " ### Learnig rate at the end of epoch 83 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 85/90\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5255 - acc: 0.7609 - f1_m: 0.7592 - val_loss: 0.7117 - val_acc: 0.6986 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 84 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 86/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5255 - acc: 0.7609 - f1_m: 0.7593 - val_loss: 0.7117 - val_acc: 0.6988 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 85 is 2.000000222324161e-06 \n",
      "\n",
      "Epoch 87/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5254 - acc: 0.7610 - f1_m: 0.7593 - val_loss: 0.7117 - val_acc: 0.6987 - val_f1_m: 0.6964\n",
      " ### Learnig rate at the end of epoch 86 is 2.000000165480742e-07 \n",
      "\n",
      "Epoch 88/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5254 - acc: 0.7609 - f1_m: 0.7593 - val_loss: 0.7117 - val_acc: 0.6987 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 87 is 2.000000165480742e-07 \n",
      "\n",
      "Epoch 89/90\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5254 - acc: 0.7610 - f1_m: 0.7593 - val_loss: 0.7117 - val_acc: 0.6987 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 88 is 2.000000165480742e-07 \n",
      "\n",
      "Epoch 90/90\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.5254 - acc: 0.7610 - f1_m: 0.7593 - val_loss: 0.7117 - val_acc: 0.6987 - val_f1_m: 0.6963\n",
      " ### Learnig rate at the end of epoch 89 is 2.000000165480742e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # building the model\n",
    "# basic_activation = ['elu','selu','relu','tanh','sigmoid']\n",
    "# adv_activation = [LeakyReLU(alpha=0.3),ELU(alpha=1.0),ThresholdedReLU(theta=1.0),\n",
    "#                   ReLU(max_value=None, negative_slope=0.0, threshold=0.0)]\n",
    "\n",
    "# best_activations = ['relu','tanh',ReLU(max_value=None, negative_slope=0.0, threshold=0.0),'sigmoid']\n",
    "\n",
    "\n",
    "\n",
    "a = ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
    "lr_call = lr_mon()\n",
    "lr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=0, \n",
    "                                               mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=128, activation='sigmoid', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=128, activation=a, input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=128, activation=a, input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=128, activation=a, input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=128, activation=a, input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=128, activation=a, input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=128, activation=a, input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy',f1_m])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=90, batch_size=128,validation_data=(X_val,y_val),\n",
    "                    callbacks=[lr_call,lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu\n",
      "3\n",
      "3\n",
      "3\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 9s 41us/step - loss: 0.7569 - acc: 0.6298 - f1_m: 0.6168 - val_loss: 0.7316 - val_acc: 0.6458 - val_f1_m: 0.6407\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.7137 - acc: 0.6585 - f1_m: 0.6527 - val_loss: 0.7128 - val_acc: 0.6607 - val_f1_m: 0.6551\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.7014 - acc: 0.6674 - f1_m: 0.6627 - val_loss: 0.7080 - val_acc: 0.6650 - val_f1_m: 0.6613\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6932 - acc: 0.6728 - f1_m: 0.6681 - val_loss: 0.7061 - val_acc: 0.6649 - val_f1_m: 0.6595\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6870 - acc: 0.6765 - f1_m: 0.6721 - val_loss: 0.6983 - val_acc: 0.6716 - val_f1_m: 0.6673\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.6811 - acc: 0.6804 - f1_m: 0.6761 - val_loss: 0.7006 - val_acc: 0.6753 - val_f1_m: 0.6698\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6760 - acc: 0.6832 - f1_m: 0.6793 - val_loss: 0.6939 - val_acc: 0.6778 - val_f1_m: 0.6734\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6709 - acc: 0.6849 - f1_m: 0.6810 - val_loss: 0.6949 - val_acc: 0.6774 - val_f1_m: 0.6741\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 4s 22us/step - loss: 0.6665 - acc: 0.6888 - f1_m: 0.6847 - val_loss: 0.6923 - val_acc: 0.6766 - val_f1_m: 0.6710\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.6627 - acc: 0.6904 - f1_m: 0.6868 - val_loss: 0.6947 - val_acc: 0.6767 - val_f1_m: 0.6715\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6584 - acc: 0.6929 - f1_m: 0.6896 - val_loss: 0.6924 - val_acc: 0.6763 - val_f1_m: 0.6727\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6546 - acc: 0.6956 - f1_m: 0.6921 - val_loss: 0.6940 - val_acc: 0.6768 - val_f1_m: 0.6724\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.6505 - acc: 0.6972 - f1_m: 0.6938 - val_loss: 0.6890 - val_acc: 0.6776 - val_f1_m: 0.6740\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6470 - acc: 0.6988 - f1_m: 0.6959 - val_loss: 0.6913 - val_acc: 0.6782 - val_f1_m: 0.6747\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 4s 22us/step - loss: 0.6444 - acc: 0.7007 - f1_m: 0.6975 - val_loss: 0.6965 - val_acc: 0.6775 - val_f1_m: 0.6745\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.6411 - acc: 0.7014 - f1_m: 0.6983 - val_loss: 0.6923 - val_acc: 0.6800 - val_f1_m: 0.6757\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6376 - acc: 0.7043 - f1_m: 0.7010 - val_loss: 0.6945 - val_acc: 0.6788 - val_f1_m: 0.6748\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.6344 - acc: 0.7052 - f1_m: 0.7023 - val_loss: 0.6918 - val_acc: 0.6814 - val_f1_m: 0.6775\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.6314 - acc: 0.7073 - f1_m: 0.7043 - val_loss: 0.6933 - val_acc: 0.6820 - val_f1_m: 0.6791\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6281 - acc: 0.7089 - f1_m: 0.7059 - val_loss: 0.6924 - val_acc: 0.6857 - val_f1_m: 0.6821\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6262 - acc: 0.7104 - f1_m: 0.7077 - val_loss: 0.6916 - val_acc: 0.6805 - val_f1_m: 0.6761\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6232 - acc: 0.7109 - f1_m: 0.7085 - val_loss: 0.7030 - val_acc: 0.6819 - val_f1_m: 0.6766\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6208 - acc: 0.7124 - f1_m: 0.7100 - val_loss: 0.6967 - val_acc: 0.6811 - val_f1_m: 0.6781\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6179 - acc: 0.7145 - f1_m: 0.7118 - val_loss: 0.7074 - val_acc: 0.6790 - val_f1_m: 0.6758\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.6161 - acc: 0.7154 - f1_m: 0.7130 - val_loss: 0.7004 - val_acc: 0.6841 - val_f1_m: 0.6810\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6142 - acc: 0.7155 - f1_m: 0.7131 - val_loss: 0.7083 - val_acc: 0.6822 - val_f1_m: 0.6796\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6111 - acc: 0.7171 - f1_m: 0.7144 - val_loss: 0.7031 - val_acc: 0.6813 - val_f1_m: 0.6782\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6097 - acc: 0.7182 - f1_m: 0.7153 - val_loss: 0.7060 - val_acc: 0.6798 - val_f1_m: 0.6765\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6062 - acc: 0.7205 - f1_m: 0.7181 - val_loss: 0.7031 - val_acc: 0.6837 - val_f1_m: 0.6810\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6046 - acc: 0.7214 - f1_m: 0.7190 - val_loss: 0.7080 - val_acc: 0.6819 - val_f1_m: 0.6784\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5738 - acc: 0.7359 - f1_m: 0.7341 - val_loss: 0.7154 - val_acc: 0.6909 - val_f1_m: 0.6885\n",
      " ### Learnig rate at the end of epoch 30 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5611 - acc: 0.7423 - f1_m: 0.7404 - val_loss: 0.7226 - val_acc: 0.6883 - val_f1_m: 0.6856\n",
      " ### Learnig rate at the end of epoch 31 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5549 - acc: 0.7455 - f1_m: 0.7438 - val_loss: 0.7282 - val_acc: 0.6876 - val_f1_m: 0.6847\n",
      " ### Learnig rate at the end of epoch 32 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5507 - acc: 0.7475 - f1_m: 0.7455 - val_loss: 0.7356 - val_acc: 0.6870 - val_f1_m: 0.6843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 33 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5474 - acc: 0.7491 - f1_m: 0.7472 - val_loss: 0.7435 - val_acc: 0.6874 - val_f1_m: 0.6849\n",
      " ### Learnig rate at the end of epoch 34 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5444 - acc: 0.7504 - f1_m: 0.7487 - val_loss: 0.7470 - val_acc: 0.6875 - val_f1_m: 0.6849\n",
      " ### Learnig rate at the end of epoch 35 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5416 - acc: 0.7520 - f1_m: 0.7503 - val_loss: 0.7547 - val_acc: 0.6854 - val_f1_m: 0.6832\n",
      " ### Learnig rate at the end of epoch 36 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5392 - acc: 0.7534 - f1_m: 0.7519 - val_loss: 0.7593 - val_acc: 0.6850 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 37 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5371 - acc: 0.7540 - f1_m: 0.7524 - val_loss: 0.7632 - val_acc: 0.6858 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 38 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5349 - acc: 0.7559 - f1_m: 0.7544 - val_loss: 0.7665 - val_acc: 0.6845 - val_f1_m: 0.6824\n",
      " ### Learnig rate at the end of epoch 39 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5332 - acc: 0.7560 - f1_m: 0.7546 - val_loss: 0.7727 - val_acc: 0.6846 - val_f1_m: 0.6820\n",
      " ### Learnig rate at the end of epoch 40 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5260 - acc: 0.7601 - f1_m: 0.7585 - val_loss: 0.7761 - val_acc: 0.6844 - val_f1_m: 0.6819\n",
      " ### Learnig rate at the end of epoch 41 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5249 - acc: 0.7608 - f1_m: 0.7593 - val_loss: 0.7787 - val_acc: 0.6844 - val_f1_m: 0.6820\n",
      " ### Learnig rate at the end of epoch 42 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5243 - acc: 0.7614 - f1_m: 0.7597 - val_loss: 0.7801 - val_acc: 0.6843 - val_f1_m: 0.6819\n",
      " ### Learnig rate at the end of epoch 43 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5239 - acc: 0.7615 - f1_m: 0.7597 - val_loss: 0.7810 - val_acc: 0.6839 - val_f1_m: 0.6818\n",
      " ### Learnig rate at the end of epoch 44 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5236 - acc: 0.7616 - f1_m: 0.7601 - val_loss: 0.7825 - val_acc: 0.6834 - val_f1_m: 0.6809\n",
      " ### Learnig rate at the end of epoch 45 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5233 - acc: 0.7617 - f1_m: 0.7603 - val_loss: 0.7833 - val_acc: 0.6837 - val_f1_m: 0.6812\n",
      " ### Learnig rate at the end of epoch 46 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5229 - acc: 0.7620 - f1_m: 0.7605 - val_loss: 0.7842 - val_acc: 0.6839 - val_f1_m: 0.6812\n",
      " ### Learnig rate at the end of epoch 47 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5227 - acc: 0.7621 - f1_m: 0.7606 - val_loss: 0.7859 - val_acc: 0.6837 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 48 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.5224 - acc: 0.7623 - f1_m: 0.7609 - val_loss: 0.7863 - val_acc: 0.6834 - val_f1_m: 0.6810\n",
      " ### Learnig rate at the end of epoch 49 is 2.0000001313746907e-05 \n",
      "\n",
      "relu\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 9s 43us/step - loss: 0.7567 - acc: 0.6294 - f1_m: 0.6164 - val_loss: 0.7310 - val_acc: 0.6446 - val_f1_m: 0.6383\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.7164 - acc: 0.6575 - f1_m: 0.6515 - val_loss: 0.7193 - val_acc: 0.6573 - val_f1_m: 0.6489\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.7039 - acc: 0.6668 - f1_m: 0.6619 - val_loss: 0.7086 - val_acc: 0.6646 - val_f1_m: 0.6593\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6957 - acc: 0.6719 - f1_m: 0.6673 - val_loss: 0.7069 - val_acc: 0.6679 - val_f1_m: 0.6652\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6893 - acc: 0.6758 - f1_m: 0.6712 - val_loss: 0.7023 - val_acc: 0.6686 - val_f1_m: 0.6635\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6840 - acc: 0.6787 - f1_m: 0.6744 - val_loss: 0.6966 - val_acc: 0.6731 - val_f1_m: 0.6690\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6793 - acc: 0.6822 - f1_m: 0.6777 - val_loss: 0.6984 - val_acc: 0.6714 - val_f1_m: 0.6677\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6743 - acc: 0.6837 - f1_m: 0.6795 - val_loss: 0.6925 - val_acc: 0.6768 - val_f1_m: 0.6731\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6703 - acc: 0.6861 - f1_m: 0.6821 - val_loss: 0.6946 - val_acc: 0.6743 - val_f1_m: 0.6690\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6655 - acc: 0.6895 - f1_m: 0.6854 - val_loss: 0.6950 - val_acc: 0.6724 - val_f1_m: 0.6665\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6618 - acc: 0.6915 - f1_m: 0.6874 - val_loss: 0.6961 - val_acc: 0.6777 - val_f1_m: 0.6737\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6575 - acc: 0.6930 - f1_m: 0.6889 - val_loss: 0.6920 - val_acc: 0.6791 - val_f1_m: 0.6754\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6545 - acc: 0.6949 - f1_m: 0.6905 - val_loss: 0.6894 - val_acc: 0.6785 - val_f1_m: 0.6743\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6504 - acc: 0.6970 - f1_m: 0.6934 - val_loss: 0.6956 - val_acc: 0.6790 - val_f1_m: 0.6744\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6469 - acc: 0.6999 - f1_m: 0.6963 - val_loss: 0.6912 - val_acc: 0.6793 - val_f1_m: 0.6747\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6435 - acc: 0.7006 - f1_m: 0.6968 - val_loss: 0.6913 - val_acc: 0.6779 - val_f1_m: 0.6728\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6418 - acc: 0.7027 - f1_m: 0.6988 - val_loss: 0.6891 - val_acc: 0.6837 - val_f1_m: 0.6810\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6376 - acc: 0.7042 - f1_m: 0.7007 - val_loss: 0.6892 - val_acc: 0.6804 - val_f1_m: 0.6769\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6343 - acc: 0.7062 - f1_m: 0.7025 - val_loss: 0.6976 - val_acc: 0.6801 - val_f1_m: 0.6769\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6317 - acc: 0.7071 - f1_m: 0.7038 - val_loss: 0.6946 - val_acc: 0.6842 - val_f1_m: 0.6811\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6286 - acc: 0.7086 - f1_m: 0.7053 - val_loss: 0.6910 - val_acc: 0.6831 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6261 - acc: 0.7105 - f1_m: 0.7068 - val_loss: 0.6916 - val_acc: 0.6838 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6238 - acc: 0.7103 - f1_m: 0.7070 - val_loss: 0.6969 - val_acc: 0.6828 - val_f1_m: 0.6792\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6219 - acc: 0.7125 - f1_m: 0.7094 - val_loss: 0.6990 - val_acc: 0.6823 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6192 - acc: 0.7140 - f1_m: 0.7109 - val_loss: 0.7037 - val_acc: 0.6834 - val_f1_m: 0.6792\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6152 - acc: 0.7158 - f1_m: 0.7128 - val_loss: 0.6944 - val_acc: 0.6839 - val_f1_m: 0.6795\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6131 - acc: 0.7170 - f1_m: 0.7140 - val_loss: 0.7051 - val_acc: 0.6813 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6113 - acc: 0.7180 - f1_m: 0.7148 - val_loss: 0.7066 - val_acc: 0.6814 - val_f1_m: 0.6776\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6086 - acc: 0.7189 - f1_m: 0.7158 - val_loss: 0.7041 - val_acc: 0.6825 - val_f1_m: 0.6784\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6062 - acc: 0.7201 - f1_m: 0.7171 - val_loss: 0.7006 - val_acc: 0.6828 - val_f1_m: 0.6788\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5764 - acc: 0.7351 - f1_m: 0.7324 - val_loss: 0.7105 - val_acc: 0.6866 - val_f1_m: 0.6838\n",
      " ### Learnig rate at the end of epoch 30 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5639 - acc: 0.7411 - f1_m: 0.7385 - val_loss: 0.7138 - val_acc: 0.6854 - val_f1_m: 0.6825\n",
      " ### Learnig rate at the end of epoch 31 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5577 - acc: 0.7442 - f1_m: 0.7420 - val_loss: 0.7205 - val_acc: 0.6847 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 32 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5532 - acc: 0.7464 - f1_m: 0.7442 - val_loss: 0.7284 - val_acc: 0.6857 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 33 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5497 - acc: 0.7481 - f1_m: 0.7462 - val_loss: 0.7321 - val_acc: 0.6851 - val_f1_m: 0.6824\n",
      " ### Learnig rate at the end of epoch 34 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5464 - acc: 0.7498 - f1_m: 0.7480 - val_loss: 0.7371 - val_acc: 0.6855 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 35 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5436 - acc: 0.7510 - f1_m: 0.7491 - val_loss: 0.7434 - val_acc: 0.6847 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 36 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5410 - acc: 0.7522 - f1_m: 0.7504 - val_loss: 0.7426 - val_acc: 0.6843 - val_f1_m: 0.6817\n",
      " ### Learnig rate at the end of epoch 37 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5384 - acc: 0.7532 - f1_m: 0.7516 - val_loss: 0.7517 - val_acc: 0.6835 - val_f1_m: 0.6812\n",
      " ### Learnig rate at the end of epoch 38 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5361 - acc: 0.7549 - f1_m: 0.7532 - val_loss: 0.7542 - val_acc: 0.6833 - val_f1_m: 0.6811\n",
      " ### Learnig rate at the end of epoch 39 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5339 - acc: 0.7559 - f1_m: 0.7541 - val_loss: 0.7549 - val_acc: 0.6835 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 40 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5268 - acc: 0.7597 - f1_m: 0.7580 - val_loss: 0.7626 - val_acc: 0.6840 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 41 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5254 - acc: 0.7605 - f1_m: 0.7589 - val_loss: 0.7652 - val_acc: 0.6837 - val_f1_m: 0.6811\n",
      " ### Learnig rate at the end of epoch 42 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5247 - acc: 0.7604 - f1_m: 0.7590 - val_loss: 0.7667 - val_acc: 0.6836 - val_f1_m: 0.6812\n",
      " ### Learnig rate at the end of epoch 43 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5242 - acc: 0.7611 - f1_m: 0.7596 - val_loss: 0.7683 - val_acc: 0.6827 - val_f1_m: 0.6806\n",
      " ### Learnig rate at the end of epoch 44 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5238 - acc: 0.7611 - f1_m: 0.7597 - val_loss: 0.7698 - val_acc: 0.6826 - val_f1_m: 0.6804\n",
      " ### Learnig rate at the end of epoch 45 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5234 - acc: 0.7612 - f1_m: 0.7598 - val_loss: 0.7708 - val_acc: 0.6821 - val_f1_m: 0.6800\n",
      " ### Learnig rate at the end of epoch 46 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5230 - acc: 0.7612 - f1_m: 0.7599 - val_loss: 0.7706 - val_acc: 0.6824 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 47 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5227 - acc: 0.7615 - f1_m: 0.7601 - val_loss: 0.7723 - val_acc: 0.6826 - val_f1_m: 0.6802\n",
      " ### Learnig rate at the end of epoch 48 is 2.0000001313746907e-05 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5224 - acc: 0.7619 - f1_m: 0.7604 - val_loss: 0.7735 - val_acc: 0.6822 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 49 is 2.0000001313746907e-05 \n",
      "\n",
      "relu\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 10s 48us/step - loss: 0.7567 - acc: 0.6295 - f1_m: 0.6169 - val_loss: 0.7304 - val_acc: 0.6545 - val_f1_m: 0.6495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.7162 - acc: 0.6575 - f1_m: 0.6509 - val_loss: 0.7231 - val_acc: 0.6594 - val_f1_m: 0.6566\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.7039 - acc: 0.6671 - f1_m: 0.6616 - val_loss: 0.7166 - val_acc: 0.6612 - val_f1_m: 0.6571\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6955 - acc: 0.6723 - f1_m: 0.6674 - val_loss: 0.7016 - val_acc: 0.6687 - val_f1_m: 0.6639\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6887 - acc: 0.6756 - f1_m: 0.6710 - val_loss: 0.7015 - val_acc: 0.6712 - val_f1_m: 0.6628\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6824 - acc: 0.6808 - f1_m: 0.6764 - val_loss: 0.6970 - val_acc: 0.6730 - val_f1_m: 0.6682\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6772 - acc: 0.6822 - f1_m: 0.6780 - val_loss: 0.6973 - val_acc: 0.6740 - val_f1_m: 0.6714\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6722 - acc: 0.6864 - f1_m: 0.6825 - val_loss: 0.6966 - val_acc: 0.6701 - val_f1_m: 0.6645\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6675 - acc: 0.6878 - f1_m: 0.6835 - val_loss: 0.6900 - val_acc: 0.6783 - val_f1_m: 0.6736\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6641 - acc: 0.6907 - f1_m: 0.6869 - val_loss: 0.6886 - val_acc: 0.6772 - val_f1_m: 0.6719\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6593 - acc: 0.6928 - f1_m: 0.6892 - val_loss: 0.6887 - val_acc: 0.6803 - val_f1_m: 0.6763\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6557 - acc: 0.6949 - f1_m: 0.6908 - val_loss: 0.6897 - val_acc: 0.6791 - val_f1_m: 0.6754\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6531 - acc: 0.6962 - f1_m: 0.6925 - val_loss: 0.6893 - val_acc: 0.6771 - val_f1_m: 0.6731\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6499 - acc: 0.6976 - f1_m: 0.6942 - val_loss: 0.6829 - val_acc: 0.6845 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6468 - acc: 0.6997 - f1_m: 0.6955 - val_loss: 0.6860 - val_acc: 0.6789 - val_f1_m: 0.6743\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6439 - acc: 0.7007 - f1_m: 0.6966 - val_loss: 0.6856 - val_acc: 0.6816 - val_f1_m: 0.6772\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6414 - acc: 0.7029 - f1_m: 0.6992 - val_loss: 0.6860 - val_acc: 0.6831 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6374 - acc: 0.7036 - f1_m: 0.6998 - val_loss: 0.6883 - val_acc: 0.6843 - val_f1_m: 0.6803\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6342 - acc: 0.7065 - f1_m: 0.7027 - val_loss: 0.6852 - val_acc: 0.6833 - val_f1_m: 0.6786\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6316 - acc: 0.7066 - f1_m: 0.7030 - val_loss: 0.6862 - val_acc: 0.6848 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6286 - acc: 0.7088 - f1_m: 0.7052 - val_loss: 0.6861 - val_acc: 0.6857 - val_f1_m: 0.6802\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6259 - acc: 0.7095 - f1_m: 0.7060 - val_loss: 0.6891 - val_acc: 0.6853 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6240 - acc: 0.7105 - f1_m: 0.7073 - val_loss: 0.6919 - val_acc: 0.6842 - val_f1_m: 0.6807\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6217 - acc: 0.7120 - f1_m: 0.7087 - val_loss: 0.6887 - val_acc: 0.6852 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6193 - acc: 0.7139 - f1_m: 0.7104 - val_loss: 0.6958 - val_acc: 0.6809 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.6175 - acc: 0.7140 - f1_m: 0.7105 - val_loss: 0.6912 - val_acc: 0.6832 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6137 - acc: 0.7151 - f1_m: 0.7116 - val_loss: 0.6954 - val_acc: 0.6858 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6106 - acc: 0.7166 - f1_m: 0.7134 - val_loss: 0.6970 - val_acc: 0.6851 - val_f1_m: 0.6820\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6088 - acc: 0.7183 - f1_m: 0.7157 - val_loss: 0.6945 - val_acc: 0.6834 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6070 - acc: 0.7185 - f1_m: 0.7151 - val_loss: 0.6928 - val_acc: 0.6865 - val_f1_m: 0.6831\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6042 - acc: 0.7199 - f1_m: 0.7169 - val_loss: 0.7035 - val_acc: 0.6845 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6035 - acc: 0.7206 - f1_m: 0.7176 - val_loss: 0.6954 - val_acc: 0.6829 - val_f1_m: 0.6779\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5999 - acc: 0.7228 - f1_m: 0.7197 - val_loss: 0.7064 - val_acc: 0.6853 - val_f1_m: 0.6821\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5979 - acc: 0.7233 - f1_m: 0.7202 - val_loss: 0.7044 - val_acc: 0.6861 - val_f1_m: 0.6831\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5966 - acc: 0.7239 - f1_m: 0.7213 - val_loss: 0.7006 - val_acc: 0.6859 - val_f1_m: 0.6829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5940 - acc: 0.7247 - f1_m: 0.7219 - val_loss: 0.7096 - val_acc: 0.6821 - val_f1_m: 0.6792\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5917 - acc: 0.7263 - f1_m: 0.7234 - val_loss: 0.7163 - val_acc: 0.6835 - val_f1_m: 0.6805\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5903 - acc: 0.7270 - f1_m: 0.7242 - val_loss: 0.7050 - val_acc: 0.6831 - val_f1_m: 0.6804\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5882 - acc: 0.7282 - f1_m: 0.7253 - val_loss: 0.7071 - val_acc: 0.6850 - val_f1_m: 0.6820\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5865 - acc: 0.7288 - f1_m: 0.7258 - val_loss: 0.7170 - val_acc: 0.6802 - val_f1_m: 0.6770\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5556 - acc: 0.7431 - f1_m: 0.7410 - val_loss: 0.7271 - val_acc: 0.6859 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 40 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5425 - acc: 0.7490 - f1_m: 0.7470 - val_loss: 0.7422 - val_acc: 0.6867 - val_f1_m: 0.6832\n",
      " ### Learnig rate at the end of epoch 41 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5359 - acc: 0.7522 - f1_m: 0.7502 - val_loss: 0.7472 - val_acc: 0.6874 - val_f1_m: 0.6842\n",
      " ### Learnig rate at the end of epoch 42 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5313 - acc: 0.7540 - f1_m: 0.7524 - val_loss: 0.7607 - val_acc: 0.6857 - val_f1_m: 0.6833\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5277 - acc: 0.7563 - f1_m: 0.7545 - val_loss: 0.7655 - val_acc: 0.6870 - val_f1_m: 0.6847\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5243 - acc: 0.7581 - f1_m: 0.7563 - val_loss: 0.7729 - val_acc: 0.6867 - val_f1_m: 0.6842\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5214 - acc: 0.7590 - f1_m: 0.7573 - val_loss: 0.7785 - val_acc: 0.6854 - val_f1_m: 0.6823\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5187 - acc: 0.7604 - f1_m: 0.7588 - val_loss: 0.7827 - val_acc: 0.6856 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5162 - acc: 0.7618 - f1_m: 0.7604 - val_loss: 0.7938 - val_acc: 0.6856 - val_f1_m: 0.6827\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5139 - acc: 0.7633 - f1_m: 0.7618 - val_loss: 0.8018 - val_acc: 0.6847 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "tanh\n",
      "3\n",
      "3\n",
      "3\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 9s 43us/step - loss: 0.7625 - acc: 0.6260 - f1_m: 0.6138 - val_loss: 0.7460 - val_acc: 0.6419 - val_f1_m: 0.6330\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.7193 - acc: 0.6566 - f1_m: 0.6509 - val_loss: 0.7219 - val_acc: 0.6568 - val_f1_m: 0.6508\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.7054 - acc: 0.6659 - f1_m: 0.6616 - val_loss: 0.7087 - val_acc: 0.6660 - val_f1_m: 0.6612\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6960 - acc: 0.6726 - f1_m: 0.6685 - val_loss: 0.7005 - val_acc: 0.6726 - val_f1_m: 0.6681\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6885 - acc: 0.6771 - f1_m: 0.6731 - val_loss: 0.7008 - val_acc: 0.6696 - val_f1_m: 0.6652\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6815 - acc: 0.6807 - f1_m: 0.6766 - val_loss: 0.7014 - val_acc: 0.6725 - val_f1_m: 0.6681\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6758 - acc: 0.6844 - f1_m: 0.6809 - val_loss: 0.6980 - val_acc: 0.6745 - val_f1_m: 0.6707\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6707 - acc: 0.6868 - f1_m: 0.6834 - val_loss: 0.7001 - val_acc: 0.6723 - val_f1_m: 0.6679\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6660 - acc: 0.6899 - f1_m: 0.6862 - val_loss: 0.6950 - val_acc: 0.6754 - val_f1_m: 0.6726\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6605 - acc: 0.6927 - f1_m: 0.6892 - val_loss: 0.6953 - val_acc: 0.6770 - val_f1_m: 0.6742\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6559 - acc: 0.6954 - f1_m: 0.6920 - val_loss: 0.6925 - val_acc: 0.6805 - val_f1_m: 0.6781\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6520 - acc: 0.6974 - f1_m: 0.6943 - val_loss: 0.6977 - val_acc: 0.6743 - val_f1_m: 0.6716\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6482 - acc: 0.6998 - f1_m: 0.6966 - val_loss: 0.6929 - val_acc: 0.6790 - val_f1_m: 0.6765\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6445 - acc: 0.7012 - f1_m: 0.6980 - val_loss: 0.6950 - val_acc: 0.6792 - val_f1_m: 0.6750\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6400 - acc: 0.7045 - f1_m: 0.7014 - val_loss: 0.6944 - val_acc: 0.6805 - val_f1_m: 0.6769\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6367 - acc: 0.7054 - f1_m: 0.7025 - val_loss: 0.6975 - val_acc: 0.6803 - val_f1_m: 0.6766\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6335 - acc: 0.7082 - f1_m: 0.7052 - val_loss: 0.6999 - val_acc: 0.6773 - val_f1_m: 0.6754\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6311 - acc: 0.7093 - f1_m: 0.7063 - val_loss: 0.6997 - val_acc: 0.6786 - val_f1_m: 0.6754\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6274 - acc: 0.7112 - f1_m: 0.7083 - val_loss: 0.6968 - val_acc: 0.6824 - val_f1_m: 0.6794\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6257 - acc: 0.7134 - f1_m: 0.7101 - val_loss: 0.7003 - val_acc: 0.6727 - val_f1_m: 0.6674\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6226 - acc: 0.7138 - f1_m: 0.7106 - val_loss: 0.6992 - val_acc: 0.6744 - val_f1_m: 0.6703\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6208 - acc: 0.7148 - f1_m: 0.7118 - val_loss: 0.6949 - val_acc: 0.6826 - val_f1_m: 0.6792\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6173 - acc: 0.7172 - f1_m: 0.7145 - val_loss: 0.7005 - val_acc: 0.6788 - val_f1_m: 0.6736\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6152 - acc: 0.7178 - f1_m: 0.7153 - val_loss: 0.6969 - val_acc: 0.6779 - val_f1_m: 0.6742\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6130 - acc: 0.7199 - f1_m: 0.7170 - val_loss: 0.7014 - val_acc: 0.6804 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6107 - acc: 0.7202 - f1_m: 0.7174 - val_loss: 0.7021 - val_acc: 0.6753 - val_f1_m: 0.6714\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6093 - acc: 0.7211 - f1_m: 0.7185 - val_loss: 0.6993 - val_acc: 0.6778 - val_f1_m: 0.6741\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6067 - acc: 0.7219 - f1_m: 0.7194 - val_loss: 0.7017 - val_acc: 0.6779 - val_f1_m: 0.6733\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6053 - acc: 0.7232 - f1_m: 0.7204 - val_loss: 0.7115 - val_acc: 0.6750 - val_f1_m: 0.6720\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6036 - acc: 0.7235 - f1_m: 0.7208 - val_loss: 0.7093 - val_acc: 0.6805 - val_f1_m: 0.6767\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6008 - acc: 0.7256 - f1_m: 0.7231 - val_loss: 0.7013 - val_acc: 0.6836 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.5988 - acc: 0.7263 - f1_m: 0.7240 - val_loss: 0.7078 - val_acc: 0.6759 - val_f1_m: 0.6725\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.5975 - acc: 0.7289 - f1_m: 0.7262 - val_loss: 0.7102 - val_acc: 0.6791 - val_f1_m: 0.6759\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5961 - acc: 0.7279 - f1_m: 0.7252 - val_loss: 0.7062 - val_acc: 0.6805 - val_f1_m: 0.6786\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.5946 - acc: 0.7279 - f1_m: 0.7256 - val_loss: 0.7089 - val_acc: 0.6784 - val_f1_m: 0.6759\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.5928 - acc: 0.7296 - f1_m: 0.7270 - val_loss: 0.7038 - val_acc: 0.6784 - val_f1_m: 0.6758\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5921 - acc: 0.7297 - f1_m: 0.7271 - val_loss: 0.7141 - val_acc: 0.6721 - val_f1_m: 0.6681\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5896 - acc: 0.7312 - f1_m: 0.7285 - val_loss: 0.7130 - val_acc: 0.6771 - val_f1_m: 0.6747\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5892 - acc: 0.7309 - f1_m: 0.7287 - val_loss: 0.7160 - val_acc: 0.6773 - val_f1_m: 0.6751\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5902 - acc: 0.7309 - f1_m: 0.7286 - val_loss: 0.7107 - val_acc: 0.6777 - val_f1_m: 0.6750\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5875 - acc: 0.7327 - f1_m: 0.7303 - val_loss: 0.7101 - val_acc: 0.6782 - val_f1_m: 0.6755\n",
      " ### Learnig rate at the end of epoch 40 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5511 - acc: 0.7535 - f1_m: 0.7514 - val_loss: 0.7120 - val_acc: 0.6826 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 41 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.5312 - acc: 0.7628 - f1_m: 0.7611 - val_loss: 0.7215 - val_acc: 0.6813 - val_f1_m: 0.6790\n",
      " ### Learnig rate at the end of epoch 42 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5208 - acc: 0.7683 - f1_m: 0.7665 - val_loss: 0.7282 - val_acc: 0.6802 - val_f1_m: 0.6780\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5132 - acc: 0.7723 - f1_m: 0.7707 - val_loss: 0.7354 - val_acc: 0.6792 - val_f1_m: 0.6770\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5075 - acc: 0.7747 - f1_m: 0.7730 - val_loss: 0.7396 - val_acc: 0.6794 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5025 - acc: 0.7772 - f1_m: 0.7757 - val_loss: 0.7471 - val_acc: 0.6789 - val_f1_m: 0.6766\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.4982 - acc: 0.7792 - f1_m: 0.7778 - val_loss: 0.7505 - val_acc: 0.6792 - val_f1_m: 0.6769\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.4945 - acc: 0.7815 - f1_m: 0.7801 - val_loss: 0.7551 - val_acc: 0.6771 - val_f1_m: 0.6748\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.4909 - acc: 0.7837 - f1_m: 0.7823 - val_loss: 0.7598 - val_acc: 0.6757 - val_f1_m: 0.6734\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "tanh\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 10s 50us/step - loss: 0.7654 - acc: 0.6224 - f1_m: 0.6096 - val_loss: 0.7288 - val_acc: 0.6520 - val_f1_m: 0.6449\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.7225 - acc: 0.6537 - f1_m: 0.6486 - val_loss: 0.7216 - val_acc: 0.6575 - val_f1_m: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.7096 - acc: 0.6618 - f1_m: 0.6576 - val_loss: 0.7134 - val_acc: 0.6608 - val_f1_m: 0.6548\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.7010 - acc: 0.6683 - f1_m: 0.6640 - val_loss: 0.7037 - val_acc: 0.6714 - val_f1_m: 0.6666\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6931 - acc: 0.6740 - f1_m: 0.6702 - val_loss: 0.7063 - val_acc: 0.6698 - val_f1_m: 0.6671\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6863 - acc: 0.6769 - f1_m: 0.6731 - val_loss: 0.7050 - val_acc: 0.6665 - val_f1_m: 0.6612\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6811 - acc: 0.6812 - f1_m: 0.6776 - val_loss: 0.7051 - val_acc: 0.6699 - val_f1_m: 0.6650\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6763 - acc: 0.6843 - f1_m: 0.6806 - val_loss: 0.6978 - val_acc: 0.6729 - val_f1_m: 0.6694\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6715 - acc: 0.6866 - f1_m: 0.6831 - val_loss: 0.6979 - val_acc: 0.6733 - val_f1_m: 0.6677\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6673 - acc: 0.6882 - f1_m: 0.6848 - val_loss: 0.6957 - val_acc: 0.6758 - val_f1_m: 0.6719\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6637 - acc: 0.6906 - f1_m: 0.6871 - val_loss: 0.6972 - val_acc: 0.6754 - val_f1_m: 0.6720\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6595 - acc: 0.6938 - f1_m: 0.6907 - val_loss: 0.6947 - val_acc: 0.6775 - val_f1_m: 0.6755\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6572 - acc: 0.6952 - f1_m: 0.6922 - val_loss: 0.6955 - val_acc: 0.6774 - val_f1_m: 0.6747\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6539 - acc: 0.6974 - f1_m: 0.6942 - val_loss: 0.6953 - val_acc: 0.6758 - val_f1_m: 0.6715\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6506 - acc: 0.6991 - f1_m: 0.6960 - val_loss: 0.6920 - val_acc: 0.6769 - val_f1_m: 0.6727\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6481 - acc: 0.7012 - f1_m: 0.6981 - val_loss: 0.7011 - val_acc: 0.6736 - val_f1_m: 0.6712\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6459 - acc: 0.7006 - f1_m: 0.6976 - val_loss: 0.7004 - val_acc: 0.6747 - val_f1_m: 0.6720\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6431 - acc: 0.7036 - f1_m: 0.7005 - val_loss: 0.6917 - val_acc: 0.6805 - val_f1_m: 0.6776\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6415 - acc: 0.7038 - f1_m: 0.7012 - val_loss: 0.6979 - val_acc: 0.6779 - val_f1_m: 0.6742\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6390 - acc: 0.7056 - f1_m: 0.7028 - val_loss: 0.6992 - val_acc: 0.6788 - val_f1_m: 0.6761\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6378 - acc: 0.7071 - f1_m: 0.7042 - val_loss: 0.6944 - val_acc: 0.6778 - val_f1_m: 0.6742\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6359 - acc: 0.7079 - f1_m: 0.7049 - val_loss: 0.6923 - val_acc: 0.6788 - val_f1_m: 0.6744\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6334 - acc: 0.7089 - f1_m: 0.7063 - val_loss: 0.7024 - val_acc: 0.6736 - val_f1_m: 0.6704\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6321 - acc: 0.7086 - f1_m: 0.7060 - val_loss: 0.6996 - val_acc: 0.6722 - val_f1_m: 0.6688\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6305 - acc: 0.7105 - f1_m: 0.7078 - val_loss: 0.6950 - val_acc: 0.6792 - val_f1_m: 0.6758\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6286 - acc: 0.7116 - f1_m: 0.7090 - val_loss: 0.7066 - val_acc: 0.6761 - val_f1_m: 0.6730\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6279 - acc: 0.7110 - f1_m: 0.7085 - val_loss: 0.6960 - val_acc: 0.6799 - val_f1_m: 0.6761\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6261 - acc: 0.7131 - f1_m: 0.7105 - val_loss: 0.6928 - val_acc: 0.6816 - val_f1_m: 0.6779\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6251 - acc: 0.7144 - f1_m: 0.7116 - val_loss: 0.7045 - val_acc: 0.6752 - val_f1_m: 0.6720\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6242 - acc: 0.7133 - f1_m: 0.7109 - val_loss: 0.7024 - val_acc: 0.6765 - val_f1_m: 0.6724\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6225 - acc: 0.7149 - f1_m: 0.7122 - val_loss: 0.6977 - val_acc: 0.6794 - val_f1_m: 0.6760\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6208 - acc: 0.7153 - f1_m: 0.7127 - val_loss: 0.7003 - val_acc: 0.6787 - val_f1_m: 0.6751\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6200 - acc: 0.7156 - f1_m: 0.7130 - val_loss: 0.6965 - val_acc: 0.6810 - val_f1_m: 0.6788\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6192 - acc: 0.7161 - f1_m: 0.7136 - val_loss: 0.6997 - val_acc: 0.6802 - val_f1_m: 0.6771\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6184 - acc: 0.7171 - f1_m: 0.7144 - val_loss: 0.6998 - val_acc: 0.6754 - val_f1_m: 0.6714\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6179 - acc: 0.7163 - f1_m: 0.7138 - val_loss: 0.7020 - val_acc: 0.6760 - val_f1_m: 0.6736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6171 - acc: 0.7175 - f1_m: 0.7148 - val_loss: 0.7016 - val_acc: 0.6792 - val_f1_m: 0.6754\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6169 - acc: 0.7180 - f1_m: 0.7153 - val_loss: 0.7088 - val_acc: 0.6777 - val_f1_m: 0.6754\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5875 - acc: 0.7347 - f1_m: 0.7331 - val_loss: 0.7026 - val_acc: 0.6837 - val_f1_m: 0.6816\n",
      " ### Learnig rate at the end of epoch 38 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5700 - acc: 0.7443 - f1_m: 0.7426 - val_loss: 0.7066 - val_acc: 0.6829 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 39 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5603 - acc: 0.7491 - f1_m: 0.7472 - val_loss: 0.7107 - val_acc: 0.6823 - val_f1_m: 0.6796\n",
      " ### Learnig rate at the end of epoch 40 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5529 - acc: 0.7530 - f1_m: 0.7515 - val_loss: 0.7130 - val_acc: 0.6826 - val_f1_m: 0.6804\n",
      " ### Learnig rate at the end of epoch 41 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5468 - acc: 0.7563 - f1_m: 0.7546 - val_loss: 0.7190 - val_acc: 0.6829 - val_f1_m: 0.6805\n",
      " ### Learnig rate at the end of epoch 42 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5417 - acc: 0.7594 - f1_m: 0.7578 - val_loss: 0.7231 - val_acc: 0.6822 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5371 - acc: 0.7618 - f1_m: 0.7602 - val_loss: 0.7255 - val_acc: 0.6814 - val_f1_m: 0.6794\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5329 - acc: 0.7637 - f1_m: 0.7622 - val_loss: 0.7293 - val_acc: 0.6837 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5291 - acc: 0.7656 - f1_m: 0.7641 - val_loss: 0.7308 - val_acc: 0.6827 - val_f1_m: 0.6804\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5255 - acc: 0.7679 - f1_m: 0.7664 - val_loss: 0.7361 - val_acc: 0.6818 - val_f1_m: 0.6802\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5223 - acc: 0.7693 - f1_m: 0.7679 - val_loss: 0.7372 - val_acc: 0.6818 - val_f1_m: 0.6797\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.5124 - acc: 0.7748 - f1_m: 0.7736 - val_loss: 0.7397 - val_acc: 0.6821 - val_f1_m: 0.6798\n",
      " ### Learnig rate at the end of epoch 49 is 2.0000001313746907e-05 \n",
      "\n",
      "tanh\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 11s 52us/step - loss: 0.7695 - acc: 0.6187 - f1_m: 0.6062 - val_loss: 0.7530 - val_acc: 0.6357 - val_f1_m: 0.6302\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.7272 - acc: 0.6499 - f1_m: 0.6444 - val_loss: 0.7253 - val_acc: 0.6583 - val_f1_m: 0.6533\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.7150 - acc: 0.6600 - f1_m: 0.6554 - val_loss: 0.7268 - val_acc: 0.6546 - val_f1_m: 0.6503\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.7066 - acc: 0.6654 - f1_m: 0.6611 - val_loss: 0.7178 - val_acc: 0.6622 - val_f1_m: 0.6582\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6998 - acc: 0.6687 - f1_m: 0.6649 - val_loss: 0.7111 - val_acc: 0.6652 - val_f1_m: 0.6615\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6946 - acc: 0.6731 - f1_m: 0.6692 - val_loss: 0.7127 - val_acc: 0.6670 - val_f1_m: 0.6617\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6899 - acc: 0.6757 - f1_m: 0.6719 - val_loss: 0.7091 - val_acc: 0.6667 - val_f1_m: 0.6635\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6850 - acc: 0.6785 - f1_m: 0.6745 - val_loss: 0.7028 - val_acc: 0.6709 - val_f1_m: 0.6662\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6810 - acc: 0.6811 - f1_m: 0.6777 - val_loss: 0.7042 - val_acc: 0.6740 - val_f1_m: 0.6716\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6771 - acc: 0.6836 - f1_m: 0.6799 - val_loss: 0.7047 - val_acc: 0.6721 - val_f1_m: 0.6685\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6729 - acc: 0.6870 - f1_m: 0.6834 - val_loss: 0.7024 - val_acc: 0.6729 - val_f1_m: 0.6691\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6709 - acc: 0.6878 - f1_m: 0.6843 - val_loss: 0.7001 - val_acc: 0.6720 - val_f1_m: 0.6667\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6670 - acc: 0.6895 - f1_m: 0.6860 - val_loss: 0.6993 - val_acc: 0.6724 - val_f1_m: 0.6681\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6643 - acc: 0.6914 - f1_m: 0.6880 - val_loss: 0.6961 - val_acc: 0.6754 - val_f1_m: 0.6719\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6617 - acc: 0.6929 - f1_m: 0.6897 - val_loss: 0.6973 - val_acc: 0.6779 - val_f1_m: 0.6738\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6591 - acc: 0.6945 - f1_m: 0.6914 - val_loss: 0.6976 - val_acc: 0.6771 - val_f1_m: 0.6733\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.6564 - acc: 0.6958 - f1_m: 0.6927 - val_loss: 0.6960 - val_acc: 0.6787 - val_f1_m: 0.6745\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6537 - acc: 0.6984 - f1_m: 0.6954 - val_loss: 0.7009 - val_acc: 0.6752 - val_f1_m: 0.6721\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6521 - acc: 0.6993 - f1_m: 0.6965 - val_loss: 0.6960 - val_acc: 0.6759 - val_f1_m: 0.6728\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6500 - acc: 0.7000 - f1_m: 0.6973 - val_loss: 0.7014 - val_acc: 0.6750 - val_f1_m: 0.6715\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6476 - acc: 0.7008 - f1_m: 0.6978 - val_loss: 0.6998 - val_acc: 0.6755 - val_f1_m: 0.6729\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6464 - acc: 0.7016 - f1_m: 0.6987 - val_loss: 0.6962 - val_acc: 0.6778 - val_f1_m: 0.6749\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6449 - acc: 0.7038 - f1_m: 0.7009 - val_loss: 0.7085 - val_acc: 0.6779 - val_f1_m: 0.6756\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6434 - acc: 0.7044 - f1_m: 0.7017 - val_loss: 0.6940 - val_acc: 0.6789 - val_f1_m: 0.6748\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.6419 - acc: 0.7039 - f1_m: 0.7010 - val_loss: 0.6969 - val_acc: 0.6773 - val_f1_m: 0.6732\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6418 - acc: 0.7047 - f1_m: 0.7017 - val_loss: 0.6997 - val_acc: 0.6753 - val_f1_m: 0.6719\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6405 - acc: 0.7053 - f1_m: 0.7022 - val_loss: 0.6949 - val_acc: 0.6788 - val_f1_m: 0.6754\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6394 - acc: 0.7059 - f1_m: 0.7032 - val_loss: 0.6937 - val_acc: 0.6790 - val_f1_m: 0.6762\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6398 - acc: 0.7057 - f1_m: 0.7029 - val_loss: 0.6951 - val_acc: 0.6764 - val_f1_m: 0.6722\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6387 - acc: 0.7067 - f1_m: 0.7038 - val_loss: 0.6984 - val_acc: 0.6799 - val_f1_m: 0.6771\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.6376 - acc: 0.7067 - f1_m: 0.7042 - val_loss: 0.7009 - val_acc: 0.6748 - val_f1_m: 0.6726\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6386 - acc: 0.7059 - f1_m: 0.7031 - val_loss: 0.6996 - val_acc: 0.6791 - val_f1_m: 0.6764\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6361 - acc: 0.7092 - f1_m: 0.7065 - val_loss: 0.7054 - val_acc: 0.6762 - val_f1_m: 0.6725\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6356 - acc: 0.7099 - f1_m: 0.7073 - val_loss: 0.7002 - val_acc: 0.6776 - val_f1_m: 0.6744\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6356 - acc: 0.7076 - f1_m: 0.7051 - val_loss: 0.7035 - val_acc: 0.6761 - val_f1_m: 0.6733\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6352 - acc: 0.7090 - f1_m: 0.7066 - val_loss: 0.6965 - val_acc: 0.6773 - val_f1_m: 0.6737\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6353 - acc: 0.7091 - f1_m: 0.7063 - val_loss: 0.6986 - val_acc: 0.6788 - val_f1_m: 0.6746\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6361 - acc: 0.7091 - f1_m: 0.7064 - val_loss: 0.6939 - val_acc: 0.6802 - val_f1_m: 0.6762\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6335 - acc: 0.7099 - f1_m: 0.7072 - val_loss: 0.7095 - val_acc: 0.6735 - val_f1_m: 0.6703\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6342 - acc: 0.7089 - f1_m: 0.7066 - val_loss: 0.6997 - val_acc: 0.6793 - val_f1_m: 0.6760\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6317 - acc: 0.7110 - f1_m: 0.7083 - val_loss: 0.7028 - val_acc: 0.6780 - val_f1_m: 0.6760\n",
      " ### Learnig rate at the end of epoch 40 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.6317 - acc: 0.7113 - f1_m: 0.7085 - val_loss: 0.7031 - val_acc: 0.6750 - val_f1_m: 0.6708\n",
      " ### Learnig rate at the end of epoch 41 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6316 - acc: 0.7108 - f1_m: 0.7081 - val_loss: 0.7028 - val_acc: 0.6781 - val_f1_m: 0.6752\n",
      " ### Learnig rate at the end of epoch 42 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6317 - acc: 0.7100 - f1_m: 0.7073 - val_loss: 0.7037 - val_acc: 0.6747 - val_f1_m: 0.6711\n",
      " ### Learnig rate at the end of epoch 43 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6306 - acc: 0.7110 - f1_m: 0.7085 - val_loss: 0.6987 - val_acc: 0.6795 - val_f1_m: 0.6763\n",
      " ### Learnig rate at the end of epoch 44 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6307 - acc: 0.7123 - f1_m: 0.7099 - val_loss: 0.7029 - val_acc: 0.6766 - val_f1_m: 0.6735\n",
      " ### Learnig rate at the end of epoch 45 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6327 - acc: 0.7106 - f1_m: 0.7080 - val_loss: 0.7007 - val_acc: 0.6775 - val_f1_m: 0.6745\n",
      " ### Learnig rate at the end of epoch 46 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6321 - acc: 0.7116 - f1_m: 0.7090 - val_loss: 0.7068 - val_acc: 0.6766 - val_f1_m: 0.6745\n",
      " ### Learnig rate at the end of epoch 47 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6095 - acc: 0.7243 - f1_m: 0.7224 - val_loss: 0.6952 - val_acc: 0.6818 - val_f1_m: 0.6791\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5950 - acc: 0.7324 - f1_m: 0.7304 - val_loss: 0.6999 - val_acc: 0.6823 - val_f1_m: 0.6800\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "<keras.layers.advanced_activations.ReLU object at 0x18af93470>\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 11s 51us/step - loss: 0.7554 - acc: 0.6289 - f1_m: 0.6171 - val_loss: 0.7308 - val_acc: 0.6512 - val_f1_m: 0.6433\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.7144 - acc: 0.6592 - f1_m: 0.6533 - val_loss: 0.7271 - val_acc: 0.6556 - val_f1_m: 0.6504\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.7021 - acc: 0.6680 - f1_m: 0.6631 - val_loss: 0.7085 - val_acc: 0.6680 - val_f1_m: 0.6624\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6932 - acc: 0.6727 - f1_m: 0.6684 - val_loss: 0.7064 - val_acc: 0.6712 - val_f1_m: 0.6633\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6863 - acc: 0.6768 - f1_m: 0.6723 - val_loss: 0.7027 - val_acc: 0.6649 - val_f1_m: 0.6610\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6797 - acc: 0.6809 - f1_m: 0.6776 - val_loss: 0.6959 - val_acc: 0.6752 - val_f1_m: 0.6710\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6743 - acc: 0.6841 - f1_m: 0.6800 - val_loss: 0.6967 - val_acc: 0.6728 - val_f1_m: 0.6692\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6692 - acc: 0.6870 - f1_m: 0.6829 - val_loss: 0.6920 - val_acc: 0.6776 - val_f1_m: 0.6745\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6647 - acc: 0.6893 - f1_m: 0.6854 - val_loss: 0.6919 - val_acc: 0.6781 - val_f1_m: 0.6751\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6603 - acc: 0.6923 - f1_m: 0.6891 - val_loss: 0.6943 - val_acc: 0.6762 - val_f1_m: 0.6732\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6564 - acc: 0.6952 - f1_m: 0.6919 - val_loss: 0.6855 - val_acc: 0.6806 - val_f1_m: 0.6765\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6520 - acc: 0.6966 - f1_m: 0.6935 - val_loss: 0.6862 - val_acc: 0.6804 - val_f1_m: 0.6764\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6490 - acc: 0.6976 - f1_m: 0.6945 - val_loss: 0.6827 - val_acc: 0.6843 - val_f1_m: 0.6812\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6450 - acc: 0.7003 - f1_m: 0.6975 - val_loss: 0.6874 - val_acc: 0.6804 - val_f1_m: 0.6772\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6414 - acc: 0.7022 - f1_m: 0.6989 - val_loss: 0.6931 - val_acc: 0.6811 - val_f1_m: 0.6785\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.6393 - acc: 0.7022 - f1_m: 0.6993 - val_loss: 0.6895 - val_acc: 0.6846 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6353 - acc: 0.7050 - f1_m: 0.7019 - val_loss: 0.6913 - val_acc: 0.6812 - val_f1_m: 0.6772\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6324 - acc: 0.7058 - f1_m: 0.7028 - val_loss: 0.6896 - val_acc: 0.6858 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6296 - acc: 0.7087 - f1_m: 0.7058 - val_loss: 0.6936 - val_acc: 0.6837 - val_f1_m: 0.6810\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6269 - acc: 0.7085 - f1_m: 0.7059 - val_loss: 0.6890 - val_acc: 0.6836 - val_f1_m: 0.6806\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6240 - acc: 0.7115 - f1_m: 0.7087 - val_loss: 0.6958 - val_acc: 0.6826 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6211 - acc: 0.7128 - f1_m: 0.7100 - val_loss: 0.6957 - val_acc: 0.6827 - val_f1_m: 0.6794\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6194 - acc: 0.7132 - f1_m: 0.7102 - val_loss: 0.6921 - val_acc: 0.6841 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6165 - acc: 0.7151 - f1_m: 0.7126 - val_loss: 0.6929 - val_acc: 0.6863 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6132 - acc: 0.7161 - f1_m: 0.7134 - val_loss: 0.6998 - val_acc: 0.6808 - val_f1_m: 0.6772\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6117 - acc: 0.7179 - f1_m: 0.7154 - val_loss: 0.6982 - val_acc: 0.6848 - val_f1_m: 0.6818\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6094 - acc: 0.7190 - f1_m: 0.7165 - val_loss: 0.6978 - val_acc: 0.6842 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6075 - acc: 0.7195 - f1_m: 0.7169 - val_loss: 0.6972 - val_acc: 0.6866 - val_f1_m: 0.6825\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6047 - acc: 0.7208 - f1_m: 0.7180 - val_loss: 0.6969 - val_acc: 0.6857 - val_f1_m: 0.6837\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6038 - acc: 0.7219 - f1_m: 0.7192 - val_loss: 0.7067 - val_acc: 0.6825 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6015 - acc: 0.7226 - f1_m: 0.7204 - val_loss: 0.7112 - val_acc: 0.6831 - val_f1_m: 0.6794\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6003 - acc: 0.7221 - f1_m: 0.7199 - val_loss: 0.7061 - val_acc: 0.6826 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5986 - acc: 0.7239 - f1_m: 0.7215 - val_loss: 0.7059 - val_acc: 0.6868 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5963 - acc: 0.7253 - f1_m: 0.7229 - val_loss: 0.7088 - val_acc: 0.6859 - val_f1_m: 0.6828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5949 - acc: 0.7253 - f1_m: 0.7230 - val_loss: 0.7074 - val_acc: 0.6826 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5924 - acc: 0.7266 - f1_m: 0.7239 - val_loss: 0.7097 - val_acc: 0.6847 - val_f1_m: 0.6820\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5916 - acc: 0.7275 - f1_m: 0.7250 - val_loss: 0.7151 - val_acc: 0.6867 - val_f1_m: 0.6839\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5884 - acc: 0.7286 - f1_m: 0.7263 - val_loss: 0.7199 - val_acc: 0.6809 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5881 - acc: 0.7287 - f1_m: 0.7265 - val_loss: 0.7191 - val_acc: 0.6856 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5857 - acc: 0.7303 - f1_m: 0.7280 - val_loss: 0.7232 - val_acc: 0.6847 - val_f1_m: 0.6811\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5853 - acc: 0.7303 - f1_m: 0.7279 - val_loss: 0.7239 - val_acc: 0.6815 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 40 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5836 - acc: 0.7321 - f1_m: 0.7297 - val_loss: 0.7277 - val_acc: 0.6866 - val_f1_m: 0.6849\n",
      " ### Learnig rate at the end of epoch 41 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5819 - acc: 0.7324 - f1_m: 0.7302 - val_loss: 0.7246 - val_acc: 0.6839 - val_f1_m: 0.6811\n",
      " ### Learnig rate at the end of epoch 42 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5475 - acc: 0.7498 - f1_m: 0.7481 - val_loss: 0.7431 - val_acc: 0.6867 - val_f1_m: 0.6847\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5335 - acc: 0.7562 - f1_m: 0.7545 - val_loss: 0.7500 - val_acc: 0.6860 - val_f1_m: 0.6836\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5271 - acc: 0.7590 - f1_m: 0.7574 - val_loss: 0.7641 - val_acc: 0.6876 - val_f1_m: 0.6857\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5228 - acc: 0.7610 - f1_m: 0.7593 - val_loss: 0.7708 - val_acc: 0.6854 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5193 - acc: 0.7626 - f1_m: 0.7610 - val_loss: 0.7786 - val_acc: 0.6869 - val_f1_m: 0.6850\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5164 - acc: 0.7646 - f1_m: 0.7631 - val_loss: 0.7853 - val_acc: 0.6856 - val_f1_m: 0.6831\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5139 - acc: 0.7652 - f1_m: 0.7635 - val_loss: 0.7967 - val_acc: 0.6849 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "<keras.layers.advanced_activations.ReLU object at 0x18af93470>\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 11s 54us/step - loss: 0.7576 - acc: 0.6279 - f1_m: 0.6145 - val_loss: 0.7291 - val_acc: 0.6511 - val_f1_m: 0.6449\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.7158 - acc: 0.6582 - f1_m: 0.6524 - val_loss: 0.7162 - val_acc: 0.6584 - val_f1_m: 0.6536\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.7041 - acc: 0.6657 - f1_m: 0.6607 - val_loss: 0.7086 - val_acc: 0.6667 - val_f1_m: 0.6628\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6953 - acc: 0.6727 - f1_m: 0.6680 - val_loss: 0.7024 - val_acc: 0.6666 - val_f1_m: 0.6626\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6885 - acc: 0.6755 - f1_m: 0.6712 - val_loss: 0.7045 - val_acc: 0.6688 - val_f1_m: 0.6642\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6822 - acc: 0.6801 - f1_m: 0.6758 - val_loss: 0.6984 - val_acc: 0.6737 - val_f1_m: 0.6697\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6770 - acc: 0.6826 - f1_m: 0.6789 - val_loss: 0.6934 - val_acc: 0.6754 - val_f1_m: 0.6717\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6727 - acc: 0.6847 - f1_m: 0.6807 - val_loss: 0.6912 - val_acc: 0.6749 - val_f1_m: 0.6707\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6674 - acc: 0.6869 - f1_m: 0.6831 - val_loss: 0.6973 - val_acc: 0.6772 - val_f1_m: 0.6734\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6637 - acc: 0.6903 - f1_m: 0.6867 - val_loss: 0.6951 - val_acc: 0.6746 - val_f1_m: 0.6707\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6590 - acc: 0.6928 - f1_m: 0.6892 - val_loss: 0.6901 - val_acc: 0.6797 - val_f1_m: 0.6747\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6549 - acc: 0.6959 - f1_m: 0.6925 - val_loss: 0.6916 - val_acc: 0.6790 - val_f1_m: 0.6755\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6516 - acc: 0.6971 - f1_m: 0.6936 - val_loss: 0.6971 - val_acc: 0.6749 - val_f1_m: 0.6711\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6488 - acc: 0.6978 - f1_m: 0.6945 - val_loss: 0.6937 - val_acc: 0.6813 - val_f1_m: 0.6776\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6447 - acc: 0.7008 - f1_m: 0.6972 - val_loss: 0.6892 - val_acc: 0.6809 - val_f1_m: 0.6777\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6417 - acc: 0.7022 - f1_m: 0.6989 - val_loss: 0.6881 - val_acc: 0.6810 - val_f1_m: 0.6775\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6382 - acc: 0.7031 - f1_m: 0.7002 - val_loss: 0.6868 - val_acc: 0.6829 - val_f1_m: 0.6798\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6348 - acc: 0.7052 - f1_m: 0.7016 - val_loss: 0.6839 - val_acc: 0.6840 - val_f1_m: 0.6807\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6316 - acc: 0.7072 - f1_m: 0.7040 - val_loss: 0.6868 - val_acc: 0.6834 - val_f1_m: 0.6801\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6294 - acc: 0.7086 - f1_m: 0.7056 - val_loss: 0.6915 - val_acc: 0.6849 - val_f1_m: 0.6819\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6253 - acc: 0.7104 - f1_m: 0.7077 - val_loss: 0.6923 - val_acc: 0.6847 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6237 - acc: 0.7116 - f1_m: 0.7087 - val_loss: 0.6993 - val_acc: 0.6835 - val_f1_m: 0.6805\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6205 - acc: 0.7125 - f1_m: 0.7097 - val_loss: 0.6909 - val_acc: 0.6846 - val_f1_m: 0.6803\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6177 - acc: 0.7141 - f1_m: 0.7114 - val_loss: 0.6950 - val_acc: 0.6830 - val_f1_m: 0.6802\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6142 - acc: 0.7150 - f1_m: 0.7123 - val_loss: 0.7018 - val_acc: 0.6823 - val_f1_m: 0.6789\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6129 - acc: 0.7157 - f1_m: 0.7130 - val_loss: 0.6957 - val_acc: 0.6833 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6109 - acc: 0.7180 - f1_m: 0.7151 - val_loss: 0.6943 - val_acc: 0.6856 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6077 - acc: 0.7182 - f1_m: 0.7154 - val_loss: 0.6968 - val_acc: 0.6855 - val_f1_m: 0.6818\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6053 - acc: 0.7202 - f1_m: 0.7175 - val_loss: 0.7041 - val_acc: 0.6821 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6028 - acc: 0.7215 - f1_m: 0.7187 - val_loss: 0.6995 - val_acc: 0.6851 - val_f1_m: 0.6822\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6027 - acc: 0.7218 - f1_m: 0.7192 - val_loss: 0.7044 - val_acc: 0.6843 - val_f1_m: 0.6820\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6001 - acc: 0.7230 - f1_m: 0.7205 - val_loss: 0.7032 - val_acc: 0.6854 - val_f1_m: 0.6827\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5969 - acc: 0.7247 - f1_m: 0.7224 - val_loss: 0.7070 - val_acc: 0.6829 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5955 - acc: 0.7256 - f1_m: 0.7231 - val_loss: 0.7105 - val_acc: 0.6833 - val_f1_m: 0.6793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.5937 - acc: 0.7264 - f1_m: 0.7241 - val_loss: 0.7095 - val_acc: 0.6834 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5911 - acc: 0.7273 - f1_m: 0.7251 - val_loss: 0.7110 - val_acc: 0.6828 - val_f1_m: 0.6804\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5895 - acc: 0.7275 - f1_m: 0.7254 - val_loss: 0.7048 - val_acc: 0.6841 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.5574 - acc: 0.7426 - f1_m: 0.7408 - val_loss: 0.7258 - val_acc: 0.6869 - val_f1_m: 0.6845\n",
      " ### Learnig rate at the end of epoch 37 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.5427 - acc: 0.7495 - f1_m: 0.7475 - val_loss: 0.7378 - val_acc: 0.6870 - val_f1_m: 0.6844\n",
      " ### Learnig rate at the end of epoch 38 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5357 - acc: 0.7524 - f1_m: 0.7508 - val_loss: 0.7542 - val_acc: 0.6863 - val_f1_m: 0.6836\n",
      " ### Learnig rate at the end of epoch 39 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.5308 - acc: 0.7549 - f1_m: 0.7531 - val_loss: 0.7595 - val_acc: 0.6866 - val_f1_m: 0.6842\n",
      " ### Learnig rate at the end of epoch 40 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5269 - acc: 0.7567 - f1_m: 0.7552 - val_loss: 0.7672 - val_acc: 0.6850 - val_f1_m: 0.6825\n",
      " ### Learnig rate at the end of epoch 41 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.5235 - acc: 0.7578 - f1_m: 0.7564 - val_loss: 0.7751 - val_acc: 0.6863 - val_f1_m: 0.6839\n",
      " ### Learnig rate at the end of epoch 42 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.5203 - acc: 0.7598 - f1_m: 0.7581 - val_loss: 0.7858 - val_acc: 0.6843 - val_f1_m: 0.6817\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.5176 - acc: 0.7606 - f1_m: 0.7593 - val_loss: 0.7937 - val_acc: 0.6838 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.5149 - acc: 0.7623 - f1_m: 0.7610 - val_loss: 0.7961 - val_acc: 0.6844 - val_f1_m: 0.6825\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5125 - acc: 0.7637 - f1_m: 0.7622 - val_loss: 0.8031 - val_acc: 0.6836 - val_f1_m: 0.6809\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.5104 - acc: 0.7642 - f1_m: 0.7629 - val_loss: 0.8135 - val_acc: 0.6823 - val_f1_m: 0.6800\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5083 - acc: 0.7653 - f1_m: 0.7639 - val_loss: 0.8170 - val_acc: 0.6827 - val_f1_m: 0.6802\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.5002 - acc: 0.7697 - f1_m: 0.7682 - val_loss: 0.8266 - val_acc: 0.6820 - val_f1_m: 0.6797\n",
      " ### Learnig rate at the end of epoch 49 is 2.0000001313746907e-05 \n",
      "\n",
      "<keras.layers.advanced_activations.ReLU object at 0x18af93470>\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 13s 62us/step - loss: 0.7575 - acc: 0.6279 - f1_m: 0.6145 - val_loss: 0.7386 - val_acc: 0.6455 - val_f1_m: 0.6395\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.7192 - acc: 0.6559 - f1_m: 0.6499 - val_loss: 0.7255 - val_acc: 0.6586 - val_f1_m: 0.6538\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.7067 - acc: 0.6652 - f1_m: 0.6603 - val_loss: 0.7099 - val_acc: 0.6646 - val_f1_m: 0.6603\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6984 - acc: 0.6702 - f1_m: 0.6653 - val_loss: 0.7066 - val_acc: 0.6689 - val_f1_m: 0.6631\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.6918 - acc: 0.6746 - f1_m: 0.6702 - val_loss: 0.7035 - val_acc: 0.6714 - val_f1_m: 0.6651\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6865 - acc: 0.6782 - f1_m: 0.6740 - val_loss: 0.7019 - val_acc: 0.6702 - val_f1_m: 0.6661\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6811 - acc: 0.6807 - f1_m: 0.6761 - val_loss: 0.6997 - val_acc: 0.6712 - val_f1_m: 0.6655\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6763 - acc: 0.6832 - f1_m: 0.6783 - val_loss: 0.6955 - val_acc: 0.6725 - val_f1_m: 0.6665\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 12s 57us/step - loss: 0.6718 - acc: 0.6851 - f1_m: 0.6806 - val_loss: 0.6898 - val_acc: 0.6787 - val_f1_m: 0.6742\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.6669 - acc: 0.6884 - f1_m: 0.6844 - val_loss: 0.6915 - val_acc: 0.6774 - val_f1_m: 0.6724\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6631 - acc: 0.6901 - f1_m: 0.6860 - val_loss: 0.6919 - val_acc: 0.6773 - val_f1_m: 0.6729\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6595 - acc: 0.6929 - f1_m: 0.6887 - val_loss: 0.6906 - val_acc: 0.6794 - val_f1_m: 0.6751\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6553 - acc: 0.6944 - f1_m: 0.6901 - val_loss: 0.6904 - val_acc: 0.6772 - val_f1_m: 0.6718\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6530 - acc: 0.6964 - f1_m: 0.6925 - val_loss: 0.6914 - val_acc: 0.6798 - val_f1_m: 0.6760\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6492 - acc: 0.6971 - f1_m: 0.6935 - val_loss: 0.6895 - val_acc: 0.6822 - val_f1_m: 0.6779\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6461 - acc: 0.7003 - f1_m: 0.6964 - val_loss: 0.6904 - val_acc: 0.6793 - val_f1_m: 0.6738\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6434 - acc: 0.7012 - f1_m: 0.6974 - val_loss: 0.6891 - val_acc: 0.6805 - val_f1_m: 0.6767\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6396 - acc: 0.7029 - f1_m: 0.6991 - val_loss: 0.6891 - val_acc: 0.6823 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6374 - acc: 0.7041 - f1_m: 0.7004 - val_loss: 0.6888 - val_acc: 0.6803 - val_f1_m: 0.6764\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6346 - acc: 0.7055 - f1_m: 0.7023 - val_loss: 0.6864 - val_acc: 0.6840 - val_f1_m: 0.6802\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6319 - acc: 0.7073 - f1_m: 0.7034 - val_loss: 0.6906 - val_acc: 0.6814 - val_f1_m: 0.6772\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6293 - acc: 0.7078 - f1_m: 0.7044 - val_loss: 0.6944 - val_acc: 0.6787 - val_f1_m: 0.6752\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.6272 - acc: 0.7094 - f1_m: 0.7064 - val_loss: 0.6917 - val_acc: 0.6821 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6234 - acc: 0.7104 - f1_m: 0.7073 - val_loss: 0.7002 - val_acc: 0.6814 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6221 - acc: 0.7122 - f1_m: 0.7088 - val_loss: 0.6889 - val_acc: 0.6808 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6189 - acc: 0.7128 - f1_m: 0.7096 - val_loss: 0.6912 - val_acc: 0.6828 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6168 - acc: 0.7143 - f1_m: 0.7109 - val_loss: 0.6927 - val_acc: 0.6820 - val_f1_m: 0.6772\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6142 - acc: 0.7164 - f1_m: 0.7133 - val_loss: 0.6964 - val_acc: 0.6845 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6120 - acc: 0.7168 - f1_m: 0.7140 - val_loss: 0.6993 - val_acc: 0.6814 - val_f1_m: 0.6756\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6114 - acc: 0.7167 - f1_m: 0.7140 - val_loss: 0.7036 - val_acc: 0.6839 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6078 - acc: 0.7195 - f1_m: 0.7167 - val_loss: 0.7030 - val_acc: 0.6856 - val_f1_m: 0.6817\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6065 - acc: 0.7190 - f1_m: 0.7160 - val_loss: 0.7032 - val_acc: 0.6845 - val_f1_m: 0.6809\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6040 - acc: 0.7216 - f1_m: 0.7186 - val_loss: 0.7047 - val_acc: 0.6819 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6013 - acc: 0.7221 - f1_m: 0.7194 - val_loss: 0.7030 - val_acc: 0.6809 - val_f1_m: 0.6758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6000 - acc: 0.7233 - f1_m: 0.7206 - val_loss: 0.7111 - val_acc: 0.6805 - val_f1_m: 0.6764\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5995 - acc: 0.7228 - f1_m: 0.7203 - val_loss: 0.7144 - val_acc: 0.6853 - val_f1_m: 0.6817\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5966 - acc: 0.7250 - f1_m: 0.7222 - val_loss: 0.7163 - val_acc: 0.6838 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5935 - acc: 0.7265 - f1_m: 0.7240 - val_loss: 0.7120 - val_acc: 0.6821 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5928 - acc: 0.7262 - f1_m: 0.7236 - val_loss: 0.7149 - val_acc: 0.6853 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5904 - acc: 0.7278 - f1_m: 0.7251 - val_loss: 0.7211 - val_acc: 0.6822 - val_f1_m: 0.6794\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5888 - acc: 0.7287 - f1_m: 0.7260 - val_loss: 0.7152 - val_acc: 0.6830 - val_f1_m: 0.6797\n",
      " ### Learnig rate at the end of epoch 40 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5578 - acc: 0.7438 - f1_m: 0.7416 - val_loss: 0.7372 - val_acc: 0.6863 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 41 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5447 - acc: 0.7498 - f1_m: 0.7477 - val_loss: 0.7466 - val_acc: 0.6851 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 42 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5382 - acc: 0.7525 - f1_m: 0.7507 - val_loss: 0.7602 - val_acc: 0.6854 - val_f1_m: 0.6825\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5335 - acc: 0.7543 - f1_m: 0.7528 - val_loss: 0.7639 - val_acc: 0.6854 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5296 - acc: 0.7569 - f1_m: 0.7552 - val_loss: 0.7738 - val_acc: 0.6841 - val_f1_m: 0.6812\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5263 - acc: 0.7586 - f1_m: 0.7572 - val_loss: 0.7862 - val_acc: 0.6843 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5233 - acc: 0.7599 - f1_m: 0.7586 - val_loss: 0.7910 - val_acc: 0.6850 - val_f1_m: 0.6820\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5205 - acc: 0.7613 - f1_m: 0.7600 - val_loss: 0.7993 - val_acc: 0.6835 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5180 - acc: 0.7635 - f1_m: 0.7619 - val_loss: 0.8049 - val_acc: 0.6828 - val_f1_m: 0.6799\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "sigmoid\n",
      "3\n",
      "3\n",
      "3\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 10s 47us/step - loss: 0.7925 - acc: 0.5960 - f1_m: 0.5813 - val_loss: 0.7816 - val_acc: 0.6040 - val_f1_m: 0.5930\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.7688 - acc: 0.6144 - f1_m: 0.6031 - val_loss: 0.7648 - val_acc: 0.6222 - val_f1_m: 0.6139\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.7401 - acc: 0.6387 - f1_m: 0.6309 - val_loss: 0.7254 - val_acc: 0.6510 - val_f1_m: 0.6453\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.7148 - acc: 0.6593 - f1_m: 0.6551 - val_loss: 0.7141 - val_acc: 0.6654 - val_f1_m: 0.6605\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.7062 - acc: 0.6663 - f1_m: 0.6621 - val_loss: 0.7120 - val_acc: 0.6670 - val_f1_m: 0.6640\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6995 - acc: 0.6716 - f1_m: 0.6676 - val_loss: 0.7067 - val_acc: 0.6721 - val_f1_m: 0.6699\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6949 - acc: 0.6748 - f1_m: 0.6708 - val_loss: 0.7076 - val_acc: 0.6673 - val_f1_m: 0.6594\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6904 - acc: 0.6764 - f1_m: 0.6729 - val_loss: 0.7009 - val_acc: 0.6740 - val_f1_m: 0.6688\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6863 - acc: 0.6799 - f1_m: 0.6762 - val_loss: 0.7006 - val_acc: 0.6720 - val_f1_m: 0.6683\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6824 - acc: 0.6817 - f1_m: 0.6784 - val_loss: 0.6970 - val_acc: 0.6769 - val_f1_m: 0.6744\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6792 - acc: 0.6841 - f1_m: 0.6807 - val_loss: 0.6960 - val_acc: 0.6736 - val_f1_m: 0.6707\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6761 - acc: 0.6854 - f1_m: 0.6820 - val_loss: 0.6998 - val_acc: 0.6712 - val_f1_m: 0.6652\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6733 - acc: 0.6866 - f1_m: 0.6833 - val_loss: 0.6949 - val_acc: 0.6772 - val_f1_m: 0.6745\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6703 - acc: 0.6888 - f1_m: 0.6855 - val_loss: 0.6959 - val_acc: 0.6767 - val_f1_m: 0.6707\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6672 - acc: 0.6904 - f1_m: 0.6870 - val_loss: 0.6898 - val_acc: 0.6793 - val_f1_m: 0.6751\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6643 - acc: 0.6919 - f1_m: 0.6886 - val_loss: 0.6954 - val_acc: 0.6773 - val_f1_m: 0.6717\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6618 - acc: 0.6933 - f1_m: 0.6902 - val_loss: 0.6905 - val_acc: 0.6811 - val_f1_m: 0.6789\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6589 - acc: 0.6951 - f1_m: 0.6923 - val_loss: 0.6872 - val_acc: 0.6822 - val_f1_m: 0.6787\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6563 - acc: 0.6968 - f1_m: 0.6934 - val_loss: 0.6876 - val_acc: 0.6815 - val_f1_m: 0.6784\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6535 - acc: 0.6980 - f1_m: 0.6947 - val_loss: 0.6899 - val_acc: 0.6812 - val_f1_m: 0.6776\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6507 - acc: 0.6996 - f1_m: 0.6968 - val_loss: 0.6894 - val_acc: 0.6810 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6478 - acc: 0.7017 - f1_m: 0.6987 - val_loss: 0.6920 - val_acc: 0.6788 - val_f1_m: 0.6765\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6457 - acc: 0.7029 - f1_m: 0.6999 - val_loss: 0.6945 - val_acc: 0.6778 - val_f1_m: 0.6749\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6432 - acc: 0.7038 - f1_m: 0.7010 - val_loss: 0.6898 - val_acc: 0.6809 - val_f1_m: 0.6786\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6404 - acc: 0.7060 - f1_m: 0.7033 - val_loss: 0.6991 - val_acc: 0.6745 - val_f1_m: 0.6707\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6383 - acc: 0.7067 - f1_m: 0.7040 - val_loss: 0.6914 - val_acc: 0.6826 - val_f1_m: 0.6799\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6358 - acc: 0.7075 - f1_m: 0.7049 - val_loss: 0.6857 - val_acc: 0.6815 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6337 - acc: 0.7088 - f1_m: 0.7063 - val_loss: 0.6865 - val_acc: 0.6853 - val_f1_m: 0.6822\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6312 - acc: 0.7103 - f1_m: 0.7078 - val_loss: 0.6877 - val_acc: 0.6840 - val_f1_m: 0.6821\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6287 - acc: 0.7112 - f1_m: 0.7086 - val_loss: 0.6893 - val_acc: 0.6872 - val_f1_m: 0.6841\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6262 - acc: 0.7127 - f1_m: 0.7102 - val_loss: 0.6900 - val_acc: 0.6843 - val_f1_m: 0.6823\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6244 - acc: 0.7135 - f1_m: 0.7110 - val_loss: 0.6878 - val_acc: 0.6856 - val_f1_m: 0.6825\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6225 - acc: 0.7152 - f1_m: 0.7125 - val_loss: 0.6927 - val_acc: 0.6847 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6199 - acc: 0.7163 - f1_m: 0.7139 - val_loss: 0.6912 - val_acc: 0.6850 - val_f1_m: 0.6807\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6181 - acc: 0.7167 - f1_m: 0.7144 - val_loss: 0.6963 - val_acc: 0.6846 - val_f1_m: 0.6824\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 8s 40us/step - loss: 0.6159 - acc: 0.7183 - f1_m: 0.7161 - val_loss: 0.6978 - val_acc: 0.6812 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6134 - acc: 0.7185 - f1_m: 0.7161 - val_loss: 0.6935 - val_acc: 0.6863 - val_f1_m: 0.6837\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6120 - acc: 0.7206 - f1_m: 0.7181 - val_loss: 0.6987 - val_acc: 0.6884 - val_f1_m: 0.6861\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6095 - acc: 0.7218 - f1_m: 0.7192 - val_loss: 0.6949 - val_acc: 0.6866 - val_f1_m: 0.6838\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6077 - acc: 0.7225 - f1_m: 0.7202 - val_loss: 0.7051 - val_acc: 0.6796 - val_f1_m: 0.6755\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6056 - acc: 0.7224 - f1_m: 0.7202 - val_loss: 0.6991 - val_acc: 0.6870 - val_f1_m: 0.6845\n",
      " ### Learnig rate at the end of epoch 40 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6039 - acc: 0.7239 - f1_m: 0.7217 - val_loss: 0.7020 - val_acc: 0.6873 - val_f1_m: 0.6849\n",
      " ### Learnig rate at the end of epoch 41 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6018 - acc: 0.7256 - f1_m: 0.7234 - val_loss: 0.7021 - val_acc: 0.6843 - val_f1_m: 0.6810\n",
      " ### Learnig rate at the end of epoch 42 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6003 - acc: 0.7259 - f1_m: 0.7240 - val_loss: 0.7070 - val_acc: 0.6844 - val_f1_m: 0.6823\n",
      " ### Learnig rate at the end of epoch 43 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.5980 - acc: 0.7271 - f1_m: 0.7248 - val_loss: 0.7075 - val_acc: 0.6865 - val_f1_m: 0.6842\n",
      " ### Learnig rate at the end of epoch 44 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.5961 - acc: 0.7270 - f1_m: 0.7248 - val_loss: 0.7121 - val_acc: 0.6842 - val_f1_m: 0.6815\n",
      " ### Learnig rate at the end of epoch 45 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.5941 - acc: 0.7290 - f1_m: 0.7268 - val_loss: 0.7048 - val_acc: 0.6876 - val_f1_m: 0.6847\n",
      " ### Learnig rate at the end of epoch 46 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.5920 - acc: 0.7302 - f1_m: 0.7282 - val_loss: 0.7182 - val_acc: 0.6869 - val_f1_m: 0.6850\n",
      " ### Learnig rate at the end of epoch 47 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5714 - acc: 0.7403 - f1_m: 0.7386 - val_loss: 0.7236 - val_acc: 0.6873 - val_f1_m: 0.6851\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.5674 - acc: 0.7426 - f1_m: 0.7409 - val_loss: 0.7294 - val_acc: 0.6863 - val_f1_m: 0.6841\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "sigmoid\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 12s 56us/step - loss: 0.7965 - acc: 0.5913 - f1_m: 0.5761 - val_loss: 0.7811 - val_acc: 0.6084 - val_f1_m: 0.6007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.7678 - acc: 0.6144 - f1_m: 0.6032 - val_loss: 0.7673 - val_acc: 0.6170 - val_f1_m: 0.6089\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.7408 - acc: 0.6371 - f1_m: 0.6301 - val_loss: 0.7277 - val_acc: 0.6520 - val_f1_m: 0.6465\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.7162 - acc: 0.6575 - f1_m: 0.6531 - val_loss: 0.7182 - val_acc: 0.6599 - val_f1_m: 0.6560\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.7072 - acc: 0.6662 - f1_m: 0.6624 - val_loss: 0.7129 - val_acc: 0.6653 - val_f1_m: 0.6582\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.7002 - acc: 0.6714 - f1_m: 0.6675 - val_loss: 0.7089 - val_acc: 0.6641 - val_f1_m: 0.6598\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6955 - acc: 0.6736 - f1_m: 0.6702 - val_loss: 0.7039 - val_acc: 0.6703 - val_f1_m: 0.6677\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6907 - acc: 0.6770 - f1_m: 0.6737 - val_loss: 0.7002 - val_acc: 0.6721 - val_f1_m: 0.6682\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6866 - acc: 0.6803 - f1_m: 0.6770 - val_loss: 0.7054 - val_acc: 0.6705 - val_f1_m: 0.6651\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6826 - acc: 0.6829 - f1_m: 0.6794 - val_loss: 0.7098 - val_acc: 0.6720 - val_f1_m: 0.6694\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6791 - acc: 0.6846 - f1_m: 0.6816 - val_loss: 0.7043 - val_acc: 0.6686 - val_f1_m: 0.6622\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6749 - acc: 0.6865 - f1_m: 0.6837 - val_loss: 0.6981 - val_acc: 0.6744 - val_f1_m: 0.6717\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6713 - acc: 0.6880 - f1_m: 0.6851 - val_loss: 0.6936 - val_acc: 0.6769 - val_f1_m: 0.6753\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6676 - acc: 0.6903 - f1_m: 0.6875 - val_loss: 0.6911 - val_acc: 0.6785 - val_f1_m: 0.6757\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6640 - acc: 0.6925 - f1_m: 0.6896 - val_loss: 0.6889 - val_acc: 0.6801 - val_f1_m: 0.6774\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6615 - acc: 0.6935 - f1_m: 0.6909 - val_loss: 0.6894 - val_acc: 0.6811 - val_f1_m: 0.6793\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6575 - acc: 0.6958 - f1_m: 0.6929 - val_loss: 0.6896 - val_acc: 0.6776 - val_f1_m: 0.6745\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6548 - acc: 0.6970 - f1_m: 0.6944 - val_loss: 0.6843 - val_acc: 0.6842 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6518 - acc: 0.6987 - f1_m: 0.6962 - val_loss: 0.6889 - val_acc: 0.6814 - val_f1_m: 0.6788\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6489 - acc: 0.6998 - f1_m: 0.6971 - val_loss: 0.6912 - val_acc: 0.6786 - val_f1_m: 0.6767\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.6466 - acc: 0.7017 - f1_m: 0.6994 - val_loss: 0.6908 - val_acc: 0.6822 - val_f1_m: 0.6803\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6441 - acc: 0.7029 - f1_m: 0.7003 - val_loss: 0.6887 - val_acc: 0.6840 - val_f1_m: 0.6814\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6418 - acc: 0.7053 - f1_m: 0.7029 - val_loss: 0.6887 - val_acc: 0.6855 - val_f1_m: 0.6836\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.6387 - acc: 0.7059 - f1_m: 0.7035 - val_loss: 0.6829 - val_acc: 0.6867 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6362 - acc: 0.7074 - f1_m: 0.7050 - val_loss: 0.6826 - val_acc: 0.6874 - val_f1_m: 0.6841\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6342 - acc: 0.7082 - f1_m: 0.7057 - val_loss: 0.6857 - val_acc: 0.6849 - val_f1_m: 0.6806\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6317 - acc: 0.7090 - f1_m: 0.7066 - val_loss: 0.6928 - val_acc: 0.6841 - val_f1_m: 0.6813\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6299 - acc: 0.7102 - f1_m: 0.7077 - val_loss: 0.6878 - val_acc: 0.6857 - val_f1_m: 0.6828\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6278 - acc: 0.7114 - f1_m: 0.7087 - val_loss: 0.6896 - val_acc: 0.6851 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6259 - acc: 0.7120 - f1_m: 0.7095 - val_loss: 0.6892 - val_acc: 0.6837 - val_f1_m: 0.6812\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6240 - acc: 0.7124 - f1_m: 0.7102 - val_loss: 0.6933 - val_acc: 0.6841 - val_f1_m: 0.6805\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.6220 - acc: 0.7139 - f1_m: 0.7111 - val_loss: 0.6896 - val_acc: 0.6870 - val_f1_m: 0.6842\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6200 - acc: 0.7156 - f1_m: 0.7131 - val_loss: 0.6918 - val_acc: 0.6858 - val_f1_m: 0.6829\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6184 - acc: 0.7161 - f1_m: 0.7137 - val_loss: 0.6930 - val_acc: 0.6832 - val_f1_m: 0.6803\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6166 - acc: 0.7172 - f1_m: 0.7150 - val_loss: 0.6915 - val_acc: 0.6887 - val_f1_m: 0.6867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6145 - acc: 0.7181 - f1_m: 0.7155 - val_loss: 0.6952 - val_acc: 0.6815 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6127 - acc: 0.7189 - f1_m: 0.7167 - val_loss: 0.7013 - val_acc: 0.6846 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6107 - acc: 0.7203 - f1_m: 0.7181 - val_loss: 0.6955 - val_acc: 0.6863 - val_f1_m: 0.6823\n",
      " ### Learnig rate at the end of epoch 37 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6090 - acc: 0.7207 - f1_m: 0.7185 - val_loss: 0.6999 - val_acc: 0.6855 - val_f1_m: 0.6825\n",
      " ### Learnig rate at the end of epoch 38 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6075 - acc: 0.7222 - f1_m: 0.7200 - val_loss: 0.7065 - val_acc: 0.6844 - val_f1_m: 0.6818\n",
      " ### Learnig rate at the end of epoch 39 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.6055 - acc: 0.7225 - f1_m: 0.7201 - val_loss: 0.6955 - val_acc: 0.6857 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 40 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 7s 31us/step - loss: 0.6041 - acc: 0.7237 - f1_m: 0.7213 - val_loss: 0.6992 - val_acc: 0.6862 - val_f1_m: 0.6836\n",
      " ### Learnig rate at the end of epoch 41 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6020 - acc: 0.7242 - f1_m: 0.7221 - val_loss: 0.7110 - val_acc: 0.6871 - val_f1_m: 0.6854\n",
      " ### Learnig rate at the end of epoch 42 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.6006 - acc: 0.7251 - f1_m: 0.7229 - val_loss: 0.7019 - val_acc: 0.6868 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 43 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.5987 - acc: 0.7253 - f1_m: 0.7232 - val_loss: 0.7004 - val_acc: 0.6857 - val_f1_m: 0.6824\n",
      " ### Learnig rate at the end of epoch 44 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.5793 - acc: 0.7361 - f1_m: 0.7342 - val_loss: 0.7185 - val_acc: 0.6869 - val_f1_m: 0.6842\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5754 - acc: 0.7381 - f1_m: 0.7361 - val_loss: 0.7216 - val_acc: 0.6872 - val_f1_m: 0.6849\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.5739 - acc: 0.7388 - f1_m: 0.7370 - val_loss: 0.7270 - val_acc: 0.6871 - val_f1_m: 0.6848\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.5728 - acc: 0.7389 - f1_m: 0.7370 - val_loss: 0.7295 - val_acc: 0.6875 - val_f1_m: 0.6851\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5721 - acc: 0.7398 - f1_m: 0.7379 - val_loss: 0.7304 - val_acc: 0.6878 - val_f1_m: 0.6856\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n",
      "sigmoid\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/50\n",
      "208480/208480 [==============================] - 12s 58us/step - loss: 0.8017 - acc: 0.5886 - f1_m: 0.5740 - val_loss: 0.7892 - val_acc: 0.6032 - val_f1_m: 0.5921\n",
      " ### Learnig rate at the end of epoch 0 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 2/50\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.7724 - acc: 0.6134 - f1_m: 0.6029 - val_loss: 0.7725 - val_acc: 0.6179 - val_f1_m: 0.6039\n",
      " ### Learnig rate at the end of epoch 1 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 3/50\n",
      "208480/208480 [==============================] - 8s 41us/step - loss: 0.7407 - acc: 0.6377 - f1_m: 0.6312 - val_loss: 0.7353 - val_acc: 0.6377 - val_f1_m: 0.6342\n",
      " ### Learnig rate at the end of epoch 2 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 4/50\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.7214 - acc: 0.6535 - f1_m: 0.6488 - val_loss: 0.7269 - val_acc: 0.6560 - val_f1_m: 0.6512\n",
      " ### Learnig rate at the end of epoch 3 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 5/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.7135 - acc: 0.6620 - f1_m: 0.6579 - val_loss: 0.7222 - val_acc: 0.6617 - val_f1_m: 0.6588\n",
      " ### Learnig rate at the end of epoch 4 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 6/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.7065 - acc: 0.6681 - f1_m: 0.6646 - val_loss: 0.7145 - val_acc: 0.6633 - val_f1_m: 0.6582\n",
      " ### Learnig rate at the end of epoch 5 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 7/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.7012 - acc: 0.6714 - f1_m: 0.6681 - val_loss: 0.7085 - val_acc: 0.6686 - val_f1_m: 0.6659\n",
      " ### Learnig rate at the end of epoch 6 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 8/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6967 - acc: 0.6740 - f1_m: 0.6703 - val_loss: 0.7063 - val_acc: 0.6693 - val_f1_m: 0.6667\n",
      " ### Learnig rate at the end of epoch 7 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 9/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6928 - acc: 0.6762 - f1_m: 0.6729 - val_loss: 0.7034 - val_acc: 0.6703 - val_f1_m: 0.6675\n",
      " ### Learnig rate at the end of epoch 8 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 10/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6892 - acc: 0.6796 - f1_m: 0.6765 - val_loss: 0.7029 - val_acc: 0.6722 - val_f1_m: 0.6692\n",
      " ### Learnig rate at the end of epoch 9 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 11/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6858 - acc: 0.6807 - f1_m: 0.6777 - val_loss: 0.7082 - val_acc: 0.6685 - val_f1_m: 0.6651\n",
      " ### Learnig rate at the end of epoch 10 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 12/50\n",
      "208480/208480 [==============================] - 8s 40us/step - loss: 0.6824 - acc: 0.6829 - f1_m: 0.6797 - val_loss: 0.7010 - val_acc: 0.6740 - val_f1_m: 0.6697\n",
      " ### Learnig rate at the end of epoch 11 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 13/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6800 - acc: 0.6847 - f1_m: 0.6815 - val_loss: 0.6966 - val_acc: 0.6758 - val_f1_m: 0.6729\n",
      " ### Learnig rate at the end of epoch 12 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 14/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6770 - acc: 0.6859 - f1_m: 0.6826 - val_loss: 0.6985 - val_acc: 0.6742 - val_f1_m: 0.6704\n",
      " ### Learnig rate at the end of epoch 13 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 15/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6740 - acc: 0.6869 - f1_m: 0.6837 - val_loss: 0.7038 - val_acc: 0.6694 - val_f1_m: 0.6663\n",
      " ### Learnig rate at the end of epoch 14 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 16/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6709 - acc: 0.6893 - f1_m: 0.6864 - val_loss: 0.6964 - val_acc: 0.6761 - val_f1_m: 0.6736\n",
      " ### Learnig rate at the end of epoch 15 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 17/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6685 - acc: 0.6903 - f1_m: 0.6872 - val_loss: 0.6931 - val_acc: 0.6789 - val_f1_m: 0.6746\n",
      " ### Learnig rate at the end of epoch 16 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 18/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.6657 - acc: 0.6908 - f1_m: 0.6877 - val_loss: 0.6963 - val_acc: 0.6760 - val_f1_m: 0.6739\n",
      " ### Learnig rate at the end of epoch 17 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 19/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6627 - acc: 0.6923 - f1_m: 0.6891 - val_loss: 0.6952 - val_acc: 0.6801 - val_f1_m: 0.6776\n",
      " ### Learnig rate at the end of epoch 18 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 20/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6598 - acc: 0.6952 - f1_m: 0.6919 - val_loss: 0.6908 - val_acc: 0.6811 - val_f1_m: 0.6771\n",
      " ### Learnig rate at the end of epoch 19 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 21/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6570 - acc: 0.6956 - f1_m: 0.6925 - val_loss: 0.6955 - val_acc: 0.6760 - val_f1_m: 0.6723\n",
      " ### Learnig rate at the end of epoch 20 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 22/50\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.6544 - acc: 0.6974 - f1_m: 0.6944 - val_loss: 0.6901 - val_acc: 0.6807 - val_f1_m: 0.6786\n",
      " ### Learnig rate at the end of epoch 21 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 23/50\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6517 - acc: 0.6987 - f1_m: 0.6957 - val_loss: 0.6891 - val_acc: 0.6803 - val_f1_m: 0.6766\n",
      " ### Learnig rate at the end of epoch 22 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 24/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6489 - acc: 0.7000 - f1_m: 0.6974 - val_loss: 0.6929 - val_acc: 0.6802 - val_f1_m: 0.6770\n",
      " ### Learnig rate at the end of epoch 23 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 25/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.6464 - acc: 0.7015 - f1_m: 0.6987 - val_loss: 0.6938 - val_acc: 0.6789 - val_f1_m: 0.6757\n",
      " ### Learnig rate at the end of epoch 24 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 26/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.6439 - acc: 0.7034 - f1_m: 0.7007 - val_loss: 0.6924 - val_acc: 0.6812 - val_f1_m: 0.6769\n",
      " ### Learnig rate at the end of epoch 25 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 27/50\n",
      "208480/208480 [==============================] - 9s 41us/step - loss: 0.6415 - acc: 0.7044 - f1_m: 0.7015 - val_loss: 0.6894 - val_acc: 0.6841 - val_f1_m: 0.6816\n",
      " ### Learnig rate at the end of epoch 26 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 28/50\n",
      "208480/208480 [==============================] - 7s 36us/step - loss: 0.6395 - acc: 0.7050 - f1_m: 0.7025 - val_loss: 0.6896 - val_acc: 0.6831 - val_f1_m: 0.6807\n",
      " ### Learnig rate at the end of epoch 27 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 29/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.6374 - acc: 0.7070 - f1_m: 0.7044 - val_loss: 0.6897 - val_acc: 0.6820 - val_f1_m: 0.6786\n",
      " ### Learnig rate at the end of epoch 28 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 30/50\n",
      "208480/208480 [==============================] - 9s 41us/step - loss: 0.6348 - acc: 0.7081 - f1_m: 0.7053 - val_loss: 0.6923 - val_acc: 0.6817 - val_f1_m: 0.6783\n",
      " ### Learnig rate at the end of epoch 29 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 31/50\n",
      "208480/208480 [==============================] - 9s 41us/step - loss: 0.6329 - acc: 0.7097 - f1_m: 0.7072 - val_loss: 0.6896 - val_acc: 0.6831 - val_f1_m: 0.6785\n",
      " ### Learnig rate at the end of epoch 30 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 32/50\n",
      "208480/208480 [==============================] - 9s 42us/step - loss: 0.6304 - acc: 0.7106 - f1_m: 0.7081 - val_loss: 0.6954 - val_acc: 0.6822 - val_f1_m: 0.6791\n",
      " ### Learnig rate at the end of epoch 31 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 33/50\n",
      "208480/208480 [==============================] - 9s 41us/step - loss: 0.6286 - acc: 0.7111 - f1_m: 0.7086 - val_loss: 0.6963 - val_acc: 0.6835 - val_f1_m: 0.6808\n",
      " ### Learnig rate at the end of epoch 32 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 34/50\n",
      "208480/208480 [==============================] - 9s 42us/step - loss: 0.6265 - acc: 0.7117 - f1_m: 0.7092 - val_loss: 0.6941 - val_acc: 0.6816 - val_f1_m: 0.6769\n",
      " ### Learnig rate at the end of epoch 33 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 35/50\n",
      "208480/208480 [==============================] - 9s 43us/step - loss: 0.6247 - acc: 0.7134 - f1_m: 0.7111 - val_loss: 0.6980 - val_acc: 0.6820 - val_f1_m: 0.6770\n",
      " ### Learnig rate at the end of epoch 34 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 36/50\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.6224 - acc: 0.7143 - f1_m: 0.7119 - val_loss: 0.6926 - val_acc: 0.6810 - val_f1_m: 0.6778\n",
      " ### Learnig rate at the end of epoch 35 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 37/50\n",
      "208480/208480 [==============================] - 9s 41us/step - loss: 0.6208 - acc: 0.7151 - f1_m: 0.7129 - val_loss: 0.6982 - val_acc: 0.6822 - val_f1_m: 0.6799\n",
      " ### Learnig rate at the end of epoch 36 is 0.0020000000949949026 \n",
      "\n",
      "Epoch 38/50\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.6018 - acc: 0.7255 - f1_m: 0.7237 - val_loss: 0.6973 - val_acc: 0.6858 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 37 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 39/50\n",
      "208480/208480 [==============================] - 9s 43us/step - loss: 0.5981 - acc: 0.7279 - f1_m: 0.7258 - val_loss: 0.6998 - val_acc: 0.6862 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 38 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 40/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.5967 - acc: 0.7286 - f1_m: 0.7267 - val_loss: 0.6997 - val_acc: 0.6862 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 39 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 41/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5958 - acc: 0.7288 - f1_m: 0.7270 - val_loss: 0.7027 - val_acc: 0.6860 - val_f1_m: 0.6832\n",
      " ### Learnig rate at the end of epoch 40 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 42/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.5949 - acc: 0.7297 - f1_m: 0.7277 - val_loss: 0.7047 - val_acc: 0.6858 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 41 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 43/50\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.5942 - acc: 0.7296 - f1_m: 0.7275 - val_loss: 0.7039 - val_acc: 0.6860 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 42 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 44/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.5934 - acc: 0.7301 - f1_m: 0.7285 - val_loss: 0.7022 - val_acc: 0.6852 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 43 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 45/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5929 - acc: 0.7305 - f1_m: 0.7286 - val_loss: 0.7085 - val_acc: 0.6859 - val_f1_m: 0.6833\n",
      " ### Learnig rate at the end of epoch 44 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 46/50\n",
      "208480/208480 [==============================] - 8s 36us/step - loss: 0.5923 - acc: 0.7307 - f1_m: 0.7288 - val_loss: 0.7083 - val_acc: 0.6861 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 45 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 47/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5917 - acc: 0.7309 - f1_m: 0.7291 - val_loss: 0.7072 - val_acc: 0.6864 - val_f1_m: 0.6836\n",
      " ### Learnig rate at the end of epoch 46 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 48/50\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5912 - acc: 0.7320 - f1_m: 0.7301 - val_loss: 0.7089 - val_acc: 0.6851 - val_f1_m: 0.6824\n",
      " ### Learnig rate at the end of epoch 47 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 49/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.5906 - acc: 0.7318 - f1_m: 0.7299 - val_loss: 0.7086 - val_acc: 0.6843 - val_f1_m: 0.6822\n",
      " ### Learnig rate at the end of epoch 48 is 0.00020000000949949026 \n",
      "\n",
      "Epoch 50/50\n",
      "208480/208480 [==============================] - 8s 37us/step - loss: 0.5900 - acc: 0.7321 - f1_m: 0.7300 - val_loss: 0.7104 - val_acc: 0.6853 - val_f1_m: 0.6836\n",
      " ### Learnig rate at the end of epoch 49 is 0.00020000000949949026 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using optimizer Nadam\n",
    "best_activations = ['relu','tanh',ReLU(max_value=None, negative_slope=0.0, threshold=0.0),'sigmoid']\n",
    "\n",
    "lr_call = lr_mon()\n",
    "lr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=0, \n",
    "                                               mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(best_activations)):\n",
    "    for j in range(3,6):\n",
    "        nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "        print(best_activations[i])\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=128, activation=best_activations[i], input_dim=60))\n",
    "        for k in range(j):\n",
    "            model.add(Dense(units=128, activation=best_activations[i]))\n",
    "            print(j)\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy',f1_m])\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=128,validation_data=(X_val,y_val),\n",
    "                        callbacks=[lr_call,lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/100\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.1853 - acc: 0.5927 - f1_m: 0.5917 - val_loss: 0.1762 - val_acc: 0.6162 - val_f1_m: 0.6162\n",
      " ### Learnig rate at the end of epoch 0 is 0.019999999552965164 \n",
      "\n",
      "Epoch 2/100\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.1777 - acc: 0.6153 - f1_m: 0.6150 - val_loss: 0.1839 - val_acc: 0.6120 - val_f1_m: 0.6120\n",
      " ### Learnig rate at the end of epoch 1 is 0.019999999552965164 \n",
      "\n",
      "Epoch 3/100\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.1762 - acc: 0.6214 - f1_m: 0.6214 - val_loss: 0.1749 - val_acc: 0.6264 - val_f1_m: 0.6264\n",
      " ### Learnig rate at the end of epoch 2 is 0.019999999552965164 \n",
      "\n",
      "Epoch 4/100\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.1754 - acc: 0.6234 - f1_m: 0.6231 - val_loss: 0.1765 - val_acc: 0.6209 - val_f1_m: 0.6209\n",
      " ### Learnig rate at the end of epoch 3 is 0.019999999552965164 \n",
      "\n",
      "Epoch 5/100\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.1753 - acc: 0.6244 - f1_m: 0.6244 - val_loss: 0.1736 - val_acc: 0.6271 - val_f1_m: 0.6271\n",
      " ### Learnig rate at the end of epoch 4 is 0.019999999552965164 \n",
      "\n",
      "Epoch 6/100\n",
      "208480/208480 [==============================] - 4s 18us/step - loss: 0.1742 - acc: 0.6265 - f1_m: 0.6265 - val_loss: 0.1733 - val_acc: 0.6227 - val_f1_m: 0.6227\n",
      " ### Learnig rate at the end of epoch 5 is 0.019999999552965164 \n",
      "\n",
      "Epoch 7/100\n",
      "208480/208480 [==============================] - 4s 18us/step - loss: 0.1736 - acc: 0.6265 - f1_m: 0.6265 - val_loss: 0.1738 - val_acc: 0.6267 - val_f1_m: 0.6267\n",
      " ### Learnig rate at the end of epoch 6 is 0.019999999552965164 \n",
      "\n",
      "Epoch 8/100\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.1729 - acc: 0.6278 - f1_m: 0.6278 - val_loss: 0.1728 - val_acc: 0.6278 - val_f1_m: 0.6278\n",
      " ### Learnig rate at the end of epoch 7 is 0.019999999552965164 \n",
      "\n",
      "Epoch 9/100\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.1726 - acc: 0.6299 - f1_m: 0.6299 - val_loss: 0.1814 - val_acc: 0.6049 - val_f1_m: 0.6049\n",
      " ### Learnig rate at the end of epoch 8 is 0.019999999552965164 \n",
      "\n",
      "Epoch 10/100\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.1729 - acc: 0.6283 - f1_m: 0.6283 - val_loss: 0.1731 - val_acc: 0.6308 - val_f1_m: 0.6308\n",
      " ### Learnig rate at the end of epoch 9 is 0.019999999552965164 \n",
      "\n",
      "Epoch 11/100\n",
      "208480/208480 [==============================] - 4s 18us/step - loss: 0.1728 - acc: 0.6294 - f1_m: 0.6294 - val_loss: 0.1717 - val_acc: 0.6327 - val_f1_m: 0.6327\n",
      " ### Learnig rate at the end of epoch 10 is 0.019999999552965164 \n",
      "\n",
      "Epoch 12/100\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.1727 - acc: 0.6280 - f1_m: 0.6280 - val_loss: 0.1765 - val_acc: 0.6095 - val_f1_m: 0.6095\n",
      " ### Learnig rate at the end of epoch 11 is 0.019999999552965164 \n",
      "\n",
      "Epoch 13/100\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.1729 - acc: 0.6290 - f1_m: 0.6288 - val_loss: 0.1725 - val_acc: 0.6270 - val_f1_m: 0.6270\n",
      " ### Learnig rate at the end of epoch 12 is 0.019999999552965164 \n",
      "\n",
      "Epoch 14/100\n",
      "208480/208480 [==============================] - 4s 18us/step - loss: 0.1725 - acc: 0.6296 - f1_m: 0.6296 - val_loss: 0.1728 - val_acc: 0.6274 - val_f1_m: 0.6274\n",
      " ### Learnig rate at the end of epoch 13 is 0.019999999552965164 \n",
      "\n",
      "Epoch 15/100\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.1726 - acc: 0.6278 - f1_m: 0.6278 - val_loss: 0.1734 - val_acc: 0.6267 - val_f1_m: 0.6267\n",
      " ### Learnig rate at the end of epoch 14 is 0.019999999552965164 \n",
      "\n",
      "Epoch 16/100\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.1726 - acc: 0.6279 - f1_m: 0.6278 - val_loss: 0.1732 - val_acc: 0.6257 - val_f1_m: 0.6257\n",
      " ### Learnig rate at the end of epoch 15 is 0.019999999552965164 \n",
      "\n",
      "Epoch 17/100\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.1726 - acc: 0.6288 - f1_m: 0.6288 - val_loss: 0.1721 - val_acc: 0.6292 - val_f1_m: 0.6292\n",
      " ### Learnig rate at the end of epoch 16 is 0.019999999552965164 \n",
      "\n",
      "Epoch 18/100\n",
      "208480/208480 [==============================] - 4s 18us/step - loss: 0.1725 - acc: 0.6296 - f1_m: 0.6296 - val_loss: 0.1738 - val_acc: 0.6304 - val_f1_m: 0.6304\n",
      " ### Learnig rate at the end of epoch 17 is 0.019999999552965164 \n",
      "\n",
      "Epoch 19/100\n",
      "208480/208480 [==============================] - 4s 18us/step - loss: 0.1730 - acc: 0.6293 - f1_m: 0.6293 - val_loss: 0.1752 - val_acc: 0.6249 - val_f1_m: 0.6249\n",
      " ### Learnig rate at the end of epoch 18 is 0.019999999552965164 \n",
      "\n",
      "Epoch 20/100\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.1734 - acc: 0.6269 - f1_m: 0.6269 - val_loss: 0.1776 - val_acc: 0.6154 - val_f1_m: 0.6154\n",
      " ### Learnig rate at the end of epoch 19 is 0.019999999552965164 \n",
      "\n",
      "Epoch 21/100\n",
      "208480/208480 [==============================] - 4s 19us/step - loss: 0.1730 - acc: 0.6286 - f1_m: 0.6286 - val_loss: 0.1724 - val_acc: 0.6308 - val_f1_m: 0.6308\n",
      " ### Learnig rate at the end of epoch 20 is 0.019999999552965164 \n",
      "\n",
      "Epoch 22/100\n",
      "208480/208480 [==============================] - 4s 21us/step - loss: 0.1707 - acc: 0.6363 - f1_m: 0.6363 - val_loss: 0.1713 - val_acc: 0.6343 - val_f1_m: 0.6343\n",
      " ### Learnig rate at the end of epoch 21 is 0.001999999862164259 \n",
      "\n",
      "Epoch 23/100\n",
      "208480/208480 [==============================] - 4s 20us/step - loss: 0.1701 - acc: 0.6387 - f1_m: 0.6387 - val_loss: 0.1708 - val_acc: 0.6367 - val_f1_m: 0.6367\n",
      " ### Learnig rate at the end of epoch 22 is 0.001999999862164259 \n",
      "\n",
      "Epoch 24/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.1697 - acc: 0.6398 - f1_m: 0.6398 - val_loss: 0.1705 - val_acc: 0.6384 - val_f1_m: 0.6384\n",
      " ### Learnig rate at the end of epoch 23 is 0.001999999862164259 \n",
      "\n",
      "Epoch 25/100\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.1693 - acc: 0.6414 - f1_m: 0.6414 - val_loss: 0.1706 - val_acc: 0.6361 - val_f1_m: 0.6361\n",
      " ### Learnig rate at the end of epoch 24 is 0.001999999862164259 \n",
      "\n",
      "Epoch 26/100\n",
      "207360/208480 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.6431 - f1_m: 0.6431"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-a045fa82b434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m history = model.fit(X_train, y_train, epochs=100, batch_size=128,validation_data=(X_val,y_val),\n\u001b[0;32m---> 24\u001b[0;31m                     callbacks=[lr_call,lr_scheduler])\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer Nadam, activation ReLU\n",
    "RELU = ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
    "\n",
    "nadam = keras.optimizers.Nadam(lr=0.02, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "adam = keras.optimizers.Adam(lr=0.0001,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "RMS = keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "lr_call = lr_mon()\n",
    "lr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=0, \n",
    "                                               mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation=RELU, input_dim=60))\n",
    "model.add(Dense(units=128, activation='sigmoid'))\n",
    "model.add(Dense(units=128, activation='sigmoid'))\n",
    "model.add(Dense(units=128, activation='sigmoid'))\n",
    "#model.add(Dense(units=128, activation=RELU))\n",
    "#model.add(Dense(units=128, activation=RELU))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse', optimizer=nadam, metrics=['accuracy',f1_m])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128,validation_data=(X_val,y_val),\n",
    "                    callbacks=[lr_call,lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-2634e2ea1235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "X_train.concat(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(np.array(a))\n",
    "        dataY.append(np.array(dataset[i + look_back:i+look_back+1]))\n",
    "\n",
    "    return np.array(dataX),np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = create_dataset(train, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  import sys\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, input_shape=(None, 60), units=100)`\n",
      "  import sys\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=60)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.reset_states()\n",
    "\n",
    "model.add(LSTM(\n",
    "    input_dim=60,\n",
    "    output_dim=100,\n",
    "    return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "    100,\n",
    "    return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(\n",
    "    output_dim=3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam',activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=60, activation='sigmoid')\n",
    "    model.add(Dense(units=128, activation=preLU))\n",
    "    model.add(Dense(units=128, activation=preLU))\n",
    "    model.add(Dense(units=128, activation=preLU))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    adamax = keras.optimizers.Adamax(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adamax, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "#activations = ['relu','tanh','sigmoid','elu']\n",
    "#basic_activations = ['tanh','sigmoid']\n",
    "#adv_activations = [LeakyReLU(alpha=0.3),\n",
    "              #    ReLU(max_value=None, negative_slope=0.0, threshold=0.0)]\n",
    "adv_activations = [ReLU(max_value=None, negative_slope=0.0, threshold=0.0)]\n",
    "#optimizers = ['RMSprop','adam','adagrad']\n",
    "#optimizers = ['adadelta', 'adamax', 'nadam']\n",
    "#best_basic_opt = ['adam','adamax']\n",
    "#best_adv_opt = ['adam','adamax']\n",
    "best_acts = ['sigmoid',ReLU(max_value=None, negative_slope=0.0, threshold=0.0)]\n",
    "best_opt = ['adamax']\n",
    "param_grid = dict(optimizer=best_opt,activation=best_acts)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=0, \n",
    "                                               #mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "grid_result = grid.fit(X, y_labels) \n",
    "                       #validation_data=(X_val,y_val),callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.672684 using {'activation': 'sigmoid', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# optimizers RMSprop and adam and adagrad of basic activations\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.672856 using {'activation': 'tanh', 'optimizer': 'adamax'}\n"
     ]
    }
   ],
   "source": [
    "# optimizers adadelta', 'adamax', 'nadam' of basic activations\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.681463 using {'activation': 'sigmoid', 'optimizer': 'adamax'}\n"
     ]
    }
   ],
   "source": [
    "# best of basic activations/opt sigmoid adamax\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.671340 using {'activation': <keras.layers.advanced_activations.ReLU object at 0x142ef62b0>, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# optimizers RMSprop and adam and adagrad of advanced activations\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.670036 using {'activation': <keras.layers.advanced_activations.ReLU object at 0x13f2d46a0>, 'optimizer': 'adamax'}\n"
     ]
    }
   ],
   "source": [
    "# optimizers adadelta', 'adamax', 'nadam' of advanced activations\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.675769 using {'activation': <keras.layers.advanced_activations.ReLU object at 0x14190bd30>, 'optimizer': 'adamax'}\n"
     ]
    }
   ],
   "source": [
    "# best of advanced activations/opt \n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.682818 using {'activation': 'sigmoid', 'optimizer': 'adamax'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of train values and labels\n",
    "Train_values_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_values.csv\")\n",
    "train_labels_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of test values\n",
    "Test_values_df = get_data(\"https://s3.amazonaws.com/drivendata/data/57/public/test_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_dummies = pd.get_dummies(Train_values_df,drop_first=True)\n",
    "Test_dummies = pd.get_dummies(Test_values_df,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Specs          Score\n",
      "0                              building_id  474738.212863\n",
      "2                           geo_level_2_id  128238.967548\n",
      "5                                      age   89437.601578\n",
      "3                           geo_level_3_id   83664.759136\n",
      "40                     ground_floor_type_v   32465.421066\n",
      "38                             roof_type_x   28048.595012\n",
      "33                       foundation_type_i   27929.304672\n",
      "1                           geo_level_1_id   24186.950757\n",
      "13  has_superstructure_cement_mortar_brick   18949.414382\n",
      "44                      other_floor_type_s   18549.408221\n",
      "17        has_superstructure_rc_engineered   14411.297027\n",
      "6                          area_percentage   11938.314627\n",
      "16    has_superstructure_rc_non_engineered    8725.518483\n",
      "36                       foundation_type_w    8315.794578\n",
      "9      has_superstructure_mud_mortar_stone    6969.601844\n",
      "34                       foundation_type_r    6391.952318\n",
      "35                       foundation_type_u    5494.248443\n",
      "43                      other_floor_type_q    5108.461280\n",
      "22                 has_secondary_use_hotel    2937.304376\n",
      "23                has_secondary_use_rental    2635.341540\n",
      "20                       has_secondary_use    1485.663306\n",
      "8             has_superstructure_adobe_mud    1340.145544\n",
      "4                      count_floors_pre_eq    1241.172305\n",
      "37                             roof_type_q    1156.976251\n",
      "10           has_superstructure_stone_flag    1108.404575\n",
      "57                    plan_configuration_u    1011.761619\n",
      "12     has_superstructure_mud_mortar_brick     987.065751\n",
      "15               has_superstructure_bamboo     981.491828\n",
      "14               has_superstructure_timber     977.290073\n",
      "11  has_superstructure_cement_mortar_stone     936.793309\n",
      "48                              position_t     622.421247\n",
      "21           has_secondary_use_agriculture     540.461154\n",
      "7                        height_percentage     499.970897\n",
      "45                      other_floor_type_x     406.602490\n",
      "55                    plan_configuration_q     323.110604\n",
      "24           has_secondary_use_institution     292.860658\n",
      "18                has_superstructure_other     279.470979\n",
      "60                legal_ownership_status_w     229.493063\n",
      "19                          count_families     168.949183\n",
      "49                    plan_configuration_c     150.572497\n"
     ]
    }
   ],
   "source": [
    "bestfeatures = SelectKBest(score_func=chi2, k=40)\n",
    "fit = bestfeatures.fit(Train_dummies,y_labels)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(Train_dummies.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']\n",
    "ft_cols = featureScores.nlargest(40,'Score')\n",
    "print(ft_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels_df['damage_grade'])\n",
    "y_labels = keras.utils.to_categorical(train_labels-1, num_classes=3,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = (set(Train_dummies.columns)).difference(set(ft_cols['Specs']))\n",
    "X_features = Train_dummies.drop(labels=list(drop_cols),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>...</th>\n",
       "      <th>roof_type_x</th>\n",
       "      <th>ground_floor_type_v</th>\n",
       "      <th>other_floor_type_q</th>\n",
       "      <th>other_floor_type_s</th>\n",
       "      <th>other_floor_type_x</th>\n",
       "      <th>position_t</th>\n",
       "      <th>plan_configuration_c</th>\n",
       "      <th>plan_configuration_q</th>\n",
       "      <th>plan_configuration_u</th>\n",
       "      <th>legal_ownership_status_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802906</td>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>12198</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28830</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>2812</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94947</td>\n",
       "      <td>21</td>\n",
       "      <td>363</td>\n",
       "      <td>8973</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>590882</td>\n",
       "      <td>22</td>\n",
       "      <td>418</td>\n",
       "      <td>10694</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201944</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>1488</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>333020</td>\n",
       "      <td>8</td>\n",
       "      <td>558</td>\n",
       "      <td>6089</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>728451</td>\n",
       "      <td>9</td>\n",
       "      <td>475</td>\n",
       "      <td>12066</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>475515</td>\n",
       "      <td>20</td>\n",
       "      <td>323</td>\n",
       "      <td>12236</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>441126</td>\n",
       "      <td>0</td>\n",
       "      <td>757</td>\n",
       "      <td>7219</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>989500</td>\n",
       "      <td>26</td>\n",
       "      <td>886</td>\n",
       "      <td>994</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7962</td>\n",
       "      <td>17</td>\n",
       "      <td>1119</td>\n",
       "      <td>12188</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>452227</td>\n",
       "      <td>17</td>\n",
       "      <td>1275</td>\n",
       "      <td>4004</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>444381</td>\n",
       "      <td>12</td>\n",
       "      <td>335</td>\n",
       "      <td>8167</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>287845</td>\n",
       "      <td>17</td>\n",
       "      <td>817</td>\n",
       "      <td>6284</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>605134</td>\n",
       "      <td>18</td>\n",
       "      <td>1295</td>\n",
       "      <td>6994</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>563431</td>\n",
       "      <td>4</td>\n",
       "      <td>484</td>\n",
       "      <td>11114</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>633759</td>\n",
       "      <td>6</td>\n",
       "      <td>706</td>\n",
       "      <td>12267</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>980230</td>\n",
       "      <td>27</td>\n",
       "      <td>216</td>\n",
       "      <td>12323</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>354011</td>\n",
       "      <td>26</td>\n",
       "      <td>1401</td>\n",
       "      <td>3904</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>483543</td>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "      <td>6024</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>793397</td>\n",
       "      <td>27</td>\n",
       "      <td>216</td>\n",
       "      <td>12323</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>433870</td>\n",
       "      <td>25</td>\n",
       "      <td>211</td>\n",
       "      <td>10587</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>219578</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>11215</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>933368</td>\n",
       "      <td>4</td>\n",
       "      <td>1235</td>\n",
       "      <td>4976</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>519606</td>\n",
       "      <td>11</td>\n",
       "      <td>660</td>\n",
       "      <td>9780</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>801078</td>\n",
       "      <td>21</td>\n",
       "      <td>1051</td>\n",
       "      <td>759</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>973909</td>\n",
       "      <td>7</td>\n",
       "      <td>1328</td>\n",
       "      <td>5612</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>858160</td>\n",
       "      <td>4</td>\n",
       "      <td>707</td>\n",
       "      <td>12040</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>466558</td>\n",
       "      <td>3</td>\n",
       "      <td>1229</td>\n",
       "      <td>7251</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>529366</td>\n",
       "      <td>27</td>\n",
       "      <td>548</td>\n",
       "      <td>1119</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260572</th>\n",
       "      <td>37739</td>\n",
       "      <td>12</td>\n",
       "      <td>1254</td>\n",
       "      <td>5629</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260573</th>\n",
       "      <td>333974</td>\n",
       "      <td>7</td>\n",
       "      <td>1306</td>\n",
       "      <td>2696</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260574</th>\n",
       "      <td>663276</td>\n",
       "      <td>27</td>\n",
       "      <td>1155</td>\n",
       "      <td>5535</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260575</th>\n",
       "      <td>131692</td>\n",
       "      <td>7</td>\n",
       "      <td>901</td>\n",
       "      <td>12376</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260576</th>\n",
       "      <td>29215</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>5751</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260577</th>\n",
       "      <td>79341</td>\n",
       "      <td>17</td>\n",
       "      <td>930</td>\n",
       "      <td>2687</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260578</th>\n",
       "      <td>185096</td>\n",
       "      <td>22</td>\n",
       "      <td>1152</td>\n",
       "      <td>7292</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260579</th>\n",
       "      <td>998775</td>\n",
       "      <td>6</td>\n",
       "      <td>460</td>\n",
       "      <td>6863</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260580</th>\n",
       "      <td>977263</td>\n",
       "      <td>8</td>\n",
       "      <td>696</td>\n",
       "      <td>4415</td>\n",
       "      <td>2</td>\n",
       "      <td>995</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260581</th>\n",
       "      <td>262990</td>\n",
       "      <td>17</td>\n",
       "      <td>875</td>\n",
       "      <td>10462</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260582</th>\n",
       "      <td>518494</td>\n",
       "      <td>18</td>\n",
       "      <td>134</td>\n",
       "      <td>9577</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260583</th>\n",
       "      <td>557125</td>\n",
       "      <td>20</td>\n",
       "      <td>385</td>\n",
       "      <td>1686</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260584</th>\n",
       "      <td>192421</td>\n",
       "      <td>7</td>\n",
       "      <td>322</td>\n",
       "      <td>2843</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260585</th>\n",
       "      <td>317439</td>\n",
       "      <td>30</td>\n",
       "      <td>106</td>\n",
       "      <td>3085</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260586</th>\n",
       "      <td>722577</td>\n",
       "      <td>6</td>\n",
       "      <td>744</td>\n",
       "      <td>10556</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260587</th>\n",
       "      <td>365117</td>\n",
       "      <td>10</td>\n",
       "      <td>1227</td>\n",
       "      <td>1225</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260588</th>\n",
       "      <td>603159</td>\n",
       "      <td>8</td>\n",
       "      <td>696</td>\n",
       "      <td>4513</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260589</th>\n",
       "      <td>595396</td>\n",
       "      <td>27</td>\n",
       "      <td>269</td>\n",
       "      <td>6906</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260590</th>\n",
       "      <td>858025</td>\n",
       "      <td>6</td>\n",
       "      <td>673</td>\n",
       "      <td>12181</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260591</th>\n",
       "      <td>695987</td>\n",
       "      <td>20</td>\n",
       "      <td>922</td>\n",
       "      <td>10283</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260592</th>\n",
       "      <td>560805</td>\n",
       "      <td>20</td>\n",
       "      <td>368</td>\n",
       "      <td>5980</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260593</th>\n",
       "      <td>207683</td>\n",
       "      <td>10</td>\n",
       "      <td>1382</td>\n",
       "      <td>1903</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260594</th>\n",
       "      <td>226421</td>\n",
       "      <td>8</td>\n",
       "      <td>767</td>\n",
       "      <td>8613</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260595</th>\n",
       "      <td>159555</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "      <td>1537</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260596</th>\n",
       "      <td>827012</td>\n",
       "      <td>8</td>\n",
       "      <td>268</td>\n",
       "      <td>4718</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260597</th>\n",
       "      <td>688636</td>\n",
       "      <td>25</td>\n",
       "      <td>1335</td>\n",
       "      <td>1621</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260598</th>\n",
       "      <td>669485</td>\n",
       "      <td>17</td>\n",
       "      <td>715</td>\n",
       "      <td>2060</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260599</th>\n",
       "      <td>602512</td>\n",
       "      <td>17</td>\n",
       "      <td>51</td>\n",
       "      <td>8163</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260600</th>\n",
       "      <td>151409</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>1851</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260601</th>\n",
       "      <td>747594</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>9101</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260601 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        building_id  geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "1            802906               6             487           12198   \n",
       "2             28830               8             900            2812   \n",
       "3             94947              21             363            8973   \n",
       "4            590882              22             418           10694   \n",
       "5            201944              11             131            1488   \n",
       "6            333020               8             558            6089   \n",
       "7            728451               9             475           12066   \n",
       "8            475515              20             323           12236   \n",
       "9            441126               0             757            7219   \n",
       "10           989500              26             886             994   \n",
       "11             7962              17            1119           12188   \n",
       "12           452227              17            1275            4004   \n",
       "13           444381              12             335            8167   \n",
       "14           287845              17             817            6284   \n",
       "15           605134              18            1295            6994   \n",
       "16           563431               4             484           11114   \n",
       "17           633759               6             706           12267   \n",
       "18           980230              27             216           12323   \n",
       "19           354011              26            1401            3904   \n",
       "20           483543               8              41            6024   \n",
       "21           793397              27             216           12323   \n",
       "22           433870              25             211           10587   \n",
       "23           219578              10              90           11215   \n",
       "24           933368               4            1235            4976   \n",
       "25           519606              11             660            9780   \n",
       "26           801078              21            1051             759   \n",
       "27           973909               7            1328            5612   \n",
       "28           858160               4             707           12040   \n",
       "29           466558               3            1229            7251   \n",
       "30           529366              27             548            1119   \n",
       "...             ...             ...             ...             ...   \n",
       "260572        37739              12            1254            5629   \n",
       "260573       333974               7            1306            2696   \n",
       "260574       663276              27            1155            5535   \n",
       "260575       131692               7             901           12376   \n",
       "260576        29215               4             144            5751   \n",
       "260577        79341              17             930            2687   \n",
       "260578       185096              22            1152            7292   \n",
       "260579       998775               6             460            6863   \n",
       "260580       977263               8             696            4415   \n",
       "260581       262990              17             875           10462   \n",
       "260582       518494              18             134            9577   \n",
       "260583       557125              20             385            1686   \n",
       "260584       192421               7             322            2843   \n",
       "260585       317439              30             106            3085   \n",
       "260586       722577               6             744           10556   \n",
       "260587       365117              10            1227            1225   \n",
       "260588       603159               8             696            4513   \n",
       "260589       595396              27             269            6906   \n",
       "260590       858025               6             673           12181   \n",
       "260591       695987              20             922           10283   \n",
       "260592       560805              20             368            5980   \n",
       "260593       207683              10            1382            1903   \n",
       "260594       226421               8             767            8613   \n",
       "260595       159555              27             181            1537   \n",
       "260596       827012               8             268            4718   \n",
       "260597       688636              25            1335            1621   \n",
       "260598       669485              17             715            2060   \n",
       "260599       602512              17              51            8163   \n",
       "260600       151409              26              39            1851   \n",
       "260601       747594              21               9            9101   \n",
       "\n",
       "        count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "1                         2   30                6                  5   \n",
       "2                         2   10                8                  7   \n",
       "3                         2   10                5                  5   \n",
       "4                         2   10                6                  5   \n",
       "5                         3   30                8                  9   \n",
       "6                         2   10                9                  5   \n",
       "7                         2   25                3                  4   \n",
       "8                         2    0                8                  6   \n",
       "9                         2   15                8                  6   \n",
       "10                        1    0               13                  4   \n",
       "11                        2   20                9                  6   \n",
       "12                        1   10                8                  4   \n",
       "13                        2   15                6                  5   \n",
       "14                        3   45                7                  7   \n",
       "15                        2   25                7                  6   \n",
       "16                        2   55                4                  4   \n",
       "17                        2    5                7                  5   \n",
       "18                        3   10                7                  7   \n",
       "19                        1   20               12                  3   \n",
       "20                        2   20               16                  4   \n",
       "21                        3   40                7                  5   \n",
       "22                        2   30                6                  5   \n",
       "23                        2    0                6                  5   \n",
       "24                        3   25                6                  6   \n",
       "25                        2   80                5                  5   \n",
       "26                        2   40                5                  5   \n",
       "27                        3   10               11                  5   \n",
       "28                        2   15                8                  5   \n",
       "29                        2   10                7                  6   \n",
       "30                        3   60               27                 10   \n",
       "...                     ...  ...              ...                ...   \n",
       "260572                    2   15                7                  4   \n",
       "260573                    2   10                6                  5   \n",
       "260574                    2   35                5                  6   \n",
       "260575                    1   10                5                  3   \n",
       "260576                    2    5                4                  4   \n",
       "260577                    3   30                8                  6   \n",
       "260578                    2   15                9                  5   \n",
       "260579                    3   30               15                  8   \n",
       "260580                    2  995                4                  5   \n",
       "260581                    2    5                6                  5   \n",
       "260582                    2    5               11                  5   \n",
       "260583                    1    5               27                  3   \n",
       "260584                    2   10                6                  6   \n",
       "260585                    2   20                8                  5   \n",
       "260586                    2   45               17                  5   \n",
       "260587                    2    5               12                  6   \n",
       "260588                    3   20                7                  9   \n",
       "260589                    3   35               17                  7   \n",
       "260590                    3   15               11                 11   \n",
       "260591                    2    5                7                  5   \n",
       "260592                    1   25                5                  3   \n",
       "260593                    2   25                5                  5   \n",
       "260594                    2    5               13                  5   \n",
       "260595                    6    0               13                 12   \n",
       "260596                    2   20                8                  5   \n",
       "260597                    1   55                6                  3   \n",
       "260598                    2    0                6                  5   \n",
       "260599                    3   55                6                  7   \n",
       "260600                    2   10               14                  6   \n",
       "260601                    3   10                7                  6   \n",
       "\n",
       "        has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  \\\n",
       "1                                  1                                    1   \n",
       "2                                  0                                    1   \n",
       "3                                  0                                    1   \n",
       "4                                  0                                    1   \n",
       "5                                  1                                    0   \n",
       "6                                  0                                    1   \n",
       "7                                  0                                    1   \n",
       "8                                  0                                    0   \n",
       "9                                  0                                    1   \n",
       "10                                 0                                    0   \n",
       "11                                 0                                    1   \n",
       "12                                 0                                    0   \n",
       "13                                 0                                    1   \n",
       "14                                 0                                    1   \n",
       "15                                 0                                    0   \n",
       "16                                 0                                    1   \n",
       "17                                 0                                    1   \n",
       "18                                 1                                    1   \n",
       "19                                 0                                    0   \n",
       "20                                 0                                    1   \n",
       "21                                 1                                    0   \n",
       "22                                 0                                    1   \n",
       "23                                 0                                    1   \n",
       "24                                 0                                    1   \n",
       "25                                 0                                    1   \n",
       "26                                 0                                    1   \n",
       "27                                 0                                    1   \n",
       "28                                 0                                    1   \n",
       "29                                 0                                    1   \n",
       "30                                 1                                    0   \n",
       "...                              ...                                  ...   \n",
       "260572                             0                                    1   \n",
       "260573                             0                                    0   \n",
       "260574                             0                                    1   \n",
       "260575                             0                                    1   \n",
       "260576                             0                                    1   \n",
       "260577                             0                                    1   \n",
       "260578                             0                                    1   \n",
       "260579                             0                                    0   \n",
       "260580                             0                                    1   \n",
       "260581                             0                                    1   \n",
       "260582                             0                                    1   \n",
       "260583                             0                                    0   \n",
       "260584                             1                                    1   \n",
       "260585                             0                                    1   \n",
       "260586                             0                                    1   \n",
       "260587                             0                                    1   \n",
       "260588                             0                                    1   \n",
       "260589                             0                                    0   \n",
       "260590                             0                                    0   \n",
       "260591                             0                                    0   \n",
       "260592                             0                                    1   \n",
       "260593                             0                                    1   \n",
       "260594                             0                                    1   \n",
       "260595                             0                                    0   \n",
       "260596                             0                                    1   \n",
       "260597                             0                                    1   \n",
       "260598                             0                                    1   \n",
       "260599                             0                                    1   \n",
       "260600                             0                                    0   \n",
       "260601                             0                                    1   \n",
       "\n",
       "        ...  roof_type_x  ground_floor_type_v  other_floor_type_q  \\\n",
       "1       ...            0                    0                   1   \n",
       "2       ...            0                    0                   1   \n",
       "3       ...            0                    0                   0   \n",
       "4       ...            0                    0                   0   \n",
       "5       ...            0                    0                   0   \n",
       "6       ...            0                    0                   1   \n",
       "7       ...            0                    0                   1   \n",
       "8       ...            0                    1                   0   \n",
       "9       ...            0                    0                   1   \n",
       "10      ...            0                    1                   0   \n",
       "11      ...            0                    0                   1   \n",
       "12      ...            0                    1                   0   \n",
       "13      ...            0                    0                   0   \n",
       "14      ...            0                    0                   1   \n",
       "15      ...            0                    0                   0   \n",
       "16      ...            0                    0                   1   \n",
       "17      ...            0                    0                   1   \n",
       "18      ...            0                    0                   1   \n",
       "19      ...            0                    1                   0   \n",
       "20      ...            0                    0                   0   \n",
       "21      ...            0                    0                   1   \n",
       "22      ...            0                    0                   1   \n",
       "23      ...            0                    0                   1   \n",
       "24      ...            0                    0                   1   \n",
       "25      ...            0                    0                   1   \n",
       "26      ...            0                    0                   1   \n",
       "27      ...            0                    0                   1   \n",
       "28      ...            0                    0                   1   \n",
       "29      ...            0                    0                   1   \n",
       "30      ...            0                    0                   1   \n",
       "...     ...          ...                  ...                 ...   \n",
       "260572  ...            0                    0                   1   \n",
       "260573  ...            0                    0                   0   \n",
       "260574  ...            0                    0                   1   \n",
       "260575  ...            0                    0                   0   \n",
       "260576  ...            0                    0                   1   \n",
       "260577  ...            0                    0                   1   \n",
       "260578  ...            0                    0                   1   \n",
       "260579  ...            0                    0                   1   \n",
       "260580  ...            0                    0                   1   \n",
       "260581  ...            0                    0                   1   \n",
       "260582  ...            0                    0                   0   \n",
       "260583  ...            0                    1                   0   \n",
       "260584  ...            0                    0                   1   \n",
       "260585  ...            0                    0                   1   \n",
       "260586  ...            0                    0                   0   \n",
       "260587  ...            0                    0                   1   \n",
       "260588  ...            0                    0                   1   \n",
       "260589  ...            0                    0                   0   \n",
       "260590  ...            1                    1                   0   \n",
       "260591  ...            0                    0                   1   \n",
       "260592  ...            0                    0                   0   \n",
       "260593  ...            0                    0                   1   \n",
       "260594  ...            0                    0                   1   \n",
       "260595  ...            0                    0                   0   \n",
       "260596  ...            0                    0                   1   \n",
       "260597  ...            0                    0                   0   \n",
       "260598  ...            0                    0                   1   \n",
       "260599  ...            0                    0                   1   \n",
       "260600  ...            1                    1                   0   \n",
       "260601  ...            0                    0                   1   \n",
       "\n",
       "        other_floor_type_s  other_floor_type_x  position_t  \\\n",
       "1                        0                   0           1   \n",
       "2                        0                   0           0   \n",
       "3                        0                   1           1   \n",
       "4                        0                   1           0   \n",
       "5                        0                   1           0   \n",
       "6                        0                   0           0   \n",
       "7                        0                   0           0   \n",
       "8                        0                   1           0   \n",
       "9                        0                   0           0   \n",
       "10                       0                   0           0   \n",
       "11                       0                   0           1   \n",
       "12                       0                   0           1   \n",
       "13                       0                   1           0   \n",
       "14                       0                   0           0   \n",
       "15                       0                   1           0   \n",
       "16                       0                   0           0   \n",
       "17                       0                   0           1   \n",
       "18                       0                   0           0   \n",
       "19                       0                   0           0   \n",
       "20                       0                   1           0   \n",
       "21                       0                   0           0   \n",
       "22                       0                   0           0   \n",
       "23                       0                   0           0   \n",
       "24                       0                   0           0   \n",
       "25                       0                   0           0   \n",
       "26                       0                   0           0   \n",
       "27                       0                   0           0   \n",
       "28                       0                   0           0   \n",
       "29                       0                   0           0   \n",
       "30                       0                   0           1   \n",
       "...                    ...                 ...         ...   \n",
       "260572                   0                   0           0   \n",
       "260573                   0                   1           0   \n",
       "260574                   0                   0           0   \n",
       "260575                   0                   0           1   \n",
       "260576                   0                   0           0   \n",
       "260577                   0                   0           1   \n",
       "260578                   0                   0           0   \n",
       "260579                   0                   0           0   \n",
       "260580                   0                   0           0   \n",
       "260581                   0                   0           0   \n",
       "260582                   0                   1           0   \n",
       "260583                   0                   0           0   \n",
       "260584                   0                   0           0   \n",
       "260585                   0                   0           0   \n",
       "260586                   0                   1           1   \n",
       "260587                   0                   0           0   \n",
       "260588                   0                   0           0   \n",
       "260589                   0                   1           1   \n",
       "260590                   1                   0           0   \n",
       "260591                   0                   0           0   \n",
       "260592                   0                   0           0   \n",
       "260593                   0                   0           0   \n",
       "260594                   0                   0           0   \n",
       "260595                   0                   1           0   \n",
       "260596                   0                   0           0   \n",
       "260597                   0                   0           0   \n",
       "260598                   0                   0           0   \n",
       "260599                   0                   0           0   \n",
       "260600                   1                   0           0   \n",
       "260601                   0                   0           0   \n",
       "\n",
       "        plan_configuration_c  plan_configuration_q  plan_configuration_u  \\\n",
       "1                          0                     0                     0   \n",
       "2                          0                     0                     0   \n",
       "3                          0                     0                     0   \n",
       "4                          0                     0                     0   \n",
       "5                          0                     0                     0   \n",
       "6                          0                     0                     0   \n",
       "7                          0                     0                     0   \n",
       "8                          0                     0                     1   \n",
       "9                          0                     0                     0   \n",
       "10                         0                     0                     0   \n",
       "11                         0                     0                     0   \n",
       "12                         0                     0                     0   \n",
       "13                         0                     0                     0   \n",
       "14                         0                     0                     0   \n",
       "15                         0                     0                     0   \n",
       "16                         0                     0                     0   \n",
       "17                         0                     0                     0   \n",
       "18                         0                     0                     0   \n",
       "19                         0                     0                     0   \n",
       "20                         0                     0                     0   \n",
       "21                         0                     0                     0   \n",
       "22                         0                     0                     0   \n",
       "23                         0                     0                     0   \n",
       "24                         0                     0                     0   \n",
       "25                         0                     0                     0   \n",
       "26                         0                     0                     0   \n",
       "27                         0                     0                     0   \n",
       "28                         0                     0                     0   \n",
       "29                         0                     0                     0   \n",
       "30                         0                     0                     0   \n",
       "...                      ...                   ...                   ...   \n",
       "260572                     0                     0                     0   \n",
       "260573                     0                     0                     0   \n",
       "260574                     0                     0                     0   \n",
       "260575                     0                     0                     0   \n",
       "260576                     0                     0                     0   \n",
       "260577                     0                     0                     0   \n",
       "260578                     0                     0                     0   \n",
       "260579                     0                     0                     0   \n",
       "260580                     0                     0                     0   \n",
       "260581                     0                     0                     0   \n",
       "260582                     0                     0                     0   \n",
       "260583                     0                     0                     0   \n",
       "260584                     0                     0                     0   \n",
       "260585                     0                     0                     0   \n",
       "260586                     0                     0                     0   \n",
       "260587                     0                     0                     0   \n",
       "260588                     0                     0                     0   \n",
       "260589                     0                     0                     0   \n",
       "260590                     0                     0                     0   \n",
       "260591                     0                     0                     0   \n",
       "260592                     0                     0                     0   \n",
       "260593                     0                     0                     0   \n",
       "260594                     0                     0                     0   \n",
       "260595                     0                     0                     0   \n",
       "260596                     0                     0                     0   \n",
       "260597                     0                     1                     0   \n",
       "260598                     0                     0                     0   \n",
       "260599                     0                     0                     0   \n",
       "260600                     0                     0                     0   \n",
       "260601                     0                     0                     0   \n",
       "\n",
       "        legal_ownership_status_w  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "5                              0  \n",
       "6                              0  \n",
       "7                              0  \n",
       "8                              0  \n",
       "9                              0  \n",
       "10                             0  \n",
       "11                             0  \n",
       "12                             0  \n",
       "13                             0  \n",
       "14                             0  \n",
       "15                             0  \n",
       "16                             0  \n",
       "17                             0  \n",
       "18                             0  \n",
       "19                             0  \n",
       "20                             0  \n",
       "21                             0  \n",
       "22                             0  \n",
       "23                             0  \n",
       "24                             0  \n",
       "25                             0  \n",
       "26                             0  \n",
       "27                             0  \n",
       "28                             0  \n",
       "29                             0  \n",
       "30                             0  \n",
       "...                          ...  \n",
       "260572                         0  \n",
       "260573                         0  \n",
       "260574                         0  \n",
       "260575                         0  \n",
       "260576                         0  \n",
       "260577                         0  \n",
       "260578                         0  \n",
       "260579                         0  \n",
       "260580                         0  \n",
       "260581                         0  \n",
       "260582                         0  \n",
       "260583                         0  \n",
       "260584                         0  \n",
       "260585                         0  \n",
       "260586                         0  \n",
       "260587                         0  \n",
       "260588                         0  \n",
       "260589                         0  \n",
       "260590                         1  \n",
       "260591                         0  \n",
       "260592                         0  \n",
       "260593                         0  \n",
       "260594                         0  \n",
       "260595                         0  \n",
       "260596                         0  \n",
       "260597                         0  \n",
       "260598                         0  \n",
       "260599                         0  \n",
       "260600                         0  \n",
       "260601                         0  \n",
       "\n",
       "[260601 rows x 40 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_features = X_features.drop(labels=['building_id'],axis=1)\n",
    "Train_dummies = Train_dummies.drop(labels=['building_id'],axis=1)\n",
    "Test_dummies = Test_dummies.drop(labels=['building_id'],axis=1)\n",
    "#Train_dummies = Train_dummies.drop(labels=['building_id','geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'],axis=1).astype(float)\n",
    "#Test_dummies = Test_dummies.drop(labels=['building_id','geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'],axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Standardize data\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(Train_dummies)\n",
    "# Scale the train set\n",
    "X_std = scaler.transform(Train_dummies)\n",
    "# Scale the test set\n",
    "X_test_std = scaler.transform(Test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonstandardized data\n",
    "X_non = np.array(Train_dummies)\n",
    "X_test_non = np.array(Test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_std, y_labels, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208480, 60)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import *\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback code for monitoring learning rate\n",
    "class lr_mon(keras.callbacks.Callback):\n",
    "     def on_epoch_end(self, epoch, logs=None):\n",
    "        print(' ### Learnig rate at the end of epoch {} is {} \\n'.format(\n",
    "            epoch, K.eval(self.model.optimizer.lr) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as PReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as PReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208480 samples, validate on 52121 samples\n",
      "Epoch 1/100\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.7869 - acc: 0.6028 - f1_m: 0.5847 - val_loss: 0.7648 - val_acc: 0.6214 - val_f1_m: 0.6110\n",
      " ### Learnig rate at the end of epoch 0 is 1.0 \n",
      "\n",
      "Epoch 2/100\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.7441 - acc: 0.6373 - f1_m: 0.6285 - val_loss: 0.7544 - val_acc: 0.6397 - val_f1_m: 0.6344\n",
      " ### Learnig rate at the end of epoch 1 is 1.0 \n",
      "\n",
      "Epoch 3/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.7254 - acc: 0.6502 - f1_m: 0.6435 - val_loss: 0.7222 - val_acc: 0.6519 - val_f1_m: 0.6465\n",
      " ### Learnig rate at the end of epoch 2 is 1.0 \n",
      "\n",
      "Epoch 4/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.7169 - acc: 0.6561 - f1_m: 0.6506 - val_loss: 0.7273 - val_acc: 0.6507 - val_f1_m: 0.6439\n",
      " ### Learnig rate at the end of epoch 3 is 1.0 \n",
      "\n",
      "Epoch 5/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.7103 - acc: 0.6616 - f1_m: 0.6565 - val_loss: 0.7181 - val_acc: 0.6566 - val_f1_m: 0.6508\n",
      " ### Learnig rate at the end of epoch 4 is 1.0 \n",
      "\n",
      "Epoch 6/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.7050 - acc: 0.6655 - f1_m: 0.6609 - val_loss: 0.7107 - val_acc: 0.6633 - val_f1_m: 0.6579\n",
      " ### Learnig rate at the end of epoch 5 is 1.0 \n",
      "\n",
      "Epoch 7/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6997 - acc: 0.6694 - f1_m: 0.6654 - val_loss: 0.7087 - val_acc: 0.6691 - val_f1_m: 0.6667\n",
      " ### Learnig rate at the end of epoch 6 is 1.0 \n",
      "\n",
      "Epoch 8/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6951 - acc: 0.6717 - f1_m: 0.6682 - val_loss: 0.7024 - val_acc: 0.6697 - val_f1_m: 0.6652\n",
      " ### Learnig rate at the end of epoch 7 is 1.0 \n",
      "\n",
      "Epoch 9/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6911 - acc: 0.6755 - f1_m: 0.6716 - val_loss: 0.7025 - val_acc: 0.6738 - val_f1_m: 0.6715\n",
      " ### Learnig rate at the end of epoch 8 is 1.0 \n",
      "\n",
      "Epoch 10/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6864 - acc: 0.6775 - f1_m: 0.6737 - val_loss: 0.7012 - val_acc: 0.6724 - val_f1_m: 0.6654\n",
      " ### Learnig rate at the end of epoch 9 is 1.0 \n",
      "\n",
      "Epoch 11/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6825 - acc: 0.6799 - f1_m: 0.6766 - val_loss: 0.6926 - val_acc: 0.6773 - val_f1_m: 0.6727\n",
      " ### Learnig rate at the end of epoch 10 is 1.0 \n",
      "\n",
      "Epoch 12/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.6779 - acc: 0.6833 - f1_m: 0.6793 - val_loss: 0.6873 - val_acc: 0.6802 - val_f1_m: 0.6756\n",
      " ### Learnig rate at the end of epoch 11 is 1.0 \n",
      "\n",
      "Epoch 13/100\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6739 - acc: 0.6850 - f1_m: 0.6815 - val_loss: 0.6938 - val_acc: 0.6749 - val_f1_m: 0.6716\n",
      " ### Learnig rate at the end of epoch 12 is 1.0 \n",
      "\n",
      "Epoch 14/100\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.6709 - acc: 0.6869 - f1_m: 0.6832 - val_loss: 0.6982 - val_acc: 0.6749 - val_f1_m: 0.6721\n",
      " ### Learnig rate at the end of epoch 13 is 1.0 \n",
      "\n",
      "Epoch 15/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6677 - acc: 0.6888 - f1_m: 0.6856 - val_loss: 0.6871 - val_acc: 0.6799 - val_f1_m: 0.6774\n",
      " ### Learnig rate at the end of epoch 14 is 1.0 \n",
      "\n",
      "Epoch 16/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6643 - acc: 0.6900 - f1_m: 0.6871 - val_loss: 0.6890 - val_acc: 0.6791 - val_f1_m: 0.6759\n",
      " ### Learnig rate at the end of epoch 15 is 1.0 \n",
      "\n",
      "Epoch 17/100\n",
      "208480/208480 [==============================] - 5s 22us/step - loss: 0.6609 - acc: 0.6922 - f1_m: 0.6890 - val_loss: 0.6847 - val_acc: 0.6810 - val_f1_m: 0.6773\n",
      " ### Learnig rate at the end of epoch 16 is 1.0 \n",
      "\n",
      "Epoch 18/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6583 - acc: 0.6932 - f1_m: 0.6906 - val_loss: 0.6948 - val_acc: 0.6714 - val_f1_m: 0.6667\n",
      " ### Learnig rate at the end of epoch 17 is 1.0 \n",
      "\n",
      "Epoch 19/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6557 - acc: 0.6950 - f1_m: 0.6918 - val_loss: 0.6867 - val_acc: 0.6844 - val_f1_m: 0.6826\n",
      " ### Learnig rate at the end of epoch 18 is 1.0 \n",
      "\n",
      "Epoch 20/100\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6533 - acc: 0.6965 - f1_m: 0.6935 - val_loss: 0.6905 - val_acc: 0.6779 - val_f1_m: 0.6743\n",
      " ### Learnig rate at the end of epoch 19 is 1.0 \n",
      "\n",
      "Epoch 21/100\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6504 - acc: 0.6979 - f1_m: 0.6951 - val_loss: 0.6785 - val_acc: 0.6888 - val_f1_m: 0.6864\n",
      " ### Learnig rate at the end of epoch 20 is 1.0 \n",
      "\n",
      "Epoch 22/100\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.6481 - acc: 0.6993 - f1_m: 0.6965 - val_loss: 0.6830 - val_acc: 0.6865 - val_f1_m: 0.6841\n",
      " ### Learnig rate at the end of epoch 21 is 1.0 \n",
      "\n",
      "Epoch 23/100\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6458 - acc: 0.7011 - f1_m: 0.6981 - val_loss: 0.6863 - val_acc: 0.6864 - val_f1_m: 0.6830\n",
      " ### Learnig rate at the end of epoch 22 is 1.0 \n",
      "\n",
      "Epoch 24/100\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6433 - acc: 0.7022 - f1_m: 0.6995 - val_loss: 0.6857 - val_acc: 0.6814 - val_f1_m: 0.6792\n",
      " ### Learnig rate at the end of epoch 23 is 1.0 \n",
      "\n",
      "Epoch 25/100\n",
      "208480/208480 [==============================] - 7s 34us/step - loss: 0.6408 - acc: 0.7033 - f1_m: 0.7008 - val_loss: 0.6827 - val_acc: 0.6858 - val_f1_m: 0.6834\n",
      " ### Learnig rate at the end of epoch 24 is 1.0 \n",
      "\n",
      "Epoch 26/100\n",
      "208480/208480 [==============================] - 7s 32us/step - loss: 0.6387 - acc: 0.7047 - f1_m: 0.7020 - val_loss: 0.6862 - val_acc: 0.6852 - val_f1_m: 0.6824\n",
      " ### Learnig rate at the end of epoch 25 is 1.0 \n",
      "\n",
      "Epoch 27/100\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6362 - acc: 0.7059 - f1_m: 0.7031 - val_loss: 0.6899 - val_acc: 0.6842 - val_f1_m: 0.6816\n",
      " ### Learnig rate at the end of epoch 26 is 1.0 \n",
      "\n",
      "Epoch 28/100\n",
      "208480/208480 [==============================] - 6s 27us/step - loss: 0.6342 - acc: 0.7067 - f1_m: 0.7041 - val_loss: 0.6896 - val_acc: 0.6861 - val_f1_m: 0.6839\n",
      " ### Learnig rate at the end of epoch 27 is 1.0 \n",
      "\n",
      "Epoch 29/100\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.6321 - acc: 0.7076 - f1_m: 0.7050 - val_loss: 0.6851 - val_acc: 0.6867 - val_f1_m: 0.6835\n",
      " ### Learnig rate at the end of epoch 28 is 1.0 \n",
      "\n",
      "Epoch 30/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.6294 - acc: 0.7094 - f1_m: 0.7072 - val_loss: 0.6830 - val_acc: 0.6875 - val_f1_m: 0.6857\n",
      " ### Learnig rate at the end of epoch 29 is 1.0 \n",
      "\n",
      "Epoch 31/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.6270 - acc: 0.7103 - f1_m: 0.7077 - val_loss: 0.6874 - val_acc: 0.6849 - val_f1_m: 0.6827\n",
      " ### Learnig rate at the end of epoch 30 is 1.0 \n",
      "\n",
      "Epoch 32/100\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.6008 - acc: 0.7227 - f1_m: 0.7209 - val_loss: 0.6763 - val_acc: 0.6953 - val_f1_m: 0.6929\n",
      " ### Learnig rate at the end of epoch 31 is 0.10000000149011612 \n",
      "\n",
      "Epoch 33/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5949 - acc: 0.7268 - f1_m: 0.7247 - val_loss: 0.6807 - val_acc: 0.6952 - val_f1_m: 0.6932\n",
      " ### Learnig rate at the end of epoch 32 is 0.10000000149011612 \n",
      "\n",
      "Epoch 34/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5924 - acc: 0.7279 - f1_m: 0.7261 - val_loss: 0.6800 - val_acc: 0.6961 - val_f1_m: 0.6937\n",
      " ### Learnig rate at the end of epoch 33 is 0.10000000149011612 \n",
      "\n",
      "Epoch 35/100\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5907 - acc: 0.7286 - f1_m: 0.7266 - val_loss: 0.6818 - val_acc: 0.6958 - val_f1_m: 0.6943\n",
      " ### Learnig rate at the end of epoch 34 is 0.10000000149011612 \n",
      "\n",
      "Epoch 36/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5891 - acc: 0.7300 - f1_m: 0.7282 - val_loss: 0.6823 - val_acc: 0.6960 - val_f1_m: 0.6936\n",
      " ### Learnig rate at the end of epoch 35 is 0.10000000149011612 \n",
      "\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5878 - acc: 0.7301 - f1_m: 0.7283 - val_loss: 0.6829 - val_acc: 0.6953 - val_f1_m: 0.6931\n",
      " ### Learnig rate at the end of epoch 36 is 0.10000000149011612 \n",
      "\n",
      "Epoch 38/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5864 - acc: 0.7311 - f1_m: 0.7292 - val_loss: 0.6829 - val_acc: 0.6966 - val_f1_m: 0.6941\n",
      " ### Learnig rate at the end of epoch 37 is 0.10000000149011612 \n",
      "\n",
      "Epoch 39/100\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5852 - acc: 0.7318 - f1_m: 0.7300 - val_loss: 0.6852 - val_acc: 0.6947 - val_f1_m: 0.6928\n",
      " ### Learnig rate at the end of epoch 38 is 0.10000000149011612 \n",
      "\n",
      "Epoch 40/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5841 - acc: 0.7323 - f1_m: 0.7305 - val_loss: 0.6841 - val_acc: 0.6954 - val_f1_m: 0.6929\n",
      " ### Learnig rate at the end of epoch 39 is 0.10000000149011612 \n",
      "\n",
      "Epoch 41/100\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5830 - acc: 0.7326 - f1_m: 0.7307 - val_loss: 0.6866 - val_acc: 0.6959 - val_f1_m: 0.6943\n",
      " ### Learnig rate at the end of epoch 40 is 0.10000000149011612 \n",
      "\n",
      "Epoch 42/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5819 - acc: 0.7335 - f1_m: 0.7317 - val_loss: 0.6860 - val_acc: 0.6963 - val_f1_m: 0.6941\n",
      " ### Learnig rate at the end of epoch 41 is 0.10000000149011612 \n",
      "\n",
      "Epoch 43/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5810 - acc: 0.7342 - f1_m: 0.7324 - val_loss: 0.6853 - val_acc: 0.6954 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 42 is 0.10000000149011612 \n",
      "\n",
      "Epoch 44/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5800 - acc: 0.7347 - f1_m: 0.7330 - val_loss: 0.6886 - val_acc: 0.6958 - val_f1_m: 0.6935\n",
      " ### Learnig rate at the end of epoch 43 is 0.10000000149011612 \n",
      "\n",
      "Epoch 45/100\n",
      "208480/208480 [==============================] - 6s 30us/step - loss: 0.5790 - acc: 0.7351 - f1_m: 0.7332 - val_loss: 0.6878 - val_acc: 0.6946 - val_f1_m: 0.6925\n",
      " ### Learnig rate at the end of epoch 44 is 0.10000000149011612 \n",
      "\n",
      "Epoch 46/100\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5780 - acc: 0.7357 - f1_m: 0.7337 - val_loss: 0.6895 - val_acc: 0.6954 - val_f1_m: 0.6933\n",
      " ### Learnig rate at the end of epoch 45 is 0.10000000149011612 \n",
      "\n",
      "Epoch 47/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5769 - acc: 0.7360 - f1_m: 0.7342 - val_loss: 0.6928 - val_acc: 0.6958 - val_f1_m: 0.6938\n",
      " ### Learnig rate at the end of epoch 46 is 0.10000000149011612 \n",
      "\n",
      "Epoch 48/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5761 - acc: 0.7365 - f1_m: 0.7348 - val_loss: 0.6898 - val_acc: 0.6957 - val_f1_m: 0.6936\n",
      " ### Learnig rate at the end of epoch 47 is 0.10000000149011612 \n",
      "\n",
      "Epoch 49/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5711 - acc: 0.7400 - f1_m: 0.7382 - val_loss: 0.6917 - val_acc: 0.6957 - val_f1_m: 0.6933\n",
      " ### Learnig rate at the end of epoch 48 is 0.009999999776482582 \n",
      "\n",
      "Epoch 50/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5704 - acc: 0.7396 - f1_m: 0.7381 - val_loss: 0.6916 - val_acc: 0.6960 - val_f1_m: 0.6938\n",
      " ### Learnig rate at the end of epoch 49 is 0.009999999776482582 \n",
      "\n",
      "Epoch 51/100\n",
      "208480/208480 [==============================] - 6s 26us/step - loss: 0.5702 - acc: 0.7397 - f1_m: 0.7382 - val_loss: 0.6918 - val_acc: 0.6961 - val_f1_m: 0.6936\n",
      " ### Learnig rate at the end of epoch 50 is 0.009999999776482582 \n",
      "\n",
      "Epoch 52/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5700 - acc: 0.7398 - f1_m: 0.7381 - val_loss: 0.6919 - val_acc: 0.6962 - val_f1_m: 0.6939\n",
      " ### Learnig rate at the end of epoch 51 is 0.009999999776482582 \n",
      "\n",
      "Epoch 53/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5699 - acc: 0.7399 - f1_m: 0.7384 - val_loss: 0.6921 - val_acc: 0.6962 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 52 is 0.009999999776482582 \n",
      "\n",
      "Epoch 54/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5697 - acc: 0.7400 - f1_m: 0.7384 - val_loss: 0.6926 - val_acc: 0.6961 - val_f1_m: 0.6938\n",
      " ### Learnig rate at the end of epoch 53 is 0.009999999776482582 \n",
      "\n",
      "Epoch 55/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5696 - acc: 0.7401 - f1_m: 0.7387 - val_loss: 0.6923 - val_acc: 0.6960 - val_f1_m: 0.6936\n",
      " ### Learnig rate at the end of epoch 54 is 0.009999999776482582 \n",
      "\n",
      "Epoch 56/100\n",
      "208480/208480 [==============================] - 7s 33us/step - loss: 0.5694 - acc: 0.7402 - f1_m: 0.7387 - val_loss: 0.6930 - val_acc: 0.6962 - val_f1_m: 0.6935\n",
      " ### Learnig rate at the end of epoch 55 is 0.009999999776482582 \n",
      "\n",
      "Epoch 57/100\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5693 - acc: 0.7402 - f1_m: 0.7386 - val_loss: 0.6929 - val_acc: 0.6958 - val_f1_m: 0.6933\n",
      " ### Learnig rate at the end of epoch 56 is 0.009999999776482582 \n",
      "\n",
      "Epoch 58/100\n",
      "208480/208480 [==============================] - 8s 38us/step - loss: 0.5692 - acc: 0.7402 - f1_m: 0.7386 - val_loss: 0.6931 - val_acc: 0.6957 - val_f1_m: 0.6933\n",
      " ### Learnig rate at the end of epoch 57 is 0.009999999776482582 \n",
      "\n",
      "Epoch 59/100\n",
      "208480/208480 [==============================] - 7s 35us/step - loss: 0.5686 - acc: 0.7407 - f1_m: 0.7391 - val_loss: 0.6931 - val_acc: 0.6959 - val_f1_m: 0.6935\n",
      " ### Learnig rate at the end of epoch 58 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 60/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5685 - acc: 0.7406 - f1_m: 0.7392 - val_loss: 0.6931 - val_acc: 0.6960 - val_f1_m: 0.6935\n",
      " ### Learnig rate at the end of epoch 59 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 61/100\n",
      "208480/208480 [==============================] - 5s 25us/step - loss: 0.5685 - acc: 0.7407 - f1_m: 0.7392 - val_loss: 0.6931 - val_acc: 0.6959 - val_f1_m: 0.6933\n",
      " ### Learnig rate at the end of epoch 60 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 62/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5685 - acc: 0.7408 - f1_m: 0.7392 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 61 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 63/100\n",
      "208480/208480 [==============================] - 5s 26us/step - loss: 0.5684 - acc: 0.7407 - f1_m: 0.7392 - val_loss: 0.6932 - val_acc: 0.6961 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 62 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 64/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5684 - acc: 0.7407 - f1_m: 0.7392 - val_loss: 0.6932 - val_acc: 0.6959 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 63 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 65/100\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5684 - acc: 0.7407 - f1_m: 0.7392 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 64 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 66/100\n",
      "208480/208480 [==============================] - 5s 23us/step - loss: 0.5684 - acc: 0.7407 - f1_m: 0.7392 - val_loss: 0.6932 - val_acc: 0.6959 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 65 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 67/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5684 - acc: 0.7408 - f1_m: 0.7392 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 66 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 68/100\n",
      "208480/208480 [==============================] - 5s 24us/step - loss: 0.5684 - acc: 0.7407 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 67 is 0.0009999999310821295 \n",
      "\n",
      "Epoch 69/100\n",
      "208480/208480 [==============================] - 10s 46us/step - loss: 0.5683 - acc: 0.7408 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6933\n",
      " ### Learnig rate at the end of epoch 68 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 70/100\n",
      "208480/208480 [==============================] - 11s 55us/step - loss: 0.5683 - acc: 0.7408 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 69 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208480/208480 [==============================] - 11s 51us/step - loss: 0.5683 - acc: 0.7408 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6959 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 70 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 72/100\n",
      "208480/208480 [==============================] - 8s 39us/step - loss: 0.5683 - acc: 0.7408 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6959 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 71 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 73/100\n",
      "208480/208480 [==============================] - 6s 29us/step - loss: 0.5683 - acc: 0.7408 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 72 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 74/100\n",
      "208480/208480 [==============================] - 6s 28us/step - loss: 0.5683 - acc: 0.7408 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 73 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 75/100\n",
      "208480/208480 [==============================] - 6s 31us/step - loss: 0.5683 - acc: 0.7408 - f1_m: 0.7393 - val_loss: 0.6932 - val_acc: 0.6960 - val_f1_m: 0.6934\n",
      " ### Learnig rate at the end of epoch 74 is 9.99999901978299e-05 \n",
      "\n",
      "Epoch 76/100\n",
      " 39168/208480 [====>.........................] - ETA: 4s - loss: 0.5676 - acc: 0.7423 - f1_m: 0.7408"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-31fbdb8a734b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m history = model.fit(X_train, y_train, epochs=100, batch_size=128,validation_data=(X_val,y_val),\n\u001b[0;32m---> 25\u001b[0;31m                     callbacks=[lr_call,lr_scheduler])\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eLU=keras.layers.ELU(alpha=1.0)\n",
    "preLU=keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "\n",
    "reLU=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=60, activation='sigmoid'))\n",
    "model.add(Dense(units=128, activation=preLU))\n",
    "model.add(Dense(units=128, activation=reLU))\n",
    "model.add(Dense(units=128, activation=preLU))\n",
    "model.add(Dense(units=128, activation=reLU))\n",
    "#model.add(Dense(units=128, activation='sigmoid'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = keras.optimizers.optimizer(lr=0.02, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy',f1_m])\n",
    "\n",
    "lr_call = lr_mon()\n",
    "lr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=0, \n",
    "                                               mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128,validation_data=(X_val,y_val),\n",
    "                    callbacks=[lr_call,lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4457851e-05, 1.7148101e-01, 8.2849449e-01],\n",
       "       [1.6089629e-02, 8.7955129e-01, 1.0435908e-01],\n",
       "       [1.2752262e-02, 8.7242651e-01, 1.1482119e-01],\n",
       "       ...,\n",
       "       [2.7077764e-02, 6.9067764e-01, 2.8224462e-01],\n",
       "       [5.9272065e-03, 8.6788934e-01, 1.2618341e-01],\n",
       "       [2.3895143e-01, 7.4554312e-01, 1.5505441e-02]], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = np.argmax(y_pred,1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Number of Buildings of Predicted Damage Grade')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_prediction)\n",
    "plt.title('Number of Buildings of Predicted Damage Grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "submission_df = Test_values_df[['building_id']]\n",
    "submission_df['damage_grade'] = y_prediction\n",
    "submission_df.to_csv('submission_ReLU_ft90.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
